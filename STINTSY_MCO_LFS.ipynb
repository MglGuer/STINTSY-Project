{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Introduction ##\n",
    "\n",
    "In this notebook, the dataset to be processed is the Labor Force Survey conducted April 2016 and retrieved through Philippine Statistics Authority database. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# autoreload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing LFS PUF April 2016.CSV</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lfs_data = pd.read_csv(\"LFS PUF April 2016.CSV\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV file not found. Please make sure the file exists in the correct directory or provide the correct path.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Information, Pre-Processing, and Cleaning</h1>\n",
    "\n",
    "Let's get an overview of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 180862 entries, 0 to 180861\n",
      "Data columns (total 50 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   PUFREG           180862 non-null  int64  \n",
      " 1   PUFPRV           180862 non-null  int64  \n",
      " 2   PUFPRRCD         180862 non-null  int64  \n",
      " 3   PUFHHNUM         180862 non-null  int64  \n",
      " 4   PUFURB2K10       180862 non-null  int64  \n",
      " 5   PUFPWGTFIN       180862 non-null  float64\n",
      " 6   PUFSVYMO         180862 non-null  int64  \n",
      " 7   PUFSVYYR         180862 non-null  int64  \n",
      " 8   PUFPSU           180862 non-null  int64  \n",
      " 9   PUFRPL           180862 non-null  int64  \n",
      " 10  PUFHHSIZE        180862 non-null  int64  \n",
      " 11  PUFC01_LNO       180862 non-null  int64  \n",
      " 12  PUFC03_REL       180862 non-null  int64  \n",
      " 13  PUFC04_SEX       180862 non-null  int64  \n",
      " 14  PUFC05_AGE       180862 non-null  int64  \n",
      " 15  PUFC06_MSTAT     180862 non-null  object \n",
      " 16  PUFC07_GRADE     180862 non-null  object \n",
      " 17  PUFC08_CURSCH    180862 non-null  object \n",
      " 18  PUFC09_GRADTECH  180862 non-null  object \n",
      " 19  PUFC10_CONWR     180862 non-null  object \n",
      " 20  PUFC11_WORK      180862 non-null  object \n",
      " 21  PUFC12_JOB       180862 non-null  object \n",
      " 22  PUFC14_PROCC     180862 non-null  object \n",
      " 23  PUFC16_PKB       180862 non-null  object \n",
      " 24  PUFC17_NATEM     180862 non-null  object \n",
      " 25  PUFC18_PNWHRS    180862 non-null  object \n",
      " 26  PUFC19_PHOURS    180862 non-null  object \n",
      " 27  PUFC20_PWMORE    180862 non-null  object \n",
      " 28  PUFC21_PLADDW    180862 non-null  object \n",
      " 29  PUFC22_PFWRK     180862 non-null  object \n",
      " 30  PUFC23_PCLASS    180862 non-null  object \n",
      " 31  PUFC24_PBASIS    180862 non-null  object \n",
      " 32  PUFC25_PBASIC    180862 non-null  object \n",
      " 33  PUFC26_OJOB      180862 non-null  object \n",
      " 34  PUFC27_NJOBS     180862 non-null  object \n",
      " 35  PUFC28_THOURS    180862 non-null  object \n",
      " 36  PUFC29_WWM48H    180862 non-null  object \n",
      " 37  PUFC30_LOOKW     180862 non-null  object \n",
      " 38  PUFC31_FLWRK     180862 non-null  object \n",
      " 39  PUFC32_JOBSM     180862 non-null  object \n",
      " 40  PUFC33_WEEKS     180862 non-null  object \n",
      " 41  PUFC34_WYNOT     180862 non-null  object \n",
      " 42  PUFC35_LTLOOKW   180862 non-null  object \n",
      " 43  PUFC36_AVAIL     180862 non-null  object \n",
      " 44  PUFC37_WILLING   180862 non-null  object \n",
      " 45  PUFC38_PREVJOB   180862 non-null  object \n",
      " 46  PUFC40_POCC      180862 non-null  object \n",
      " 47  PUFC41_WQTR      180862 non-null  object \n",
      " 48  PUFC43_QKB       180862 non-null  object \n",
      " 49  PUFNEWEMPSTAT    180862 non-null  object \n",
      "dtypes: float64(1), int64(14), object(35)\n",
      "memory usage: 69.0+ MB\n"
     ]
    }
   ],
   "source": [
    "lfs_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Of interest to us, there are:\n",
    "<ul><li>1 contains float values, </li>\n",
    "<li>14 contain integer values, and </li>\n",
    "<li><b>35 are object values</b>.</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's check for duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfs_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicates here, and therefore no cleaning need follow in this regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems to contain null values in the form of whitespaces. Let's count those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Empty Cells:\n",
      "PUFC06_MSTAT        18339\n",
      "PUFC07_GRADE        18339\n",
      "PUFC08_CURSCH      107137\n",
      "PUFC09_GRADTECH     57782\n",
      "PUFC10_CONWR        57782\n",
      "PUFC11_WORK         21894\n",
      "PUFC12_JOB          93306\n",
      "PUFC14_PROCC       108360\n",
      "PUFC16_PKB         108360\n",
      "PUFC17_NATEM       109507\n",
      "PUFC18_PNWHRS      109507\n",
      "PUFC19_PHOURS      109507\n",
      "PUFC20_PWMORE      109507\n",
      "PUFC21_PLADDW      109507\n",
      "PUFC22_PFWRK       109507\n",
      "PUFC23_PCLASS      109507\n",
      "PUFC24_PBASIS      138947\n",
      "PUFC25_PBASIC      144274\n",
      "PUFC26_OJOB        109507\n",
      "PUFC27_NJOBS       174924\n",
      "PUFC28_THOURS      109507\n",
      "PUFC29_WWM48H      163629\n",
      "PUFC30_LOOKW       132692\n",
      "PUFC31_FLWRK       178569\n",
      "PUFC32_JOBSM       178569\n",
      "PUFC33_WEEKS       178569\n",
      "PUFC34_WYNOT       134985\n",
      "PUFC35_LTLOOKW     179269\n",
      "PUFC36_AVAIL       174893\n",
      "PUFC37_WILLING     174893\n",
      "PUFC38_PREVJOB     132692\n",
      "PUFC40_POCC        152982\n",
      "PUFC41_WQTR         81627\n",
      "PUFC43_QKB         107825\n",
      "PUFNEWEMPSTAT       61337\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "has_null = lfs_data.apply(lambda col: col.str.isspace().sum() if col.dtype == 'object' else 0)\n",
    "\n",
    "print(\"Number Empty Cells:\")\n",
    "print(has_null[has_null > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And standardize, replacing these whitespace values with -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "lfs_data.replace(r\"^\\s+$\", -1, regex=True, inplace=True)\n",
    "nan_counts_per_column = lfs_data.isna().sum()\n",
    "print(nan_counts_per_column[nan_counts_per_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these are -1, let's return to the data types, and find if our object columns from earlier are convertible to integers (or float):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safely convertable to int:\n",
      "['PUFC06_MSTAT', 'PUFC07_GRADE', 'PUFC08_CURSCH', 'PUFC09_GRADTECH', 'PUFC10_CONWR', 'PUFC11_WORK', 'PUFC12_JOB', 'PUFC14_PROCC', 'PUFC16_PKB', 'PUFC17_NATEM', 'PUFC18_PNWHRS', 'PUFC19_PHOURS', 'PUFC20_PWMORE', 'PUFC21_PLADDW', 'PUFC22_PFWRK', 'PUFC23_PCLASS', 'PUFC24_PBASIS', 'PUFC25_PBASIC', 'PUFC26_OJOB', 'PUFC27_NJOBS', 'PUFC28_THOURS', 'PUFC29_WWM48H', 'PUFC30_LOOKW', 'PUFC31_FLWRK', 'PUFC32_JOBSM', 'PUFC33_WEEKS', 'PUFC34_WYNOT', 'PUFC35_LTLOOKW', 'PUFC36_AVAIL', 'PUFC37_WILLING', 'PUFC38_PREVJOB', 'PUFC40_POCC', 'PUFC41_WQTR', 'PUFC43_QKB', 'PUFNEWEMPSTAT']\n"
     ]
    }
   ],
   "source": [
    "int_convertible_columns = []\n",
    "\n",
    "for col in lfs_data.columns:\n",
    "    if lfs_data[col].dtypes == 'object':  \n",
    "        try:\n",
    "            float_vals = lfs_data[col].dropna().astype(float)\n",
    "            if (float_vals % 1 == 0).all():\n",
    "                int_convertible_columns.append(col)\n",
    "        except ValueError:\n",
    "            pass \n",
    "\n",
    "print(\"Safely convertable to int:\")\n",
    "print(int_convertible_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And convert to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = [\n",
    "    'PUFC06_MSTAT', 'PUFC08_CURSCH', 'PUFC09_GRADTECH', 'PUFC10_CONWR', 'PUFC11_WORK', \n",
    "    'PUFC12_JOB', 'PUFC14_PROCC', 'PUFC16_PKB', 'PUFC17_NATEM', 'PUFC18_PNWHRS', \n",
    "    'PUFC19_PHOURS', 'PUFC20_PWMORE', 'PUFC21_PLADDW', 'PUFC22_PFWRK', 'PUFC23_PCLASS', \n",
    "    'PUFC24_PBASIS', 'PUFC25_PBASIC', 'PUFC26_OJOB', 'PUFC27_NJOBS', 'PUFC28_THOURS', \n",
    "    'PUFC29_WWM48H', 'PUFC30_LOOKW', 'PUFC31_FLWRK', 'PUFC32_JOBSM', 'PUFC33_WEEKS', \n",
    "    'PUFC34_WYNOT', 'PUFC35_LTLOOKW', 'PUFC36_AVAIL', 'PUFC37_WILLING', 'PUFC38_PREVJOB', \n",
    "    'PUFC40_POCC', 'PUFC41_WQTR', 'PUFC43_QKB', 'PUFNEWEMPSTAT'\n",
    "]\n",
    "\n",
    "for col in columns_to_convert:\n",
    "    lfs_data[col] = lfs_data[col].astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's also apply the unique() function to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PUFREG                17\n",
       "PUFPRV                86\n",
       "PUFPRRCD             116\n",
       "PUFHHNUM           40880\n",
       "PUFURB2K10             2\n",
       "PUFPWGTFIN         35599\n",
       "PUFSVYMO               1\n",
       "PUFSVYYR               1\n",
       "PUFPSU               975\n",
       "PUFRPL                 4\n",
       "PUFHHSIZE             20\n",
       "PUFC01_LNO            23\n",
       "PUFC03_REL            11\n",
       "PUFC04_SEX             2\n",
       "PUFC05_AGE           100\n",
       "PUFC06_MSTAT           7\n",
       "PUFC07_GRADE          68\n",
       "PUFC08_CURSCH          3\n",
       "PUFC09_GRADTECH        3\n",
       "PUFC10_CONWR           6\n",
       "PUFC11_WORK            3\n",
       "PUFC12_JOB             3\n",
       "PUFC14_PROCC          44\n",
       "PUFC16_PKB            88\n",
       "PUFC17_NATEM           4\n",
       "PUFC18_PNWHRS         17\n",
       "PUFC19_PHOURS        103\n",
       "PUFC20_PWMORE          3\n",
       "PUFC21_PLADDW          3\n",
       "PUFC22_PFWRK           3\n",
       "PUFC23_PCLASS          8\n",
       "PUFC24_PBASIS          9\n",
       "PUFC25_PBASIC       1152\n",
       "PUFC26_OJOB            3\n",
       "PUFC27_NJOBS           6\n",
       "PUFC28_THOURS        111\n",
       "PUFC29_WWM48H          6\n",
       "PUFC30_LOOKW           3\n",
       "PUFC31_FLWRK           3\n",
       "PUFC32_JOBSM           7\n",
       "PUFC33_WEEKS          36\n",
       "PUFC34_WYNOT          10\n",
       "PUFC35_LTLOOKW         4\n",
       "PUFC36_AVAIL           3\n",
       "PUFC37_WILLING         3\n",
       "PUFC38_PREVJOB         3\n",
       "PUFC40_POCC           44\n",
       "PUFC41_WQTR            3\n",
       "PUFC43_QKB            89\n",
       "PUFNEWEMPSTAT          4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfs_data.apply(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Considering our dataset has 18,000 entries, features with particularly low numbers stand out as questions that have clear, defined choices. Reviewing the [questionnaire](https://psada.psa.gov.ph/catalog/67/download/537), we find that certain questions ask the participant to specify beyond prespecified choices.\n",
    "\n",
    "This column possibly contains \"010,\" which is obviously not an integer. We ensure this column is a string, and check for values not specified in the questionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['350' '320' '250' -1 '622' '672' '240' '220' '614' '330' '010' '280'\n",
      " '632' '310' '000' '900' '820' '230' '589' '572' '210' '830' '810' '634'\n",
      " '686' '581' '681' '552' '534' '840' '658' '548' '648' '652' '662' '601'\n",
      " '642' '562' '260' '685' '631' '684' '340' '584' '621' '410' '420' '664'\n",
      " '676' '521' '638' '554' '646' '689' '522' '654' '644' '532' '531' '514'\n",
      " '558' '501' '586' '542' '576' '544' '585' '564']\n"
     ]
    }
   ],
   "source": [
    "lfs_data['PUFC07_GRADE'] = lfs_data['PUFC07_GRADE']\n",
    "valid_codes = [\n",
    "    0, 10,  # No Grade, Preschool\n",
    "    210, 220, 230, 240, 250, 260, 280,  # Elementary\n",
    "    310, 320, 330, 340, 350,  # High School\n",
    "    410, 420,  # Post Secondary; If Graduate Specify\n",
    "    810, 820, 830, 840,  # College; If Graduate Specify\n",
    "    900,  # Post Baccalaureate\n",
    "    np.nan\n",
    "]\n",
    "invalid_rows = lfs_data[~(lfs_data['PUFC07_GRADE'].isin(valid_codes))]\n",
    "\n",
    "unique_invalid_values = invalid_rows['PUFC07_GRADE'].unique()\n",
    "print(unique_invalid_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values 5XX 6XX are not detailed in the questionnaire. As it instructs the participant to specify whether they graduated from post secondary or college, we'll create a new data point to encapsulate these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700]\n"
     ]
    }
   ],
   "source": [
    "lfs_data.loc[~lfs_data['PUFC07_GRADE'].isin(valid_codes), 'PUFC07_GRADE'] = 700\n",
    "print(lfs_data['PUFC07_GRADE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strong correlations (|corr| > 0.5 and |corr| < 1):\n",
      "PUFPRV — PUFPRRCD: 1.000\n",
      "PUFREG — PUFHHNUM: 0.995\n",
      "PUFC19_PHOURS — PUFC28_THOURS: 0.972\n",
      "PUFC16_PKB — PUFC43_QKB: 0.969\n",
      "PUFC11_WORK — PUFNEWEMPSTAT: 0.964\n",
      "PUFC41_WQTR — PUFNEWEMPSTAT: 0.878\n",
      "PUFC11_WORK — PUFC41_WQTR: 0.852\n",
      "PUFC31_FLWRK — PUFC38_PREVJOB: -0.795\n",
      "PUFC36_AVAIL — PUFC37_WILLING: 0.785\n",
      "PUFC37_WILLING — PUFNEWEMPSTAT: 0.785\n",
      "PUFC18_PNWHRS — PUFC19_PHOURS: 0.785\n",
      "PUFC18_PNWHRS — PUFC28_THOURS: 0.769\n",
      "PUFPWGTFIN — PUFPSU: 0.709\n",
      "PUFC12_JOB — PUFNEWEMPSTAT: 0.704\n",
      "PUFC05_AGE — PUFC06_MSTAT: 0.701\n",
      "PUFC34_WYNOT — PUFNEWEMPSTAT: 0.631\n",
      "PUFC30_LOOKW — PUFNEWEMPSTAT: 0.625\n",
      "PUFC01_LNO — PUFC03_REL: 0.625\n",
      "PUFC05_AGE — PUFC08_CURSCH: 0.590\n",
      "PUFHHSIZE — PUFC01_LNO: 0.571\n",
      "PUFC01_LNO — PUFC05_AGE: -0.567\n",
      "PUFC20_PWMORE — PUFC21_PLADDW: 0.556\n",
      "PUFC08_CURSCH — PUFC11_WORK: -0.514\n",
      "PUFC08_CURSCH — PUFC34_WYNOT: -0.510\n",
      "PUFC08_CURSCH — PUFNEWEMPSTAT: -0.509\n"
     ]
    }
   ],
   "source": [
    "lfs_data_with_nan = lfs_data.copy()\n",
    "lfs_data_with_nan.replace(-1, np.nan, inplace=True)\n",
    "corr_matrix = lfs_data_with_nan.corr()\n",
    "\n",
    "strong_correlations = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i + 1, len(corr_matrix.columns)): \n",
    "        corr_value = corr_matrix.iloc[i, j]\n",
    "        if (0.5 < corr_value < 1) or (-1 < corr_value < -0.5):\n",
    "            strong_correlations.append((\n",
    "                corr_matrix.index[i], \n",
    "                corr_matrix.columns[j], \n",
    "                corr_value\n",
    "            ))\n",
    "\n",
    "strong_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "print(\"Strong correlations (|corr| > 0.5 and |corr| < 1):\")\n",
    "for var1, var2, corr in strong_correlations:\n",
    "    print(f\"{var1} — {var2}: {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "We will opt for a simple 80/20 train-test split. We will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for handling missing values\n",
    "class LFSDataset(Dataset):\n",
    "    def __init__(self, features, labels, missing_value=-1):\n",
    "        # Convert to numpy arrays\n",
    "        self.features = features.values.astype(np.float32)\n",
    "        \n",
    "        # Ensure labels are 0 or 1\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(unique_labels) <= 2 and 0 in unique_labels and 1 in unique_labels:\n",
    "            self.labels = labels.values.astype(np.float32)\n",
    "        else:\n",
    "            # Map lowest value to 0, others to 1\n",
    "            min_label = labels.min()\n",
    "            self.labels = (labels.values != min_label).astype(np.float32)\n",
    "            print(f\"Normalized target values from {unique_labels} to [0, 1]\")\n",
    "        \n",
    "        self.missing_value = missing_value\n",
    "        \n",
    "        # Create mask (1 = present, 0 = missing)\n",
    "        self.mask = (self.features != missing_value).astype(np.float32)\n",
    "        \n",
    "        # Replace missing values with 0\n",
    "        self.features = np.where(self.features == missing_value, 0, self.features)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': self.features[idx],\n",
    "            'mask': self.mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Logistic regression with missing value handling\n",
    "class MaskedLogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MaskedLogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, features, mask):\n",
    "        # Zero out missing features\n",
    "        masked_features = features * mask\n",
    "        output = self.linear(masked_features)\n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "def prepare_data(lfs_data, target_col='PUFC11_WORK', feature_cols=None, test_size=0.2, missing_value=-1):\n",
    "\n",
    "    feature_cols = [\n",
    "            'PUFC05_AGE', 'PUFC06_MSTAT', 'PUFC04_SEX', \n",
    "            'PUFC07_GRADE', 'PUFC08_CURSCH', \n",
    "            'PUFC38_PREVJOB', 'PUFC31_FLWRK',\n",
    "            'PUFC30_LOOKW', 'PUFC34_WYNOT'\n",
    "    ]\n",
    "    \n",
    "    # Identify rows where target is not missing\n",
    "    mask = lfs_data[target_col] != missing_value\n",
    "    filtered_data = lfs_data.loc[mask, feature_cols + [target_col]]\n",
    "\n",
    "    X = filtered_data[feature_cols]\n",
    "    y = filtered_data[target_col]\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=45)\n",
    "    \n",
    "    # Handle missing values for scaling\n",
    "    # Create masks where values are missing\n",
    "    train_missing_mask = X_train == missing_value\n",
    "    test_missing_mask = X_test == missing_value\n",
    "    \n",
    "    # Temporarily fill missing values with 0 for scaling\n",
    "    X_train_filled = X_train.copy()\n",
    "    X_test_filled = X_test.copy()\n",
    "    X_train_filled[train_missing_mask] = 0\n",
    "    X_test_filled[test_missing_mask] = 0\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train_filled),\n",
    "        columns=X_train.columns,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    \n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_test_filled),\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    # Restore missing value markers\n",
    "    X_train_scaled[train_missing_mask] = missing_value\n",
    "    X_test_scaled[test_missing_mask] = missing_value\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train_scaled,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'feature_names': feature_cols,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "\n",
    "def train_model(data_dict, learning_rate=0.01, batch_size=64, num_epochs=10, \n",
    "                scheduler_step_size=3, scheduler_gamma=0.5, missing_value=-1):\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = LFSDataset(data_dict['X_train'], data_dict['y_train'], missing_value)\n",
    "    test_dataset = LFSDataset(data_dict['X_test'], data_dict['y_test'], missing_value)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Setup model\n",
    "    input_dim = data_dict['X_train'].shape[1]\n",
    "    model = MaskedLogisticRegression(input_dim)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'test_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'test_accuracy': []\n",
    "    }\n",
    "    \n",
    "    # For confusion matrix\n",
    "    all_y_test = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Train for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            features = batch['features']\n",
    "            mask = batch['mask']\n",
    "            labels = batch['labels'].view(-1, 1)\n",
    "            \n",
    "            # Forward, backward, optimize\n",
    "            outputs = model(features, mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predicted = (outputs >= 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        epoch_y_test = []\n",
    "        epoch_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                features = batch['features']\n",
    "                mask = batch['mask']\n",
    "                labels = batch['labels'].view(-1, 1)\n",
    "                \n",
    "                outputs = model(features, mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                predicted = (outputs >= 0.5).float()\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "                \n",
    "                # Store for confusion matrix\n",
    "                epoch_y_test.extend(labels.view(-1).tolist())\n",
    "                epoch_predictions.extend(predicted.view(-1).tolist())\n",
    "        \n",
    "        # Save metrics\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        train_accuracy = correct / total\n",
    "        test_accuracy = test_correct / test_total\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['test_loss'].append(avg_test_loss)\n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "        history['test_accuracy'].append(test_accuracy)\n",
    "        \n",
    "        # Store predictions for final epoch\n",
    "        if epoch == num_epochs - 1:\n",
    "            all_y_test = epoch_y_test\n",
    "            all_predictions = epoch_predictions\n",
    "        \n",
    "        # Print each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, '\n",
    "              f'Test Loss: {avg_test_loss:.4f}, '\n",
    "              f'Train Acc: {train_accuracy:.4f}, '\n",
    "              f'Test Acc: {test_accuracy:.4f}, '\n",
    "              f'LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_y_test, all_predictions)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_y_test, all_predictions))\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'feature_names': data_dict['feature_names'],\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def analyze_model(result_dict):\n",
    "    # Get model coefficients\n",
    "    model = result_dict['model']\n",
    "    feature_names = result_dict['feature_names']\n",
    "    \n",
    "    weights = model.linear.weight.data.numpy().flatten()\n",
    "    bias = model.linear.bias.data.numpy()[0]\n",
    "    \n",
    "    # Coefficient table\n",
    "    coefficients = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': weights\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    coefficients = coefficients.reindex(coefficients['Coefficient'].abs().sort_values(ascending=False).index)\n",
    "    \n",
    "    print(\"\\nLogistic Regression Coefficients:\")\n",
    "    print(f\"Bias (Intercept): {bias:.4f}\")\n",
    "    print(coefficients)\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = result_dict.get('confusion_matrix')\n",
    "    if cm is not None:\n",
    "        print(f\"True Negative: {cm[0,0]}\")\n",
    "        print(f\"False Positive: {cm[0,1]}\")\n",
    "        print(f\"False Negative: {cm[1,0]}\")\n",
    "        print(f\"True Positive: {cm[1,1]}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if cm.shape == (2, 2):\n",
    "            accuracy = (cm[0,0] + cm[1,1]) / cm.sum()\n",
    "            if cm[1,1] + cm[0,1] > 0:\n",
    "                precision = cm[1,1] / (cm[1,1] + cm[0,1])\n",
    "            else:\n",
    "                precision = 0\n",
    "            if cm[1,1] + cm[1,0] > 0:\n",
    "                recall = cm[1,1] / (cm[1,1] + cm[1,0])\n",
    "            else:\n",
    "                recall = 0\n",
    "            if precision + recall > 0:\n",
    "                f1 = 2 * (precision * recall) / (precision + recall)\n",
    "            else:\n",
    "                f1 = 0\n",
    "                \n",
    "            print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "def predict_employment_status(lfs_data, result_dict, target_normalization=None, missing_value=-1):\n",
    "    model = result_dict['model']\n",
    "    feature_names = result_dict['feature_names']\n",
    "    \n",
    "    # Get features\n",
    "    X = lfs_data[feature_names]\n",
    "    \n",
    "    # Create mask and handle missing values\n",
    "    mask = (X != missing_value).astype(np.float32)\n",
    "    X = X.replace(missing_value, 0).astype(np.float32)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(torch.tensor(X.values), torch.tensor(mask.values))\n",
    "        binary_predictions = (predictions >= 0.5).float().numpy().flatten()\n",
    "    \n",
    "    # Map back to original values if needed\n",
    "    if target_normalization is not None and 'original_values' in target_normalization:\n",
    "        original_values = target_normalization['original_values']\n",
    "        if len(original_values) == 2:\n",
    "            value_map = {0: min(original_values), 1: max(original_values)}\n",
    "            return pd.Series([value_map[int(p)] for p in binary_predictions], index=X.index)\n",
    "    \n",
    "    return pd.Series(binary_predictions, index=X.index)\n",
    "\n",
    "def run_employment_prediction(lfs_data, target_col='PUFC11_WORK', missing_value=-1):\n",
    "    print(\"Preparing data...\")\n",
    "    data_dict = prepare_data(lfs_data, target_col=target_col, missing_value=missing_value)\n",
    "    \n",
    "    print(f\"Training on {len(data_dict['X_train'])} samples with {len(data_dict['feature_names'])} features\")\n",
    "    print(f\"Features: {data_dict['feature_names']}\")\n",
    "    \n",
    "    # Store original values for later mapping\n",
    "    original_values = sorted(list(set(lfs_data[lfs_data[target_col] != missing_value][target_col])))\n",
    "    target_normalization = {'original_values': original_values}\n",
    "    print(f\"Original target values: {original_values}\")\n",
    "    \n",
    "    result_dict = train_model(\n",
    "        data_dict,\n",
    "        learning_rate=0.01,  # Increased for faster convergence\n",
    "        batch_size=128,\n",
    "        num_epochs=10,  # Reduced to 10 epochs\n",
    "        scheduler_step_size=3,  # Adjusted for fewer epochs\n",
    "        scheduler_gamma=0.5,\n",
    "        missing_value=missing_value\n",
    "    )\n",
    "    \n",
    "    result_dict['target_normalization'] = target_normalization\n",
    "    \n",
    "    print(\"\\nModel training complete!\")\n",
    "    analyze_model(result_dict)\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "# Usage:\n",
    "# result_dict = run_employment_prediction(lfs_data, missing_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Training on 127174 samples with 9 features\n",
      "Features: ['PUFC05_AGE', 'PUFC06_MSTAT', 'PUFC04_SEX', 'PUFC07_GRADE', 'PUFC08_CURSCH', 'PUFC38_PREVJOB', 'PUFC31_FLWRK', 'PUFC30_LOOKW', 'PUFC34_WYNOT']\n",
      "Original target values: [1, 2]\n",
      "Normalized target values from [1 2] to [0, 1]\n",
      "Normalized target values from [1 2] to [0, 1]\n",
      "Epoch 1/10: Train Loss: 0.2028, Test Loss: 0.1370, Train Acc: 0.9497, Test Acc: 0.9706, LR: 0.010000\n",
      "Epoch 2/10: Train Loss: 0.1280, Test Loss: 0.1171, Train Acc: 0.9731, Test Acc: 0.9747, LR: 0.010000\n",
      "Epoch 3/10: Train Loss: 0.1181, Test Loss: 0.1121, Train Acc: 0.9746, Test Acc: 0.9746, LR: 0.005000\n",
      "Epoch 4/10: Train Loss: 0.1158, Test Loss: 0.1111, Train Acc: 0.9750, Test Acc: 0.9755, LR: 0.005000\n",
      "Epoch 5/10: Train Loss: 0.1152, Test Loss: 0.1105, Train Acc: 0.9751, Test Acc: 0.9759, LR: 0.005000\n",
      "Epoch 6/10: Train Loss: 0.1148, Test Loss: 0.1103, Train Acc: 0.9754, Test Acc: 0.9754, LR: 0.002500\n",
      "Epoch 7/10: Train Loss: 0.1148, Test Loss: 0.1100, Train Acc: 0.9754, Test Acc: 0.9755, LR: 0.002500\n",
      "Epoch 8/10: Train Loss: 0.1146, Test Loss: 0.1099, Train Acc: 0.9757, Test Acc: 0.9755, LR: 0.002500\n",
      "Epoch 9/10: Train Loss: 0.1146, Test Loss: 0.1098, Train Acc: 0.9756, Test Acc: 0.9770, LR: 0.001250\n",
      "Epoch 10/10: Train Loss: 0.1145, Test Loss: 0.1098, Train Acc: 0.9757, Test Acc: 0.9764, LR: 0.001250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13816   486]\n",
      " [  265 17227]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.97      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.98     31794\n",
      "   macro avg       0.98      0.98      0.98     31794\n",
      "weighted avg       0.98      0.98      0.98     31794\n",
      "\n",
      "\n",
      "Model training complete!\n",
      "\n",
      "Logistic Regression Coefficients:\n",
      "Bias (Intercept): -3.2563\n",
      "          Feature  Coefficient\n",
      "7    PUFC30_LOOKW     8.908339\n",
      "5  PUFC38_PREVJOB     7.570051\n",
      "8    PUFC34_WYNOT     6.869624\n",
      "0      PUFC05_AGE    -6.215552\n",
      "6    PUFC31_FLWRK     1.650136\n",
      "4   PUFC08_CURSCH    -1.318830\n",
      "2      PUFC04_SEX     0.177214\n",
      "1    PUFC06_MSTAT    -0.049853\n",
      "3    PUFC07_GRADE    -0.042361\n",
      "\n",
      "Confusion Matrix:\n",
      "True Negative: 13816\n",
      "False Positive: 486\n",
      "False Negative: 265\n",
      "True Positive: 17227\n",
      "\n",
      "Accuracy: 0.9764\n",
      "Precision: 0.9726\n",
      "Recall: 0.9849\n",
      "F1 Score: 0.9787\n"
     ]
    }
   ],
   "source": [
    "result_dict = run_employment_prediction(lfs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
