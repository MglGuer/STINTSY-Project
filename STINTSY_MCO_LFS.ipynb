{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Introduction ##\n",
    "\n",
    "In this notebook, the dataset to be processed is the Labor Force Survey conducted April 2016 and retrieved through Philippine Statistics Authority database. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# autoreload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing LFS PUF April 2016.CSV</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lfs_data = pd.read_csv(\"src/data/LFS PUF April 2016.CSV\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV file not found. Please make sure the file exists in the correct directory or provide the correct path.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Information, Pre-Processing, and Cleaning</h1>\n",
    "\n",
    "Let's get an overview of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 180862 entries, 0 to 180861\n",
      "Data columns (total 50 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   PUFREG           180862 non-null  int64  \n",
      " 1   PUFPRV           180862 non-null  int64  \n",
      " 2   PUFPRRCD         180862 non-null  int64  \n",
      " 3   PUFHHNUM         180862 non-null  int64  \n",
      " 4   PUFURB2K10       180862 non-null  int64  \n",
      " 5   PUFPWGTFIN       180862 non-null  float64\n",
      " 6   PUFSVYMO         180862 non-null  int64  \n",
      " 7   PUFSVYYR         180862 non-null  int64  \n",
      " 8   PUFPSU           180862 non-null  int64  \n",
      " 9   PUFRPL           180862 non-null  int64  \n",
      " 10  PUFHHSIZE        180862 non-null  int64  \n",
      " 11  PUFC01_LNO       180862 non-null  int64  \n",
      " 12  PUFC03_REL       180862 non-null  int64  \n",
      " 13  PUFC04_SEX       180862 non-null  int64  \n",
      " 14  PUFC05_AGE       180862 non-null  int64  \n",
      " 15  PUFC06_MSTAT     180862 non-null  object \n",
      " 16  PUFC07_GRADE     180862 non-null  object \n",
      " 17  PUFC08_CURSCH    180862 non-null  object \n",
      " 18  PUFC09_GRADTECH  180862 non-null  object \n",
      " 19  PUFC10_CONWR     180862 non-null  object \n",
      " 20  PUFC11_WORK      180862 non-null  object \n",
      " 21  PUFC12_JOB       180862 non-null  object \n",
      " 22  PUFC14_PROCC     180862 non-null  object \n",
      " 23  PUFC16_PKB       180862 non-null  object \n",
      " 24  PUFC17_NATEM     180862 non-null  object \n",
      " 25  PUFC18_PNWHRS    180862 non-null  object \n",
      " 26  PUFC19_PHOURS    180862 non-null  object \n",
      " 27  PUFC20_PWMORE    180862 non-null  object \n",
      " 28  PUFC21_PLADDW    180862 non-null  object \n",
      " 29  PUFC22_PFWRK     180862 non-null  object \n",
      " 30  PUFC23_PCLASS    180862 non-null  object \n",
      " 31  PUFC24_PBASIS    180862 non-null  object \n",
      " 32  PUFC25_PBASIC    180862 non-null  object \n",
      " 33  PUFC26_OJOB      180862 non-null  object \n",
      " 34  PUFC27_NJOBS     180862 non-null  object \n",
      " 35  PUFC28_THOURS    180862 non-null  object \n",
      " 36  PUFC29_WWM48H    180862 non-null  object \n",
      " 37  PUFC30_LOOKW     180862 non-null  object \n",
      " 38  PUFC31_FLWRK     180862 non-null  object \n",
      " 39  PUFC32_JOBSM     180862 non-null  object \n",
      " 40  PUFC33_WEEKS     180862 non-null  object \n",
      " 41  PUFC34_WYNOT     180862 non-null  object \n",
      " 42  PUFC35_LTLOOKW   180862 non-null  object \n",
      " 43  PUFC36_AVAIL     180862 non-null  object \n",
      " 44  PUFC37_WILLING   180862 non-null  object \n",
      " 45  PUFC38_PREVJOB   180862 non-null  object \n",
      " 46  PUFC40_POCC      180862 non-null  object \n",
      " 47  PUFC41_WQTR      180862 non-null  object \n",
      " 48  PUFC43_QKB       180862 non-null  object \n",
      " 49  PUFNEWEMPSTAT    180862 non-null  object \n",
      "dtypes: float64(1), int64(14), object(35)\n",
      "memory usage: 69.0+ MB\n"
     ]
    }
   ],
   "source": [
    "lfs_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Of interest to us, there are:\n",
    "<ul><li>1 contains float values, </li>\n",
    "<li>14 contain integer values, and </li>\n",
    "<li><b>35 are object values</b>.</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's check for duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfs_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicates here, and therefore no cleaning need follow in this regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems to contain null values in the form of whitespaces. Let's count those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Empty Cells:\n",
      "PUFC06_MSTAT        18339\n",
      "PUFC07_GRADE        18339\n",
      "PUFC08_CURSCH      107137\n",
      "PUFC09_GRADTECH     57782\n",
      "PUFC10_CONWR        57782\n",
      "PUFC11_WORK         21894\n",
      "PUFC12_JOB          93306\n",
      "PUFC14_PROCC       108360\n",
      "PUFC16_PKB         108360\n",
      "PUFC17_NATEM       109507\n",
      "PUFC18_PNWHRS      109507\n",
      "PUFC19_PHOURS      109507\n",
      "PUFC20_PWMORE      109507\n",
      "PUFC21_PLADDW      109507\n",
      "PUFC22_PFWRK       109507\n",
      "PUFC23_PCLASS      109507\n",
      "PUFC24_PBASIS      138947\n",
      "PUFC25_PBASIC      144274\n",
      "PUFC26_OJOB        109507\n",
      "PUFC27_NJOBS       174924\n",
      "PUFC28_THOURS      109507\n",
      "PUFC29_WWM48H      163629\n",
      "PUFC30_LOOKW       132692\n",
      "PUFC31_FLWRK       178569\n",
      "PUFC32_JOBSM       178569\n",
      "PUFC33_WEEKS       178569\n",
      "PUFC34_WYNOT       134985\n",
      "PUFC35_LTLOOKW     179269\n",
      "PUFC36_AVAIL       174893\n",
      "PUFC37_WILLING     174893\n",
      "PUFC38_PREVJOB     132692\n",
      "PUFC40_POCC        152982\n",
      "PUFC41_WQTR         81627\n",
      "PUFC43_QKB         107825\n",
      "PUFNEWEMPSTAT       61337\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "has_null = lfs_data.apply(lambda col: col.str.isspace().sum() if col.dtype == 'object' else 0)\n",
    "\n",
    "print(\"Number Empty Cells:\")\n",
    "print(has_null[has_null > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And standardize, replacing these whitespace values with -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "lfs_data.replace(r\"^\\s+$\", -1, regex=True, inplace=True)\n",
    "nan_counts_per_column = lfs_data.isna().sum()\n",
    "print(nan_counts_per_column[nan_counts_per_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these are -1, let's return to the data types, and find if our object columns from earlier are convertible to integers (or float):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safely convertable to int:\n",
      "['PUFC06_MSTAT', 'PUFC07_GRADE', 'PUFC08_CURSCH', 'PUFC09_GRADTECH', 'PUFC10_CONWR', 'PUFC11_WORK', 'PUFC12_JOB', 'PUFC14_PROCC', 'PUFC16_PKB', 'PUFC17_NATEM', 'PUFC18_PNWHRS', 'PUFC19_PHOURS', 'PUFC20_PWMORE', 'PUFC21_PLADDW', 'PUFC22_PFWRK', 'PUFC23_PCLASS', 'PUFC24_PBASIS', 'PUFC25_PBASIC', 'PUFC26_OJOB', 'PUFC27_NJOBS', 'PUFC28_THOURS', 'PUFC29_WWM48H', 'PUFC30_LOOKW', 'PUFC31_FLWRK', 'PUFC32_JOBSM', 'PUFC33_WEEKS', 'PUFC34_WYNOT', 'PUFC35_LTLOOKW', 'PUFC36_AVAIL', 'PUFC37_WILLING', 'PUFC38_PREVJOB', 'PUFC40_POCC', 'PUFC41_WQTR', 'PUFC43_QKB', 'PUFNEWEMPSTAT']\n"
     ]
    }
   ],
   "source": [
    "int_convertible_columns = []\n",
    "\n",
    "for col in lfs_data.columns:\n",
    "    if lfs_data[col].dtypes == 'object':  \n",
    "        try:\n",
    "            float_vals = lfs_data[col].dropna().astype(float)\n",
    "            if (float_vals % 1 == 0).all():\n",
    "                int_convertible_columns.append(col)\n",
    "        except ValueError:\n",
    "            pass \n",
    "\n",
    "print(\"Safely convertable to int:\")\n",
    "print(int_convertible_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And convert to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = [\n",
    "    'PUFC06_MSTAT', 'PUFC08_CURSCH', 'PUFC09_GRADTECH', 'PUFC10_CONWR', 'PUFC11_WORK', \n",
    "    'PUFC12_JOB', 'PUFC14_PROCC', 'PUFC16_PKB', 'PUFC17_NATEM', 'PUFC18_PNWHRS', \n",
    "    'PUFC19_PHOURS', 'PUFC20_PWMORE', 'PUFC21_PLADDW', 'PUFC22_PFWRK', 'PUFC23_PCLASS', \n",
    "    'PUFC24_PBASIS', 'PUFC25_PBASIC', 'PUFC26_OJOB', 'PUFC27_NJOBS', 'PUFC28_THOURS', \n",
    "    'PUFC29_WWM48H', 'PUFC30_LOOKW', 'PUFC31_FLWRK', 'PUFC32_JOBSM', 'PUFC33_WEEKS', \n",
    "    'PUFC34_WYNOT', 'PUFC35_LTLOOKW', 'PUFC36_AVAIL', 'PUFC37_WILLING', 'PUFC38_PREVJOB', \n",
    "    'PUFC40_POCC', 'PUFC41_WQTR', 'PUFC43_QKB', 'PUFNEWEMPSTAT'\n",
    "]\n",
    "\n",
    "for col in columns_to_convert:\n",
    "    lfs_data[col] = lfs_data[col].astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's also apply the unique() function to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PUFREG                17\n",
       "PUFPRV                86\n",
       "PUFPRRCD             116\n",
       "PUFHHNUM           40880\n",
       "PUFURB2K10             2\n",
       "PUFPWGTFIN         35599\n",
       "PUFSVYMO               1\n",
       "PUFSVYYR               1\n",
       "PUFPSU               975\n",
       "PUFRPL                 4\n",
       "PUFHHSIZE             20\n",
       "PUFC01_LNO            23\n",
       "PUFC03_REL            11\n",
       "PUFC04_SEX             2\n",
       "PUFC05_AGE           100\n",
       "PUFC06_MSTAT           7\n",
       "PUFC07_GRADE          68\n",
       "PUFC08_CURSCH          3\n",
       "PUFC09_GRADTECH        3\n",
       "PUFC10_CONWR           6\n",
       "PUFC11_WORK            3\n",
       "PUFC12_JOB             3\n",
       "PUFC14_PROCC          44\n",
       "PUFC16_PKB            88\n",
       "PUFC17_NATEM           4\n",
       "PUFC18_PNWHRS         17\n",
       "PUFC19_PHOURS        103\n",
       "PUFC20_PWMORE          3\n",
       "PUFC21_PLADDW          3\n",
       "PUFC22_PFWRK           3\n",
       "PUFC23_PCLASS          8\n",
       "PUFC24_PBASIS          9\n",
       "PUFC25_PBASIC       1152\n",
       "PUFC26_OJOB            3\n",
       "PUFC27_NJOBS           6\n",
       "PUFC28_THOURS        111\n",
       "PUFC29_WWM48H          6\n",
       "PUFC30_LOOKW           3\n",
       "PUFC31_FLWRK           3\n",
       "PUFC32_JOBSM           7\n",
       "PUFC33_WEEKS          36\n",
       "PUFC34_WYNOT          10\n",
       "PUFC35_LTLOOKW         4\n",
       "PUFC36_AVAIL           3\n",
       "PUFC37_WILLING         3\n",
       "PUFC38_PREVJOB         3\n",
       "PUFC40_POCC           44\n",
       "PUFC41_WQTR            3\n",
       "PUFC43_QKB            89\n",
       "PUFNEWEMPSTAT          4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfs_data.apply(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Considering our dataset has 18,000 entries, features with particularly low numbers stand out as questions that have clear, defined choices. Reviewing the [questionnaire](https://psada.psa.gov.ph/catalog/67/download/537), we find that certain questions ask the participant to specify beyond prespecified choices.\n",
    "\n",
    "This column possibly contains \"010,\" which is obviously not an integer. We ensure this column is a string, and check for values not specified in the questionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['350' '320' '250' -1 '622' '672' '240' '220' '614' '330' '010' '280'\n",
      " '632' '310' '000' '900' '820' '230' '589' '572' '210' '830' '810' '634'\n",
      " '686' '581' '681' '552' '534' '840' '658' '548' '648' '652' '662' '601'\n",
      " '642' '562' '260' '685' '631' '684' '340' '584' '621' '410' '420' '664'\n",
      " '676' '521' '638' '554' '646' '689' '522' '654' '644' '532' '531' '514'\n",
      " '558' '501' '586' '542' '576' '544' '585' '564']\n"
     ]
    }
   ],
   "source": [
    "lfs_data['PUFC07_GRADE'] = lfs_data['PUFC07_GRADE']\n",
    "valid_codes = [\n",
    "    0, 10,  # No Grade, Preschool\n",
    "    210, 220, 230, 240, 250, 260, 280,  # Elementary\n",
    "    310, 320, 330, 340, 350,  # High School\n",
    "    410, 420,  # Post Secondary; If Graduate Specify\n",
    "    810, 820, 830, 840,  # College; If Graduate Specify\n",
    "    900,  # Post Baccalaureate\n",
    "    np.nan\n",
    "]\n",
    "invalid_rows = lfs_data[~(lfs_data['PUFC07_GRADE'].isin(valid_codes))]\n",
    "\n",
    "unique_invalid_values = invalid_rows['PUFC07_GRADE'].unique()\n",
    "print(unique_invalid_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values 5XX 6XX are not detailed in the questionnaire. As it instructs the participant to specify whether they graduated from post secondary or college, we'll create a new data point to encapsulate these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700]\n"
     ]
    }
   ],
   "source": [
    "lfs_data.loc[~lfs_data['PUFC07_GRADE'].isin(valid_codes), 'PUFC07_GRADE'] = 700\n",
    "print(lfs_data['PUFC07_GRADE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strong correlations (|corr| > 0.5 and |corr| < 1):\n",
      "PUFPRV — PUFPRRCD: 1.000\n",
      "PUFREG — PUFHHNUM: 0.995\n",
      "PUFC19_PHOURS — PUFC28_THOURS: 0.972\n",
      "PUFC16_PKB — PUFC43_QKB: 0.969\n",
      "PUFC11_WORK — PUFNEWEMPSTAT: 0.964\n",
      "PUFC41_WQTR — PUFNEWEMPSTAT: 0.878\n",
      "PUFC11_WORK — PUFC41_WQTR: 0.852\n",
      "PUFC31_FLWRK — PUFC38_PREVJOB: -0.795\n",
      "PUFC36_AVAIL — PUFC37_WILLING: 0.785\n",
      "PUFC37_WILLING — PUFNEWEMPSTAT: 0.785\n",
      "PUFC18_PNWHRS — PUFC19_PHOURS: 0.785\n",
      "PUFC18_PNWHRS — PUFC28_THOURS: 0.769\n",
      "PUFPWGTFIN — PUFPSU: 0.709\n",
      "PUFC12_JOB — PUFNEWEMPSTAT: 0.704\n",
      "PUFC05_AGE — PUFC06_MSTAT: 0.701\n",
      "PUFC34_WYNOT — PUFNEWEMPSTAT: 0.631\n",
      "PUFC30_LOOKW — PUFNEWEMPSTAT: 0.625\n",
      "PUFC01_LNO — PUFC03_REL: 0.625\n",
      "PUFC05_AGE — PUFC08_CURSCH: 0.590\n",
      "PUFHHSIZE — PUFC01_LNO: 0.571\n",
      "PUFC01_LNO — PUFC05_AGE: -0.567\n",
      "PUFC20_PWMORE — PUFC21_PLADDW: 0.556\n",
      "PUFC08_CURSCH — PUFC11_WORK: -0.514\n",
      "PUFC08_CURSCH — PUFC34_WYNOT: -0.510\n",
      "PUFC08_CURSCH — PUFNEWEMPSTAT: -0.509\n"
     ]
    }
   ],
   "source": [
    "lfs_data_with_nan = lfs_data.copy()\n",
    "lfs_data_with_nan.replace(-1, np.nan, inplace=True)\n",
    "corr_matrix = lfs_data_with_nan.corr()\n",
    "\n",
    "strong_correlations = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i + 1, len(corr_matrix.columns)): \n",
    "        corr_value = corr_matrix.iloc[i, j]\n",
    "        if (0.5 < corr_value < 1) or (-1 < corr_value < -0.5):\n",
    "            strong_correlations.append((\n",
    "                corr_matrix.index[i], \n",
    "                corr_matrix.columns[j], \n",
    "                corr_value\n",
    "            ))\n",
    "\n",
    "strong_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "print(\"Strong correlations (|corr| > 0.5 and |corr| < 1):\")\n",
    "for var1, var2, corr in strong_correlations:\n",
    "    print(f\"{var1} — {var2}: {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use feature_cols as our predictor variables for PUFC11_WORK. Using Regression, we will evaluate how well we can predict whether or not someone has worked in the past week. We begin with preprocessing.\n",
    "\n",
    "### Logistic Regression\n",
    "Because this is a classification task, we can choose Logistic Regression as a baseline and a first model.\n",
    "\n",
    "We create a copy of our target variable PUFC11_WORK, and create a 80-20 train-test split for the non-empty PUFC11 data we have. We'll also standardize the values to minimize the impact of differing ranges of values on our model. We also map the values of PUFC11, normally [1,2], to [0,1]. In addition, because we've determined that our feature_cols are intentionally unanswered as appropriate, we will be treating these values as empty rather than imputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Training on 7 samples with 12 features\n",
      "Features: ['PUFC05_AGE', 'PUFC06_MSTAT', 'PUFC04_SEX', 'PUFC07_GRADE', 'PUFC08_CURSCH', 'PUFC38_PREVJOB', 'PUFC31_FLWRK', 'PUFC30_LOOKW', 'PUFC34_WYNOT']\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import prepare_data\n",
    "\n",
    "target_col = 'PUFC11_WORK'\n",
    "feature_cols = [\n",
    "    'PUFC05_AGE', 'PUFC06_MSTAT', 'PUFC04_SEX', \n",
    "    'PUFC07_GRADE', 'PUFC08_CURSCH', \n",
    "    'PUFC38_PREVJOB', 'PUFC31_FLWRK',\n",
    "    'PUFC30_LOOKW', 'PUFC34_WYNOT'\n",
    "]\n",
    "\n",
    "test_size = 0.2\n",
    "missing_value =-1\n",
    "seed = 45\n",
    "\n",
    "data_dict = prepare_data(lfs_data, target_col = target_col,\n",
    "                         test_size = test_size,\n",
    "                         missing_value = missing_value,\n",
    "                         feature_cols = feature_cols,\n",
    "                         seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "We are ready to start training. As we are setting a baseline, we'll start off with these hyperparameters. We'll use Stochastic Gradient Descent as our specified optimizer, though technically implementing mini-batch with a batch size of 128. We are looking for improvements in loss greater than 0.0001, else we stop early within three epochs. Lastly, we use a weight_decay of 0, indicating no regularization for now. All hyperparameters indicated below also indicate their default value if unspecified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss: 0.4159, Test Loss: 0.3221, Train Acc: 0.8716, Test Acc: 0.9217, LR: 0.010000\n",
      "Epoch 2/50: Train Loss: 0.2918, Test Loss: 0.2676, Train Acc: 0.9302, Test Acc: 0.9363, LR: 0.010000\n",
      "Epoch 3/50: Train Loss: 0.2530, Test Loss: 0.2391, Train Acc: 0.9416, Test Acc: 0.9433, LR: 0.010000\n",
      "Epoch 4/50: Train Loss: 0.2302, Test Loss: 0.2205, Train Acc: 0.9487, Test Acc: 0.9531, LR: 0.010000\n",
      "Epoch 5/50: Train Loss: 0.2147, Test Loss: 0.2072, Train Acc: 0.9541, Test Acc: 0.9563, LR: 0.005000\n",
      "Epoch 6/50: Train Loss: 0.2058, Test Loss: 0.2018, Train Acc: 0.9561, Test Acc: 0.9561, LR: 0.005000\n",
      "Epoch 7/50: Train Loss: 0.2007, Test Loss: 0.1970, Train Acc: 0.9572, Test Acc: 0.9589, LR: 0.005000\n",
      "Epoch 8/50: Train Loss: 0.1963, Test Loss: 0.1927, Train Acc: 0.9589, Test Acc: 0.9591, LR: 0.005000\n",
      "Epoch 9/50: Train Loss: 0.1923, Test Loss: 0.1889, Train Acc: 0.9593, Test Acc: 0.9601, LR: 0.005000\n",
      "Epoch 10/50: Train Loss: 0.1886, Test Loss: 0.1854, Train Acc: 0.9597, Test Acc: 0.9602, LR: 0.002500\n",
      "Epoch 11/50: Train Loss: 0.1861, Test Loss: 0.1837, Train Acc: 0.9608, Test Acc: 0.9622, LR: 0.002500\n",
      "Epoch 12/50: Train Loss: 0.1845, Test Loss: 0.1822, Train Acc: 0.9613, Test Acc: 0.9623, LR: 0.002500\n",
      "Epoch 13/50: Train Loss: 0.1830, Test Loss: 0.1807, Train Acc: 0.9613, Test Acc: 0.9623, LR: 0.002500\n",
      "Epoch 14/50: Train Loss: 0.1816, Test Loss: 0.1792, Train Acc: 0.9613, Test Acc: 0.9624, LR: 0.002500\n",
      "Epoch 15/50: Train Loss: 0.1802, Test Loss: 0.1779, Train Acc: 0.9617, Test Acc: 0.9628, LR: 0.001250\n",
      "Epoch 16/50: Train Loss: 0.1792, Test Loss: 0.1772, Train Acc: 0.9619, Test Acc: 0.9629, LR: 0.001250\n",
      "Epoch 17/50: Train Loss: 0.1786, Test Loss: 0.1765, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.001250\n",
      "Epoch 18/50: Train Loss: 0.1779, Test Loss: 0.1759, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.001250\n",
      "Epoch 19/50: Train Loss: 0.1773, Test Loss: 0.1753, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.001250\n",
      "Epoch 20/50: Train Loss: 0.1767, Test Loss: 0.1747, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.000625\n",
      "Epoch 21/50: Train Loss: 0.1762, Test Loss: 0.1743, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.000625\n",
      "Epoch 22/50: Train Loss: 0.1759, Test Loss: 0.1740, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.000625\n",
      "Epoch 23/50: Train Loss: 0.1757, Test Loss: 0.1737, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.000625\n",
      "Epoch 24/50: Train Loss: 0.1753, Test Loss: 0.1735, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.000625\n",
      "Epoch 25/50: Train Loss: 0.1751, Test Loss: 0.1732, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.000313\n",
      "Epoch 26/50: Train Loss: 0.1749, Test Loss: 0.1730, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.000313\n",
      "Epoch 27/50: Train Loss: 0.1747, Test Loss: 0.1729, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.000313\n",
      "Epoch 28/50: Train Loss: 0.1745, Test Loss: 0.1727, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.000313\n",
      "Epoch 29/50: Train Loss: 0.1744, Test Loss: 0.1726, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.000313\n",
      "Epoch 30/50: Train Loss: 0.1743, Test Loss: 0.1724, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.000156\n",
      "Epoch 31/50: Train Loss: 0.1742, Test Loss: 0.1724, Train Acc: 0.9621, Test Acc: 0.9629, LR: 0.000156\n",
      "Epoch 32/50: Train Loss: 0.1741, Test Loss: 0.1723, Train Acc: 0.9621, Test Acc: 0.9629, LR: 0.000156\n",
      "Epoch 33/50: Train Loss: 0.1740, Test Loss: 0.1722, Train Acc: 0.9621, Test Acc: 0.9629, LR: 0.000156\n",
      "Epoch 34/50: Train Loss: 0.1740, Test Loss: 0.1722, Train Acc: 0.9621, Test Acc: 0.9629, LR: 0.000156\n",
      "Epoch 35/50: Train Loss: 0.1739, Test Loss: 0.1721, Train Acc: 0.9621, Test Acc: 0.9629, LR: 0.000078\n",
      "Epoch 36/50: Train Loss: 0.1738, Test Loss: 0.1720, Train Acc: 0.9621, Test Acc: 0.9629, LR: 0.000078\n",
      "Epoch 37/50: Train Loss: 0.1738, Test Loss: 0.1720, Train Acc: 0.9621, Test Acc: 0.9629, LR: 0.000078\n",
      "Epoch 38/50: Train Loss: 0.1738, Test Loss: 0.1720, Train Acc: 0.9621, Test Acc: 0.9629, LR: 0.000078\n",
      "Epoch 39/50: Train Loss: 0.1737, Test Loss: 0.1719, Train Acc: 0.9621, Test Acc: 0.9629, LR: 0.000078\n",
      "Epoch 40/50: Train Loss: 0.1737, Test Loss: 0.1719, Train Acc: 0.9621, Test Acc: 0.9629, LR: 0.000039\n",
      "Epoch 41/50: Train Loss: 0.1737, Test Loss: 0.1719, Train Acc: 0.9621, Test Acc: 0.9629, LR: 0.000039\n",
      "Epoch 42/50: Train Loss: 0.1737, Test Loss: 0.1719, Train Acc: 0.9621, Test Acc: 0.9629, LR: 0.000039\n",
      "Early stopping triggered after 42 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13505   797]\n",
      " [  383 17109]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.94      0.96     14302\n",
      "         1.0       0.96      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.96     31794\n",
      "   macro avg       0.96      0.96      0.96     31794\n",
      "weighted avg       0.96      0.96      0.96     31794\n",
      "\n",
      "\n",
      "Model training complete!\n"
     ]
    }
   ],
   "source": [
    "from src.trainEval import *\n",
    "result_dict = train_model(\n",
    "    data_dict,\n",
    "    model = \"lr\",\n",
    "    \n",
    "   \n",
    "    optimizer = \"sgd\",\n",
    "    batch_size=128,\n",
    "\n",
    "    scheduler_step_size=5,\n",
    "    learning_rate=0.01,  \n",
    "    scheduler_gamma=0.5,\n",
    "    convergence_threshold=1e-4, \n",
    "    num_epochs=50,  \n",
    "    patience=3,\n",
    "    weight_decay = 0,\n",
    "    seed = seed\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"\\nModel training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error\n",
    "For both classes 0 (did not work in the past week) and 1 (did work in the past week), the initial model performed well. Precision and F1-Score are both within 1% for the ratios of class 0 to 1. The only metric of particular note is our recall, with scores of 0.94 to 0.98, indicating that we're somewhat better at avoiding false negatives for predicting those who did work rather than for those who didnt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.004641588833612777, 'scheduler_step_size': 5, 'scheduler_gamma': 0.5, 'patience': 7, 'optimizer': 'adam', 'num_epochs': 75, 'model': 'lr', 'learning_rate': 0.04832930238571752, 'batch_size': 64}\n",
      "Epoch 1/75: Train Loss: 0.1848, Test Loss: 0.1728, Train Acc: 0.9598, Test Acc: 0.9566, LR: 0.048329\n",
      "Epoch 2/75: Train Loss: 0.1778, Test Loss: 0.1732, Train Acc: 0.9626, Test Acc: 0.9644, LR: 0.048329\n",
      "Epoch 3/75: Train Loss: 0.1779, Test Loss: 0.1761, Train Acc: 0.9624, Test Acc: 0.9575, LR: 0.048329\n",
      "Epoch 4/75: Train Loss: 0.1778, Test Loss: 0.1759, Train Acc: 0.9628, Test Acc: 0.9675, LR: 0.048329\n",
      "Epoch 5/75: Train Loss: 0.1779, Test Loss: 0.1720, Train Acc: 0.9628, Test Acc: 0.9591, LR: 0.024165\n",
      "Epoch 6/75: Train Loss: 0.1772, Test Loss: 0.1749, Train Acc: 0.9635, Test Acc: 0.9659, LR: 0.024165\n",
      "Epoch 7/75: Train Loss: 0.1773, Test Loss: 0.1712, Train Acc: 0.9638, Test Acc: 0.9655, LR: 0.024165\n",
      "Epoch 8/75: Train Loss: 0.1772, Test Loss: 0.1765, Train Acc: 0.9633, Test Acc: 0.9649, LR: 0.024165\n",
      "Epoch 9/75: Train Loss: 0.1772, Test Loss: 0.1749, Train Acc: 0.9636, Test Acc: 0.9630, LR: 0.024165\n",
      "Epoch 10/75: Train Loss: 0.1772, Test Loss: 0.1751, Train Acc: 0.9635, Test Acc: 0.9639, LR: 0.012082\n",
      "Epoch 11/75: Train Loss: 0.1769, Test Loss: 0.1744, Train Acc: 0.9639, Test Acc: 0.9630, LR: 0.012082\n",
      "Epoch 12/75: Train Loss: 0.1770, Test Loss: 0.1758, Train Acc: 0.9637, Test Acc: 0.9654, LR: 0.012082\n",
      "Epoch 13/75: Train Loss: 0.1769, Test Loss: 0.1759, Train Acc: 0.9641, Test Acc: 0.9643, LR: 0.012082\n",
      "Epoch 14/75: Train Loss: 0.1770, Test Loss: 0.1741, Train Acc: 0.9640, Test Acc: 0.9642, LR: 0.012082\n",
      "Early stopping triggered after 14 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13553   749]\n",
      " [  388 17104]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.95      0.96     14302\n",
      "         1.0       0.96      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.96     31794\n",
      "   macro avg       0.97      0.96      0.96     31794\n",
      "weighted avg       0.96      0.96      0.96     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.002154434690031882, 'scheduler_step_size': 10, 'scheduler_gamma': 0.9, 'patience': 7, 'optimizer': 'sgd', 'num_epochs': 50, 'model': 'lr', 'learning_rate': 0.0006158482110660267, 'batch_size': 128}\n",
      "Epoch 1/50: Train Loss: 0.6834, Test Loss: 0.6142, Train Acc: 0.5540, Test Acc: 0.7217, LR: 0.000616\n",
      "Epoch 2/50: Train Loss: 0.5690, Test Loss: 0.5324, Train Acc: 0.7620, Test Acc: 0.8109, LR: 0.000616\n",
      "Epoch 3/50: Train Loss: 0.5061, Test Loss: 0.4842, Train Acc: 0.8485, Test Acc: 0.8833, LR: 0.000616\n",
      "Epoch 4/50: Train Loss: 0.4669, Test Loss: 0.4521, Train Acc: 0.8881, Test Acc: 0.8869, LR: 0.000616\n",
      "Epoch 5/50: Train Loss: 0.4398, Test Loss: 0.4289, Train Acc: 0.8918, Test Acc: 0.8922, LR: 0.000616\n",
      "Epoch 6/50: Train Loss: 0.4195, Test Loss: 0.4109, Train Acc: 0.8970, Test Acc: 0.8953, LR: 0.000616\n",
      "Epoch 7/50: Train Loss: 0.4033, Test Loss: 0.3963, Train Acc: 0.8994, Test Acc: 0.8992, LR: 0.000616\n",
      "Epoch 8/50: Train Loss: 0.3901, Test Loss: 0.3841, Train Acc: 0.9010, Test Acc: 0.9001, LR: 0.000616\n",
      "Epoch 9/50: Train Loss: 0.3788, Test Loss: 0.3736, Train Acc: 0.9028, Test Acc: 0.9044, LR: 0.000616\n",
      "Epoch 10/50: Train Loss: 0.3690, Test Loss: 0.3643, Train Acc: 0.9067, Test Acc: 0.9066, LR: 0.000554\n",
      "Epoch 11/50: Train Loss: 0.3607, Test Loss: 0.3569, Train Acc: 0.9093, Test Acc: 0.9094, LR: 0.000554\n",
      "Epoch 12/50: Train Loss: 0.3536, Test Loss: 0.3501, Train Acc: 0.9118, Test Acc: 0.9109, LR: 0.000554\n",
      "Epoch 13/50: Train Loss: 0.3472, Test Loss: 0.3439, Train Acc: 0.9142, Test Acc: 0.9150, LR: 0.000554\n",
      "Epoch 14/50: Train Loss: 0.3413, Test Loss: 0.3382, Train Acc: 0.9162, Test Acc: 0.9150, LR: 0.000554\n",
      "Epoch 15/50: Train Loss: 0.3358, Test Loss: 0.3329, Train Acc: 0.9173, Test Acc: 0.9178, LR: 0.000554\n",
      "Epoch 16/50: Train Loss: 0.3307, Test Loss: 0.3279, Train Acc: 0.9193, Test Acc: 0.9187, LR: 0.000554\n",
      "Epoch 17/50: Train Loss: 0.3259, Test Loss: 0.3233, Train Acc: 0.9202, Test Acc: 0.9193, LR: 0.000554\n",
      "Epoch 18/50: Train Loss: 0.3215, Test Loss: 0.3189, Train Acc: 0.9232, Test Acc: 0.9242, LR: 0.000554\n",
      "Epoch 19/50: Train Loss: 0.3173, Test Loss: 0.3149, Train Acc: 0.9259, Test Acc: 0.9248, LR: 0.000554\n",
      "Epoch 20/50: Train Loss: 0.3133, Test Loss: 0.3110, Train Acc: 0.9264, Test Acc: 0.9254, LR: 0.000499\n",
      "Epoch 21/50: Train Loss: 0.3097, Test Loss: 0.3077, Train Acc: 0.9266, Test Acc: 0.9256, LR: 0.000499\n",
      "Epoch 22/50: Train Loss: 0.3065, Test Loss: 0.3045, Train Acc: 0.9268, Test Acc: 0.9261, LR: 0.000499\n",
      "Epoch 23/50: Train Loss: 0.3035, Test Loss: 0.3015, Train Acc: 0.9274, Test Acc: 0.9268, LR: 0.000499\n",
      "Epoch 24/50: Train Loss: 0.3005, Test Loss: 0.2987, Train Acc: 0.9289, Test Acc: 0.9282, LR: 0.000499\n",
      "Epoch 25/50: Train Loss: 0.2978, Test Loss: 0.2959, Train Acc: 0.9295, Test Acc: 0.9290, LR: 0.000499\n",
      "Epoch 26/50: Train Loss: 0.2951, Test Loss: 0.2933, Train Acc: 0.9301, Test Acc: 0.9291, LR: 0.000499\n",
      "Epoch 27/50: Train Loss: 0.2925, Test Loss: 0.2907, Train Acc: 0.9303, Test Acc: 0.9293, LR: 0.000499\n",
      "Epoch 28/50: Train Loss: 0.2900, Test Loss: 0.2883, Train Acc: 0.9307, Test Acc: 0.9294, LR: 0.000499\n",
      "Epoch 29/50: Train Loss: 0.2876, Test Loss: 0.2860, Train Acc: 0.9308, Test Acc: 0.9295, LR: 0.000499\n",
      "Epoch 30/50: Train Loss: 0.2854, Test Loss: 0.2837, Train Acc: 0.9314, Test Acc: 0.9299, LR: 0.000449\n",
      "Epoch 31/50: Train Loss: 0.2833, Test Loss: 0.2817, Train Acc: 0.9316, Test Acc: 0.9314, LR: 0.000449\n",
      "Epoch 32/50: Train Loss: 0.2814, Test Loss: 0.2798, Train Acc: 0.9327, Test Acc: 0.9315, LR: 0.000449\n",
      "Epoch 33/50: Train Loss: 0.2795, Test Loss: 0.2780, Train Acc: 0.9330, Test Acc: 0.9320, LR: 0.000449\n",
      "Epoch 34/50: Train Loss: 0.2777, Test Loss: 0.2762, Train Acc: 0.9335, Test Acc: 0.9321, LR: 0.000449\n",
      "Epoch 35/50: Train Loss: 0.2759, Test Loss: 0.2745, Train Acc: 0.9339, Test Acc: 0.9328, LR: 0.000449\n",
      "Epoch 36/50: Train Loss: 0.2743, Test Loss: 0.2728, Train Acc: 0.9340, Test Acc: 0.9329, LR: 0.000449\n",
      "Epoch 37/50: Train Loss: 0.2726, Test Loss: 0.2711, Train Acc: 0.9341, Test Acc: 0.9329, LR: 0.000449\n",
      "Epoch 38/50: Train Loss: 0.2710, Test Loss: 0.2696, Train Acc: 0.9361, Test Acc: 0.9356, LR: 0.000449\n",
      "Epoch 39/50: Train Loss: 0.2695, Test Loss: 0.2680, Train Acc: 0.9371, Test Acc: 0.9364, LR: 0.000449\n",
      "Epoch 40/50: Train Loss: 0.2679, Test Loss: 0.2665, Train Acc: 0.9374, Test Acc: 0.9366, LR: 0.000404\n",
      "Epoch 41/50: Train Loss: 0.2665, Test Loss: 0.2652, Train Acc: 0.9375, Test Acc: 0.9366, LR: 0.000404\n",
      "Epoch 42/50: Train Loss: 0.2653, Test Loss: 0.2639, Train Acc: 0.9375, Test Acc: 0.9366, LR: 0.000404\n",
      "Epoch 43/50: Train Loss: 0.2640, Test Loss: 0.2626, Train Acc: 0.9377, Test Acc: 0.9374, LR: 0.000404\n",
      "Epoch 44/50: Train Loss: 0.2627, Test Loss: 0.2614, Train Acc: 0.9393, Test Acc: 0.9395, LR: 0.000404\n",
      "Epoch 45/50: Train Loss: 0.2615, Test Loss: 0.2602, Train Acc: 0.9402, Test Acc: 0.9395, LR: 0.000404\n",
      "Epoch 46/50: Train Loss: 0.2603, Test Loss: 0.2590, Train Acc: 0.9403, Test Acc: 0.9396, LR: 0.000404\n",
      "Epoch 47/50: Train Loss: 0.2592, Test Loss: 0.2579, Train Acc: 0.9404, Test Acc: 0.9397, LR: 0.000404\n",
      "Epoch 48/50: Train Loss: 0.2581, Test Loss: 0.2567, Train Acc: 0.9405, Test Acc: 0.9397, LR: 0.000404\n",
      "Epoch 49/50: Train Loss: 0.2569, Test Loss: 0.2556, Train Acc: 0.9405, Test Acc: 0.9397, LR: 0.000404\n",
      "Epoch 50/50: Train Loss: 0.2559, Test Loss: 0.2546, Train Acc: 0.9405, Test Acc: 0.9397, LR: 0.000364\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12910  1392]\n",
      " [  524 16968]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.90      0.93     14302\n",
      "         1.0       0.92      0.97      0.95     17492\n",
      "\n",
      "    accuracy                           0.94     31794\n",
      "   macro avg       0.94      0.94      0.94     31794\n",
      "weighted avg       0.94      0.94      0.94     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 4.641588833612782e-05, 'scheduler_step_size': 15, 'scheduler_gamma': 0.9, 'patience': 5, 'optimizer': 'rmsprop', 'num_epochs': 75, 'model': 'lr', 'learning_rate': 0.0001, 'batch_size': 128}\n",
      "Epoch 1/75: Train Loss: 0.6856, Test Loss: 0.6254, Train Acc: 0.5555, Test Acc: 0.7101, LR: 0.000100\n",
      "Epoch 2/75: Train Loss: 0.5791, Test Loss: 0.5391, Train Acc: 0.7238, Test Acc: 0.7553, LR: 0.000100\n",
      "Epoch 3/75: Train Loss: 0.5077, Test Loss: 0.4806, Train Acc: 0.8217, Test Acc: 0.8614, LR: 0.000100\n",
      "Epoch 4/75: Train Loss: 0.4577, Test Loss: 0.4372, Train Acc: 0.8678, Test Acc: 0.8702, LR: 0.000100\n",
      "Epoch 5/75: Train Loss: 0.4194, Test Loss: 0.4028, Train Acc: 0.8765, Test Acc: 0.8785, LR: 0.000100\n",
      "Epoch 6/75: Train Loss: 0.3882, Test Loss: 0.3744, Train Acc: 0.8856, Test Acc: 0.8922, LR: 0.000100\n",
      "Epoch 7/75: Train Loss: 0.3625, Test Loss: 0.3507, Train Acc: 0.8990, Test Acc: 0.9055, LR: 0.000100\n",
      "Epoch 8/75: Train Loss: 0.3407, Test Loss: 0.3305, Train Acc: 0.9119, Test Acc: 0.9142, LR: 0.000100\n",
      "Epoch 9/75: Train Loss: 0.3220, Test Loss: 0.3131, Train Acc: 0.9197, Test Acc: 0.9203, LR: 0.000100\n",
      "Epoch 10/75: Train Loss: 0.3058, Test Loss: 0.2978, Train Acc: 0.9239, Test Acc: 0.9237, LR: 0.000100\n",
      "Epoch 11/75: Train Loss: 0.2915, Test Loss: 0.2843, Train Acc: 0.9269, Test Acc: 0.9268, LR: 0.000100\n",
      "Epoch 12/75: Train Loss: 0.2788, Test Loss: 0.2722, Train Acc: 0.9303, Test Acc: 0.9323, LR: 0.000100\n",
      "Epoch 13/75: Train Loss: 0.2674, Test Loss: 0.2613, Train Acc: 0.9361, Test Acc: 0.9362, LR: 0.000100\n",
      "Epoch 14/75: Train Loss: 0.2571, Test Loss: 0.2514, Train Acc: 0.9377, Test Acc: 0.9371, LR: 0.000100\n",
      "Epoch 15/75: Train Loss: 0.2477, Test Loss: 0.2425, Train Acc: 0.9414, Test Acc: 0.9410, LR: 0.000090\n",
      "Epoch 16/75: Train Loss: 0.2396, Test Loss: 0.2351, Train Acc: 0.9445, Test Acc: 0.9450, LR: 0.000090\n",
      "Epoch 17/75: Train Loss: 0.2325, Test Loss: 0.2282, Train Acc: 0.9460, Test Acc: 0.9457, LR: 0.000090\n",
      "Epoch 18/75: Train Loss: 0.2259, Test Loss: 0.2218, Train Acc: 0.9484, Test Acc: 0.9486, LR: 0.000090\n",
      "Epoch 19/75: Train Loss: 0.2198, Test Loss: 0.2158, Train Acc: 0.9493, Test Acc: 0.9493, LR: 0.000090\n",
      "Epoch 20/75: Train Loss: 0.2141, Test Loss: 0.2103, Train Acc: 0.9509, Test Acc: 0.9512, LR: 0.000090\n",
      "Epoch 21/75: Train Loss: 0.2088, Test Loss: 0.2052, Train Acc: 0.9523, Test Acc: 0.9529, LR: 0.000090\n",
      "Epoch 22/75: Train Loss: 0.2039, Test Loss: 0.2003, Train Acc: 0.9534, Test Acc: 0.9531, LR: 0.000090\n",
      "Epoch 23/75: Train Loss: 0.1992, Test Loss: 0.1958, Train Acc: 0.9546, Test Acc: 0.9547, LR: 0.000090\n",
      "Epoch 24/75: Train Loss: 0.1948, Test Loss: 0.1915, Train Acc: 0.9552, Test Acc: 0.9551, LR: 0.000090\n",
      "Epoch 25/75: Train Loss: 0.1908, Test Loss: 0.1875, Train Acc: 0.9566, Test Acc: 0.9564, LR: 0.000090\n",
      "Epoch 26/75: Train Loss: 0.1870, Test Loss: 0.1838, Train Acc: 0.9571, Test Acc: 0.9570, LR: 0.000090\n",
      "Epoch 27/75: Train Loss: 0.1834, Test Loss: 0.1802, Train Acc: 0.9601, Test Acc: 0.9619, LR: 0.000090\n",
      "Epoch 28/75: Train Loss: 0.1799, Test Loss: 0.1769, Train Acc: 0.9616, Test Acc: 0.9623, LR: 0.000090\n",
      "Epoch 29/75: Train Loss: 0.1768, Test Loss: 0.1737, Train Acc: 0.9623, Test Acc: 0.9636, LR: 0.000090\n",
      "Epoch 30/75: Train Loss: 0.1738, Test Loss: 0.1707, Train Acc: 0.9633, Test Acc: 0.9645, LR: 0.000081\n",
      "Epoch 31/75: Train Loss: 0.1711, Test Loss: 0.1682, Train Acc: 0.9636, Test Acc: 0.9645, LR: 0.000081\n",
      "Epoch 32/75: Train Loss: 0.1686, Test Loss: 0.1658, Train Acc: 0.9639, Test Acc: 0.9649, LR: 0.000081\n",
      "Epoch 33/75: Train Loss: 0.1663, Test Loss: 0.1635, Train Acc: 0.9639, Test Acc: 0.9649, LR: 0.000081\n",
      "Epoch 34/75: Train Loss: 0.1641, Test Loss: 0.1613, Train Acc: 0.9641, Test Acc: 0.9651, LR: 0.000081\n",
      "Epoch 35/75: Train Loss: 0.1620, Test Loss: 0.1592, Train Acc: 0.9657, Test Acc: 0.9665, LR: 0.000081\n",
      "Epoch 36/75: Train Loss: 0.1600, Test Loss: 0.1572, Train Acc: 0.9660, Test Acc: 0.9667, LR: 0.000081\n",
      "Epoch 37/75: Train Loss: 0.1581, Test Loss: 0.1553, Train Acc: 0.9661, Test Acc: 0.9668, LR: 0.000081\n",
      "Epoch 38/75: Train Loss: 0.1563, Test Loss: 0.1535, Train Acc: 0.9662, Test Acc: 0.9668, LR: 0.000081\n",
      "Epoch 39/75: Train Loss: 0.1545, Test Loss: 0.1518, Train Acc: 0.9663, Test Acc: 0.9668, LR: 0.000081\n",
      "Epoch 40/75: Train Loss: 0.1529, Test Loss: 0.1501, Train Acc: 0.9663, Test Acc: 0.9668, LR: 0.000081\n",
      "Epoch 41/75: Train Loss: 0.1513, Test Loss: 0.1485, Train Acc: 0.9664, Test Acc: 0.9681, LR: 0.000081\n",
      "Epoch 42/75: Train Loss: 0.1499, Test Loss: 0.1470, Train Acc: 0.9681, Test Acc: 0.9681, LR: 0.000081\n",
      "Epoch 43/75: Train Loss: 0.1484, Test Loss: 0.1456, Train Acc: 0.9681, Test Acc: 0.9681, LR: 0.000081\n",
      "Epoch 44/75: Train Loss: 0.1471, Test Loss: 0.1442, Train Acc: 0.9681, Test Acc: 0.9681, LR: 0.000081\n",
      "Epoch 45/75: Train Loss: 0.1457, Test Loss: 0.1429, Train Acc: 0.9682, Test Acc: 0.9681, LR: 0.000073\n",
      "Epoch 46/75: Train Loss: 0.1445, Test Loss: 0.1418, Train Acc: 0.9683, Test Acc: 0.9683, LR: 0.000073\n",
      "Epoch 47/75: Train Loss: 0.1435, Test Loss: 0.1407, Train Acc: 0.9683, Test Acc: 0.9684, LR: 0.000073\n",
      "Epoch 48/75: Train Loss: 0.1425, Test Loss: 0.1396, Train Acc: 0.9684, Test Acc: 0.9684, LR: 0.000073\n",
      "Epoch 49/75: Train Loss: 0.1414, Test Loss: 0.1386, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000073\n",
      "Epoch 50/75: Train Loss: 0.1405, Test Loss: 0.1376, Train Acc: 0.9685, Test Acc: 0.9685, LR: 0.000073\n",
      "Epoch 51/75: Train Loss: 0.1395, Test Loss: 0.1367, Train Acc: 0.9703, Test Acc: 0.9708, LR: 0.000073\n",
      "Epoch 52/75: Train Loss: 0.1387, Test Loss: 0.1358, Train Acc: 0.9704, Test Acc: 0.9708, LR: 0.000073\n",
      "Epoch 53/75: Train Loss: 0.1380, Test Loss: 0.1350, Train Acc: 0.9709, Test Acc: 0.9713, LR: 0.000073\n",
      "Epoch 54/75: Train Loss: 0.1370, Test Loss: 0.1341, Train Acc: 0.9710, Test Acc: 0.9713, LR: 0.000073\n",
      "Epoch 55/75: Train Loss: 0.1363, Test Loss: 0.1333, Train Acc: 0.9710, Test Acc: 0.9711, LR: 0.000073\n",
      "Epoch 56/75: Train Loss: 0.1355, Test Loss: 0.1326, Train Acc: 0.9706, Test Acc: 0.9711, LR: 0.000073\n",
      "Epoch 57/75: Train Loss: 0.1348, Test Loss: 0.1318, Train Acc: 0.9706, Test Acc: 0.9711, LR: 0.000073\n",
      "Epoch 58/75: Train Loss: 0.1341, Test Loss: 0.1311, Train Acc: 0.9704, Test Acc: 0.9712, LR: 0.000073\n",
      "Epoch 59/75: Train Loss: 0.1335, Test Loss: 0.1304, Train Acc: 0.9704, Test Acc: 0.9713, LR: 0.000073\n",
      "Epoch 60/75: Train Loss: 0.1328, Test Loss: 0.1298, Train Acc: 0.9705, Test Acc: 0.9713, LR: 0.000066\n",
      "Epoch 61/75: Train Loss: 0.1322, Test Loss: 0.1292, Train Acc: 0.9706, Test Acc: 0.9713, LR: 0.000066\n",
      "Epoch 62/75: Train Loss: 0.1317, Test Loss: 0.1287, Train Acc: 0.9706, Test Acc: 0.9714, LR: 0.000066\n",
      "Epoch 63/75: Train Loss: 0.1312, Test Loss: 0.1282, Train Acc: 0.9717, Test Acc: 0.9737, LR: 0.000066\n",
      "Epoch 64/75: Train Loss: 0.1307, Test Loss: 0.1276, Train Acc: 0.9729, Test Acc: 0.9737, LR: 0.000066\n",
      "Epoch 65/75: Train Loss: 0.1302, Test Loss: 0.1271, Train Acc: 0.9729, Test Acc: 0.9737, LR: 0.000066\n",
      "Epoch 66/75: Train Loss: 0.1297, Test Loss: 0.1267, Train Acc: 0.9729, Test Acc: 0.9737, LR: 0.000066\n",
      "Epoch 67/75: Train Loss: 0.1293, Test Loss: 0.1262, Train Acc: 0.9729, Test Acc: 0.9737, LR: 0.000066\n",
      "Epoch 68/75: Train Loss: 0.1289, Test Loss: 0.1258, Train Acc: 0.9729, Test Acc: 0.9737, LR: 0.000066\n",
      "Epoch 69/75: Train Loss: 0.1284, Test Loss: 0.1253, Train Acc: 0.9729, Test Acc: 0.9737, LR: 0.000066\n",
      "Epoch 70/75: Train Loss: 0.1281, Test Loss: 0.1249, Train Acc: 0.9730, Test Acc: 0.9737, LR: 0.000066\n",
      "Epoch 71/75: Train Loss: 0.1277, Test Loss: 0.1245, Train Acc: 0.9730, Test Acc: 0.9737, LR: 0.000066\n",
      "Epoch 72/75: Train Loss: 0.1273, Test Loss: 0.1241, Train Acc: 0.9730, Test Acc: 0.9737, LR: 0.000066\n",
      "Epoch 73/75: Train Loss: 0.1269, Test Loss: 0.1237, Train Acc: 0.9730, Test Acc: 0.9737, LR: 0.000066\n",
      "Epoch 74/75: Train Loss: 0.1266, Test Loss: 0.1234, Train Acc: 0.9730, Test Acc: 0.9738, LR: 0.000066\n",
      "Epoch 75/75: Train Loss: 0.1263, Test Loss: 0.1230, Train Acc: 0.9730, Test Acc: 0.9738, LR: 0.000059\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13758   544]\n",
      " [  289 17203]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.00021544346900318823, 'scheduler_step_size': 10, 'scheduler_gamma': 0.5, 'patience': 3, 'optimizer': 'sgd', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.007847599703514606, 'batch_size': 64}\n",
      "Epoch 1/30: Train Loss: 0.3750, Test Loss: 0.2863, Train Acc: 0.8919, Test Acc: 0.9298, LR: 0.007848\n",
      "Epoch 2/30: Train Loss: 0.2586, Test Loss: 0.2366, Train Acc: 0.9395, Test Acc: 0.9473, LR: 0.007848\n",
      "Epoch 3/30: Train Loss: 0.2239, Test Loss: 0.2114, Train Acc: 0.9516, Test Acc: 0.9507, LR: 0.007848\n",
      "Epoch 4/30: Train Loss: 0.2042, Test Loss: 0.1954, Train Acc: 0.9566, Test Acc: 0.9589, LR: 0.007848\n",
      "Epoch 5/30: Train Loss: 0.1909, Test Loss: 0.1841, Train Acc: 0.9597, Test Acc: 0.9615, LR: 0.007848\n",
      "Epoch 6/30: Train Loss: 0.1812, Test Loss: 0.1755, Train Acc: 0.9615, Test Acc: 0.9629, LR: 0.007848\n",
      "Epoch 7/30: Train Loss: 0.1737, Test Loss: 0.1688, Train Acc: 0.9628, Test Acc: 0.9643, LR: 0.007848\n",
      "Epoch 8/30: Train Loss: 0.1678, Test Loss: 0.1634, Train Acc: 0.9645, Test Acc: 0.9648, LR: 0.007848\n",
      "Epoch 9/30: Train Loss: 0.1629, Test Loss: 0.1588, Train Acc: 0.9648, Test Acc: 0.9663, LR: 0.007848\n",
      "Epoch 10/30: Train Loss: 0.1588, Test Loss: 0.1550, Train Acc: 0.9667, Test Acc: 0.9668, LR: 0.003924\n",
      "Epoch 11/30: Train Loss: 0.1562, Test Loss: 0.1533, Train Acc: 0.9668, Test Acc: 0.9668, LR: 0.003924\n",
      "Epoch 12/30: Train Loss: 0.1546, Test Loss: 0.1517, Train Acc: 0.9668, Test Acc: 0.9670, LR: 0.003924\n",
      "Epoch 13/30: Train Loss: 0.1531, Test Loss: 0.1502, Train Acc: 0.9673, Test Acc: 0.9674, LR: 0.003924\n",
      "Epoch 14/30: Train Loss: 0.1518, Test Loss: 0.1489, Train Acc: 0.9675, Test Acc: 0.9677, LR: 0.003924\n",
      "Epoch 15/30: Train Loss: 0.1504, Test Loss: 0.1476, Train Acc: 0.9677, Test Acc: 0.9677, LR: 0.003924\n",
      "Epoch 16/30: Train Loss: 0.1492, Test Loss: 0.1464, Train Acc: 0.9694, Test Acc: 0.9700, LR: 0.003924\n",
      "Epoch 17/30: Train Loss: 0.1482, Test Loss: 0.1452, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.003924\n",
      "Epoch 18/30: Train Loss: 0.1470, Test Loss: 0.1442, Train Acc: 0.9697, Test Acc: 0.9701, LR: 0.003924\n",
      "Epoch 19/30: Train Loss: 0.1461, Test Loss: 0.1432, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.003924\n",
      "Epoch 20/30: Train Loss: 0.1450, Test Loss: 0.1422, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.001962\n",
      "Epoch 21/30: Train Loss: 0.1444, Test Loss: 0.1418, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.001962\n",
      "Epoch 22/30: Train Loss: 0.1439, Test Loss: 0.1413, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.001962\n",
      "Epoch 23/30: Train Loss: 0.1435, Test Loss: 0.1409, Train Acc: 0.9699, Test Acc: 0.9705, LR: 0.001962\n",
      "Epoch 24/30: Train Loss: 0.1431, Test Loss: 0.1405, Train Acc: 0.9700, Test Acc: 0.9705, LR: 0.001962\n",
      "Epoch 25/30: Train Loss: 0.1427, Test Loss: 0.1400, Train Acc: 0.9700, Test Acc: 0.9705, LR: 0.001962\n",
      "Epoch 26/30: Train Loss: 0.1428, Test Loss: 0.1396, Train Acc: 0.9700, Test Acc: 0.9705, LR: 0.001962\n",
      "Epoch 27/30: Train Loss: 0.1419, Test Loss: 0.1393, Train Acc: 0.9700, Test Acc: 0.9705, LR: 0.001962\n",
      "Epoch 28/30: Train Loss: 0.1415, Test Loss: 0.1389, Train Acc: 0.9700, Test Acc: 0.9706, LR: 0.001962\n",
      "Epoch 29/30: Train Loss: 0.1412, Test Loss: 0.1385, Train Acc: 0.9700, Test Acc: 0.9703, LR: 0.001962\n",
      "Epoch 30/30: Train Loss: 0.1408, Test Loss: 0.1381, Train Acc: 0.9700, Test Acc: 0.9704, LR: 0.000981\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13682   620]\n",
      " [  321 17171]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.004641588833612777, 'scheduler_step_size': 5, 'scheduler_gamma': 0.9, 'patience': 7, 'optimizer': 'rmsprop', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.1, 'batch_size': 256}\n",
      "Epoch 1/100: Train Loss: 0.1835, Test Loss: 0.1759, Train Acc: 0.9593, Test Acc: 0.9631, LR: 0.100000\n",
      "Epoch 2/100: Train Loss: 0.1785, Test Loss: 0.1743, Train Acc: 0.9618, Test Acc: 0.9590, LR: 0.100000\n",
      "Epoch 3/100: Train Loss: 0.1785, Test Loss: 0.1771, Train Acc: 0.9618, Test Acc: 0.9597, LR: 0.100000\n",
      "Epoch 4/100: Train Loss: 0.1784, Test Loss: 0.1772, Train Acc: 0.9620, Test Acc: 0.9642, LR: 0.100000\n",
      "Epoch 5/100: Train Loss: 0.1787, Test Loss: 0.1743, Train Acc: 0.9617, Test Acc: 0.9639, LR: 0.090000\n",
      "Epoch 6/100: Train Loss: 0.1783, Test Loss: 0.1733, Train Acc: 0.9621, Test Acc: 0.9675, LR: 0.090000\n",
      "Epoch 7/100: Train Loss: 0.1784, Test Loss: 0.1733, Train Acc: 0.9620, Test Acc: 0.9600, LR: 0.090000\n",
      "Epoch 8/100: Train Loss: 0.1781, Test Loss: 0.1813, Train Acc: 0.9621, Test Acc: 0.9535, LR: 0.090000\n",
      "Epoch 9/100: Train Loss: 0.1784, Test Loss: 0.1766, Train Acc: 0.9622, Test Acc: 0.9658, LR: 0.090000\n",
      "Epoch 10/100: Train Loss: 0.1783, Test Loss: 0.1788, Train Acc: 0.9620, Test Acc: 0.9623, LR: 0.081000\n",
      "Epoch 11/100: Train Loss: 0.1780, Test Loss: 0.1742, Train Acc: 0.9627, Test Acc: 0.9572, LR: 0.081000\n",
      "Epoch 12/100: Train Loss: 0.1781, Test Loss: 0.1746, Train Acc: 0.9621, Test Acc: 0.9638, LR: 0.081000\n",
      "Epoch 13/100: Train Loss: 0.1781, Test Loss: 0.1798, Train Acc: 0.9621, Test Acc: 0.9632, LR: 0.081000\n",
      "Early stopping triggered after 13 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13620   682]\n",
      " [  488 17004]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.95      0.96     14302\n",
      "         1.0       0.96      0.97      0.97     17492\n",
      "\n",
      "    accuracy                           0.96     31794\n",
      "   macro avg       0.96      0.96      0.96     31794\n",
      "weighted avg       0.96      0.96      0.96     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.01, 'scheduler_step_size': 15, 'scheduler_gamma': 0.9, 'patience': 7, 'optimizer': 'rmsprop', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.03359818286283781, 'batch_size': 64}\n",
      "Epoch 1/100: Train Loss: 0.2160, Test Loss: 0.2120, Train Acc: 0.9536, Test Acc: 0.9541, LR: 0.033598\n",
      "Epoch 2/100: Train Loss: 0.2134, Test Loss: 0.2102, Train Acc: 0.9544, Test Acc: 0.9562, LR: 0.033598\n",
      "Epoch 3/100: Train Loss: 0.2133, Test Loss: 0.2219, Train Acc: 0.9546, Test Acc: 0.9200, LR: 0.033598\n",
      "Epoch 4/100: Train Loss: 0.2131, Test Loss: 0.2263, Train Acc: 0.9543, Test Acc: 0.9062, LR: 0.033598\n",
      "Epoch 5/100: Train Loss: 0.2133, Test Loss: 0.2103, Train Acc: 0.9547, Test Acc: 0.9518, LR: 0.033598\n",
      "Epoch 6/100: Train Loss: 0.2133, Test Loss: 0.2196, Train Acc: 0.9540, Test Acc: 0.9684, LR: 0.033598\n",
      "Epoch 7/100: Train Loss: 0.2133, Test Loss: 0.2131, Train Acc: 0.9549, Test Acc: 0.9409, LR: 0.033598\n",
      "Epoch 8/100: Train Loss: 0.2132, Test Loss: 0.2227, Train Acc: 0.9550, Test Acc: 0.9112, LR: 0.033598\n",
      "Epoch 9/100: Train Loss: 0.2133, Test Loss: 0.2149, Train Acc: 0.9546, Test Acc: 0.9454, LR: 0.033598\n",
      "Early stopping triggered after 9 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13044  1258]\n",
      " [  478 17014]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.91      0.94     14302\n",
      "         1.0       0.93      0.97      0.95     17492\n",
      "\n",
      "    accuracy                           0.95     31794\n",
      "   macro avg       0.95      0.94      0.94     31794\n",
      "weighted avg       0.95      0.95      0.95     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.004641588833612777, 'scheduler_step_size': 10, 'scheduler_gamma': 0.5, 'patience': 5, 'optimizer': 'sgd', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.01623776739188721, 'batch_size': 64}\n",
      "Epoch 1/30: Train Loss: 0.3190, Test Loss: 0.2435, Train Acc: 0.9147, Test Acc: 0.9452, LR: 0.016238\n",
      "Epoch 2/30: Train Loss: 0.2245, Test Loss: 0.2093, Train Acc: 0.9512, Test Acc: 0.9562, LR: 0.016238\n",
      "Epoch 3/30: Train Loss: 0.2026, Test Loss: 0.1949, Train Acc: 0.9582, Test Acc: 0.9600, LR: 0.016238\n",
      "Epoch 4/30: Train Loss: 0.1923, Test Loss: 0.1873, Train Acc: 0.9605, Test Acc: 0.9619, LR: 0.016238\n",
      "Epoch 5/30: Train Loss: 0.1864, Test Loss: 0.1828, Train Acc: 0.9614, Test Acc: 0.9625, LR: 0.016238\n",
      "Epoch 6/30: Train Loss: 0.1831, Test Loss: 0.1801, Train Acc: 0.9619, Test Acc: 0.9645, LR: 0.016238\n",
      "Epoch 7/30: Train Loss: 0.1809, Test Loss: 0.1783, Train Acc: 0.9637, Test Acc: 0.9641, LR: 0.016238\n",
      "Epoch 8/30: Train Loss: 0.1794, Test Loss: 0.1772, Train Acc: 0.9637, Test Acc: 0.9640, LR: 0.016238\n",
      "Epoch 9/30: Train Loss: 0.1785, Test Loss: 0.1765, Train Acc: 0.9639, Test Acc: 0.9641, LR: 0.016238\n",
      "Epoch 10/30: Train Loss: 0.1779, Test Loss: 0.1760, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.008119\n",
      "Epoch 11/30: Train Loss: 0.1776, Test Loss: 0.1758, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.008119\n",
      "Epoch 12/30: Train Loss: 0.1774, Test Loss: 0.1756, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.008119\n",
      "Epoch 13/30: Train Loss: 0.1773, Test Loss: 0.1755, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.008119\n",
      "Epoch 14/30: Train Loss: 0.1772, Test Loss: 0.1754, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.008119\n",
      "Epoch 15/30: Train Loss: 0.1771, Test Loss: 0.1753, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.008119\n",
      "Epoch 16/30: Train Loss: 0.1770, Test Loss: 0.1753, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.008119\n",
      "Epoch 17/30: Train Loss: 0.1771, Test Loss: 0.1752, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.008119\n",
      "Epoch 18/30: Train Loss: 0.1769, Test Loss: 0.1752, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.008119\n",
      "Epoch 19/30: Train Loss: 0.1770, Test Loss: 0.1751, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.008119\n",
      "Epoch 20/30: Train Loss: 0.1768, Test Loss: 0.1751, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.004059\n",
      "Epoch 21/30: Train Loss: 0.1768, Test Loss: 0.1751, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.004059\n",
      "Epoch 22/30: Train Loss: 0.1768, Test Loss: 0.1751, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.004059\n",
      "Epoch 23/30: Train Loss: 0.1768, Test Loss: 0.1750, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.004059\n",
      "Epoch 24/30: Train Loss: 0.1768, Test Loss: 0.1750, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.004059\n",
      "Epoch 25/30: Train Loss: 0.1768, Test Loss: 0.1750, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.004059\n",
      "Early stopping triggered after 25 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13540   762]\n",
      " [  367 17125]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.95      0.96     14302\n",
      "         1.0       0.96      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.96     31794\n",
      "   macro avg       0.97      0.96      0.96     31794\n",
      "weighted avg       0.96      0.96      0.96     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 4.641588833612782e-05, 'scheduler_step_size': 15, 'scheduler_gamma': 0.9, 'patience': 7, 'optimizer': 'adam', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.00042813323987193956, 'batch_size': 256}\n",
      "Epoch 1/30: Train Loss: 0.6427, Test Loss: 0.5510, Train Acc: 0.6249, Test Acc: 0.7357, LR: 0.000428\n",
      "Epoch 2/30: Train Loss: 0.5011, Test Loss: 0.4619, Train Acc: 0.8193, Test Acc: 0.8663, LR: 0.000428\n",
      "Epoch 3/30: Train Loss: 0.4341, Test Loss: 0.4094, Train Acc: 0.8746, Test Acc: 0.8783, LR: 0.000428\n",
      "Epoch 4/30: Train Loss: 0.3894, Test Loss: 0.3706, Train Acc: 0.8889, Test Acc: 0.8941, LR: 0.000428\n",
      "Epoch 5/30: Train Loss: 0.3551, Test Loss: 0.3398, Train Acc: 0.9071, Test Acc: 0.9138, LR: 0.000428\n",
      "Epoch 6/30: Train Loss: 0.3273, Test Loss: 0.3143, Train Acc: 0.9197, Test Acc: 0.9214, LR: 0.000428\n",
      "Epoch 7/30: Train Loss: 0.3040, Test Loss: 0.2929, Train Acc: 0.9240, Test Acc: 0.9239, LR: 0.000428\n",
      "Epoch 8/30: Train Loss: 0.2843, Test Loss: 0.2744, Train Acc: 0.9308, Test Acc: 0.9327, LR: 0.000428\n",
      "Epoch 9/30: Train Loss: 0.2671, Test Loss: 0.2583, Train Acc: 0.9374, Test Acc: 0.9408, LR: 0.000428\n",
      "Epoch 10/30: Train Loss: 0.2521, Test Loss: 0.2441, Train Acc: 0.9438, Test Acc: 0.9416, LR: 0.000428\n",
      "Epoch 11/30: Train Loss: 0.2389, Test Loss: 0.2315, Train Acc: 0.9440, Test Acc: 0.9453, LR: 0.000428\n",
      "Epoch 12/30: Train Loss: 0.2270, Test Loss: 0.2202, Train Acc: 0.9471, Test Acc: 0.9479, LR: 0.000428\n",
      "Epoch 13/30: Train Loss: 0.2164, Test Loss: 0.2101, Train Acc: 0.9507, Test Acc: 0.9515, LR: 0.000428\n",
      "Epoch 14/30: Train Loss: 0.2069, Test Loss: 0.2010, Train Acc: 0.9528, Test Acc: 0.9567, LR: 0.000428\n",
      "Epoch 15/30: Train Loss: 0.1983, Test Loss: 0.1927, Train Acc: 0.9578, Test Acc: 0.9585, LR: 0.000385\n",
      "Epoch 16/30: Train Loss: 0.1908, Test Loss: 0.1859, Train Acc: 0.9591, Test Acc: 0.9600, LR: 0.000385\n",
      "Epoch 17/30: Train Loss: 0.1843, Test Loss: 0.1796, Train Acc: 0.9608, Test Acc: 0.9618, LR: 0.000385\n",
      "Epoch 18/30: Train Loss: 0.1784, Test Loss: 0.1738, Train Acc: 0.9615, Test Acc: 0.9629, LR: 0.000385\n",
      "Epoch 19/30: Train Loss: 0.1729, Test Loss: 0.1684, Train Acc: 0.9629, Test Acc: 0.9644, LR: 0.000385\n",
      "Epoch 20/30: Train Loss: 0.1679, Test Loss: 0.1636, Train Acc: 0.9644, Test Acc: 0.9649, LR: 0.000385\n",
      "Epoch 21/30: Train Loss: 0.1633, Test Loss: 0.1591, Train Acc: 0.9649, Test Acc: 0.9652, LR: 0.000385\n",
      "Epoch 22/30: Train Loss: 0.1591, Test Loss: 0.1550, Train Acc: 0.9658, Test Acc: 0.9667, LR: 0.000385\n",
      "Epoch 23/30: Train Loss: 0.1552, Test Loss: 0.1512, Train Acc: 0.9671, Test Acc: 0.9674, LR: 0.000385\n",
      "Epoch 24/30: Train Loss: 0.1517, Test Loss: 0.1477, Train Acc: 0.9674, Test Acc: 0.9676, LR: 0.000385\n",
      "Epoch 25/30: Train Loss: 0.1484, Test Loss: 0.1445, Train Acc: 0.9679, Test Acc: 0.9682, LR: 0.000385\n",
      "Epoch 26/30: Train Loss: 0.1455, Test Loss: 0.1416, Train Acc: 0.9696, Test Acc: 0.9706, LR: 0.000385\n",
      "Epoch 27/30: Train Loss: 0.1428, Test Loss: 0.1390, Train Acc: 0.9704, Test Acc: 0.9707, LR: 0.000385\n",
      "Epoch 28/30: Train Loss: 0.1403, Test Loss: 0.1365, Train Acc: 0.9703, Test Acc: 0.9706, LR: 0.000385\n",
      "Epoch 29/30: Train Loss: 0.1380, Test Loss: 0.1343, Train Acc: 0.9700, Test Acc: 0.9708, LR: 0.000385\n",
      "Epoch 30/30: Train Loss: 0.1359, Test Loss: 0.1322, Train Acc: 0.9703, Test Acc: 0.9732, LR: 0.000347\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13758   544]\n",
      " [  308 17184]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.001, 'scheduler_step_size': 10, 'scheduler_gamma': 0.9, 'patience': 3, 'optimizer': 'rmsprop', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.03359818286283781, 'batch_size': 32}\n",
      "Epoch 1/100: Train Loss: 0.1393, Test Loss: 0.1300, Train Acc: 0.9704, Test Acc: 0.9728, LR: 0.033598\n",
      "Epoch 2/100: Train Loss: 0.1344, Test Loss: 0.1305, Train Acc: 0.9724, Test Acc: 0.9738, LR: 0.033598\n",
      "Epoch 3/100: Train Loss: 0.1344, Test Loss: 0.1337, Train Acc: 0.9724, Test Acc: 0.9720, LR: 0.033598\n",
      "Epoch 4/100: Train Loss: 0.1342, Test Loss: 0.1359, Train Acc: 0.9722, Test Acc: 0.9713, LR: 0.033598\n",
      "Early stopping triggered after 4 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13622   680]\n",
      " [  232 17260]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.95      0.97     14302\n",
      "         1.0       0.96      0.99      0.97     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.002154434690031882, 'scheduler_step_size': 15, 'scheduler_gamma': 0.9, 'patience': 5, 'optimizer': 'adam', 'num_epochs': 50, 'model': 'lr', 'learning_rate': 0.00020691380811147902, 'batch_size': 128}\n",
      "Epoch 1/50: Train Loss: 0.6470, Test Loss: 0.5566, Train Acc: 0.6211, Test Acc: 0.7316, LR: 0.000207\n",
      "Epoch 2/50: Train Loss: 0.5054, Test Loss: 0.4649, Train Acc: 0.8183, Test Acc: 0.8671, LR: 0.000207\n",
      "Epoch 3/50: Train Loss: 0.4354, Test Loss: 0.4092, Train Acc: 0.8752, Test Acc: 0.8793, LR: 0.000207\n",
      "Epoch 4/50: Train Loss: 0.3876, Test Loss: 0.3674, Train Acc: 0.8899, Test Acc: 0.8995, LR: 0.000207\n",
      "Epoch 5/50: Train Loss: 0.3503, Test Loss: 0.3339, Train Acc: 0.9097, Test Acc: 0.9157, LR: 0.000207\n",
      "Epoch 6/50: Train Loss: 0.3202, Test Loss: 0.3065, Train Acc: 0.9216, Test Acc: 0.9234, LR: 0.000207\n",
      "Epoch 7/50: Train Loss: 0.2954, Test Loss: 0.2837, Train Acc: 0.9267, Test Acc: 0.9299, LR: 0.000207\n",
      "Epoch 8/50: Train Loss: 0.2747, Test Loss: 0.2646, Train Acc: 0.9347, Test Acc: 0.9399, LR: 0.000207\n",
      "Epoch 9/50: Train Loss: 0.2571, Test Loss: 0.2484, Train Acc: 0.9402, Test Acc: 0.9404, LR: 0.000207\n",
      "Epoch 10/50: Train Loss: 0.2422, Test Loss: 0.2344, Train Acc: 0.9441, Test Acc: 0.9449, LR: 0.000207\n",
      "Epoch 11/50: Train Loss: 0.2293, Test Loss: 0.2225, Train Acc: 0.9480, Test Acc: 0.9489, LR: 0.000207\n",
      "Epoch 12/50: Train Loss: 0.2182, Test Loss: 0.2120, Train Acc: 0.9502, Test Acc: 0.9518, LR: 0.000207\n",
      "Epoch 13/50: Train Loss: 0.2086, Test Loss: 0.2030, Train Acc: 0.9525, Test Acc: 0.9525, LR: 0.000207\n",
      "Epoch 14/50: Train Loss: 0.2002, Test Loss: 0.1951, Train Acc: 0.9544, Test Acc: 0.9601, LR: 0.000207\n",
      "Epoch 15/50: Train Loss: 0.1929, Test Loss: 0.1882, Train Acc: 0.9600, Test Acc: 0.9604, LR: 0.000186\n",
      "Epoch 16/50: Train Loss: 0.1869, Test Loss: 0.1828, Train Acc: 0.9605, Test Acc: 0.9615, LR: 0.000186\n",
      "Epoch 17/50: Train Loss: 0.1819, Test Loss: 0.1781, Train Acc: 0.9623, Test Acc: 0.9635, LR: 0.000186\n",
      "Epoch 18/50: Train Loss: 0.1776, Test Loss: 0.1740, Train Acc: 0.9628, Test Acc: 0.9637, LR: 0.000186\n",
      "Epoch 19/50: Train Loss: 0.1738, Test Loss: 0.1704, Train Acc: 0.9630, Test Acc: 0.9639, LR: 0.000186\n",
      "Epoch 20/50: Train Loss: 0.1706, Test Loss: 0.1674, Train Acc: 0.9642, Test Acc: 0.9653, LR: 0.000186\n",
      "Epoch 21/50: Train Loss: 0.1679, Test Loss: 0.1649, Train Acc: 0.9650, Test Acc: 0.9654, LR: 0.000186\n",
      "Epoch 22/50: Train Loss: 0.1656, Test Loss: 0.1627, Train Acc: 0.9651, Test Acc: 0.9657, LR: 0.000186\n",
      "Epoch 23/50: Train Loss: 0.1637, Test Loss: 0.1608, Train Acc: 0.9653, Test Acc: 0.9658, LR: 0.000186\n",
      "Epoch 24/50: Train Loss: 0.1620, Test Loss: 0.1592, Train Acc: 0.9654, Test Acc: 0.9659, LR: 0.000186\n",
      "Epoch 25/50: Train Loss: 0.1605, Test Loss: 0.1579, Train Acc: 0.9668, Test Acc: 0.9671, LR: 0.000186\n",
      "Epoch 26/50: Train Loss: 0.1593, Test Loss: 0.1567, Train Acc: 0.9671, Test Acc: 0.9671, LR: 0.000186\n",
      "Epoch 27/50: Train Loss: 0.1582, Test Loss: 0.1557, Train Acc: 0.9671, Test Acc: 0.9671, LR: 0.000186\n",
      "Epoch 28/50: Train Loss: 0.1573, Test Loss: 0.1548, Train Acc: 0.9671, Test Acc: 0.9671, LR: 0.000186\n",
      "Epoch 29/50: Train Loss: 0.1565, Test Loss: 0.1540, Train Acc: 0.9671, Test Acc: 0.9673, LR: 0.000186\n",
      "Epoch 30/50: Train Loss: 0.1558, Test Loss: 0.1534, Train Acc: 0.9673, Test Acc: 0.9677, LR: 0.000168\n",
      "Epoch 31/50: Train Loss: 0.1553, Test Loss: 0.1528, Train Acc: 0.9676, Test Acc: 0.9677, LR: 0.000168\n",
      "Epoch 32/50: Train Loss: 0.1547, Test Loss: 0.1524, Train Acc: 0.9677, Test Acc: 0.9677, LR: 0.000168\n",
      "Epoch 33/50: Train Loss: 0.1543, Test Loss: 0.1520, Train Acc: 0.9678, Test Acc: 0.9676, LR: 0.000168\n",
      "Epoch 34/50: Train Loss: 0.1540, Test Loss: 0.1516, Train Acc: 0.9680, Test Acc: 0.9678, LR: 0.000168\n",
      "Epoch 35/50: Train Loss: 0.1536, Test Loss: 0.1512, Train Acc: 0.9681, Test Acc: 0.9678, LR: 0.000168\n",
      "Epoch 36/50: Train Loss: 0.1533, Test Loss: 0.1509, Train Acc: 0.9696, Test Acc: 0.9700, LR: 0.000168\n",
      "Epoch 37/50: Train Loss: 0.1530, Test Loss: 0.1507, Train Acc: 0.9698, Test Acc: 0.9700, LR: 0.000168\n",
      "Epoch 38/50: Train Loss: 0.1528, Test Loss: 0.1504, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.000168\n",
      "Epoch 39/50: Train Loss: 0.1526, Test Loss: 0.1502, Train Acc: 0.9697, Test Acc: 0.9701, LR: 0.000168\n",
      "Epoch 40/50: Train Loss: 0.1524, Test Loss: 0.1500, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.000168\n",
      "Epoch 41/50: Train Loss: 0.1522, Test Loss: 0.1499, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.000168\n",
      "Epoch 42/50: Train Loss: 0.1521, Test Loss: 0.1497, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.000168\n",
      "Epoch 43/50: Train Loss: 0.1520, Test Loss: 0.1496, Train Acc: 0.9696, Test Acc: 0.9700, LR: 0.000168\n",
      "Epoch 44/50: Train Loss: 0.1518, Test Loss: 0.1495, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.000168\n",
      "Epoch 45/50: Train Loss: 0.1517, Test Loss: 0.1493, Train Acc: 0.9696, Test Acc: 0.9700, LR: 0.000151\n",
      "Epoch 46/50: Train Loss: 0.1516, Test Loss: 0.1493, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.000151\n",
      "Epoch 47/50: Train Loss: 0.1516, Test Loss: 0.1492, Train Acc: 0.9697, Test Acc: 0.9699, LR: 0.000151\n",
      "Epoch 48/50: Train Loss: 0.1515, Test Loss: 0.1491, Train Acc: 0.9697, Test Acc: 0.9699, LR: 0.000151\n",
      "Epoch 49/50: Train Loss: 0.1513, Test Loss: 0.1490, Train Acc: 0.9697, Test Acc: 0.9699, LR: 0.000151\n",
      "Epoch 50/50: Train Loss: 0.1513, Test Loss: 0.1490, Train Acc: 0.9697, Test Acc: 0.9699, LR: 0.000151\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13676   626]\n",
      " [  331 17161]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.96      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.004641588833612777, 'scheduler_step_size': 5, 'scheduler_gamma': 0.9, 'patience': 5, 'optimizer': 'adam', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.0006158482110660267, 'batch_size': 256}\n",
      "Epoch 1/100: Train Loss: 0.6071, Test Loss: 0.5045, Train Acc: 0.6717, Test Acc: 0.8313, LR: 0.000616\n",
      "Epoch 2/100: Train Loss: 0.4559, Test Loss: 0.4174, Train Acc: 0.8648, Test Acc: 0.8762, LR: 0.000616\n",
      "Epoch 3/100: Train Loss: 0.3900, Test Loss: 0.3654, Train Acc: 0.8889, Test Acc: 0.8968, LR: 0.000616\n",
      "Epoch 4/100: Train Loss: 0.3463, Test Loss: 0.3281, Train Acc: 0.9129, Test Acc: 0.9191, LR: 0.000616\n",
      "Epoch 5/100: Train Loss: 0.3140, Test Loss: 0.2997, Train Acc: 0.9238, Test Acc: 0.9246, LR: 0.000554\n",
      "Epoch 6/100: Train Loss: 0.2898, Test Loss: 0.2790, Train Acc: 0.9287, Test Acc: 0.9340, LR: 0.000554\n",
      "Epoch 7/100: Train Loss: 0.2710, Test Loss: 0.2616, Train Acc: 0.9374, Test Acc: 0.9419, LR: 0.000554\n",
      "Epoch 8/100: Train Loss: 0.2551, Test Loss: 0.2469, Train Acc: 0.9428, Test Acc: 0.9425, LR: 0.000554\n",
      "Epoch 9/100: Train Loss: 0.2416, Test Loss: 0.2343, Train Acc: 0.9444, Test Acc: 0.9463, LR: 0.000554\n",
      "Epoch 10/100: Train Loss: 0.2299, Test Loss: 0.2235, Train Acc: 0.9476, Test Acc: 0.9480, LR: 0.000499\n",
      "Epoch 11/100: Train Loss: 0.2204, Test Loss: 0.2150, Train Acc: 0.9507, Test Acc: 0.9550, LR: 0.000499\n",
      "Epoch 12/100: Train Loss: 0.2126, Test Loss: 0.2076, Train Acc: 0.9533, Test Acc: 0.9566, LR: 0.000499\n",
      "Epoch 13/100: Train Loss: 0.2057, Test Loss: 0.2012, Train Acc: 0.9574, Test Acc: 0.9580, LR: 0.000499\n",
      "Epoch 14/100: Train Loss: 0.1999, Test Loss: 0.1958, Train Acc: 0.9585, Test Acc: 0.9596, LR: 0.000499\n",
      "Epoch 15/100: Train Loss: 0.1950, Test Loss: 0.1912, Train Acc: 0.9596, Test Acc: 0.9609, LR: 0.000449\n",
      "Epoch 16/100: Train Loss: 0.1910, Test Loss: 0.1878, Train Acc: 0.9607, Test Acc: 0.9623, LR: 0.000449\n",
      "Epoch 17/100: Train Loss: 0.1880, Test Loss: 0.1850, Train Acc: 0.9613, Test Acc: 0.9623, LR: 0.000449\n",
      "Epoch 18/100: Train Loss: 0.1855, Test Loss: 0.1826, Train Acc: 0.9616, Test Acc: 0.9629, LR: 0.000449\n",
      "Epoch 19/100: Train Loss: 0.1835, Test Loss: 0.1808, Train Acc: 0.9618, Test Acc: 0.9629, LR: 0.000449\n",
      "Epoch 20/100: Train Loss: 0.1819, Test Loss: 0.1795, Train Acc: 0.9626, Test Acc: 0.9642, LR: 0.000404\n",
      "Epoch 21/100: Train Loss: 0.1808, Test Loss: 0.1785, Train Acc: 0.9637, Test Acc: 0.9643, LR: 0.000404\n",
      "Epoch 22/100: Train Loss: 0.1799, Test Loss: 0.1777, Train Acc: 0.9638, Test Acc: 0.9644, LR: 0.000404\n",
      "Epoch 23/100: Train Loss: 0.1792, Test Loss: 0.1771, Train Acc: 0.9638, Test Acc: 0.9645, LR: 0.000404\n",
      "Epoch 24/100: Train Loss: 0.1787, Test Loss: 0.1765, Train Acc: 0.9638, Test Acc: 0.9645, LR: 0.000404\n",
      "Epoch 25/100: Train Loss: 0.1782, Test Loss: 0.1761, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.000364\n",
      "Epoch 26/100: Train Loss: 0.1778, Test Loss: 0.1759, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.000364\n",
      "Epoch 27/100: Train Loss: 0.1776, Test Loss: 0.1757, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.000364\n",
      "Epoch 28/100: Train Loss: 0.1775, Test Loss: 0.1755, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.000364\n",
      "Epoch 29/100: Train Loss: 0.1773, Test Loss: 0.1753, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.000364\n",
      "Epoch 30/100: Train Loss: 0.1771, Test Loss: 0.1752, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.000327\n",
      "Epoch 31/100: Train Loss: 0.1771, Test Loss: 0.1751, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.000327\n",
      "Epoch 32/100: Train Loss: 0.1770, Test Loss: 0.1751, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.000327\n",
      "Epoch 33/100: Train Loss: 0.1769, Test Loss: 0.1750, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.000327\n",
      "Epoch 34/100: Train Loss: 0.1769, Test Loss: 0.1749, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.000327\n",
      "Epoch 35/100: Train Loss: 0.1769, Test Loss: 0.1749, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.000295\n",
      "Epoch 36/100: Train Loss: 0.1768, Test Loss: 0.1748, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.000295\n",
      "Epoch 37/100: Train Loss: 0.1768, Test Loss: 0.1748, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.000295\n",
      "Epoch 38/100: Train Loss: 0.1768, Test Loss: 0.1748, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.000295\n",
      "Epoch 39/100: Train Loss: 0.1767, Test Loss: 0.1748, Train Acc: 0.9640, Test Acc: 0.9646, LR: 0.000295\n",
      "Epoch 40/100: Train Loss: 0.1768, Test Loss: 0.1748, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.000265\n",
      "Early stopping triggered after 40 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13540   762]\n",
      " [  366 17126]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.95      0.96     14302\n",
      "         1.0       0.96      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.96     31794\n",
      "   macro avg       0.97      0.96      0.96     31794\n",
      "weighted avg       0.96      0.96      0.96     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.001, 'scheduler_step_size': 15, 'scheduler_gamma': 0.7, 'patience': 3, 'optimizer': 'rmsprop', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.005455594781168515, 'batch_size': 128}\n",
      "Epoch 1/100: Train Loss: 0.2261, Test Loss: 0.1471, Train Acc: 0.9451, Test Acc: 0.9685, LR: 0.005456\n",
      "Epoch 2/100: Train Loss: 0.1389, Test Loss: 0.1313, Train Acc: 0.9713, Test Acc: 0.9730, LR: 0.005456\n",
      "Epoch 3/100: Train Loss: 0.1339, Test Loss: 0.1305, Train Acc: 0.9728, Test Acc: 0.9735, LR: 0.005456\n",
      "Epoch 4/100: Train Loss: 0.1336, Test Loss: 0.1303, Train Acc: 0.9728, Test Acc: 0.9737, LR: 0.005456\n",
      "Epoch 5/100: Train Loss: 0.1333, Test Loss: 0.1308, Train Acc: 0.9730, Test Acc: 0.9733, LR: 0.005456\n",
      "Epoch 6/100: Train Loss: 0.1337, Test Loss: 0.1300, Train Acc: 0.9727, Test Acc: 0.9738, LR: 0.005456\n",
      "Epoch 7/100: Train Loss: 0.1336, Test Loss: 0.1298, Train Acc: 0.9729, Test Acc: 0.9731, LR: 0.005456\n",
      "Epoch 8/100: Train Loss: 0.1333, Test Loss: 0.1308, Train Acc: 0.9728, Test Acc: 0.9737, LR: 0.005456\n",
      "Epoch 9/100: Train Loss: 0.1336, Test Loss: 0.1305, Train Acc: 0.9730, Test Acc: 0.9731, LR: 0.005456\n",
      "Epoch 10/100: Train Loss: 0.1335, Test Loss: 0.1307, Train Acc: 0.9729, Test Acc: 0.9730, LR: 0.005456\n",
      "Early stopping triggered after 10 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13758   544]\n",
      " [  313 17179]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.004641588833612777, 'scheduler_step_size': 10, 'scheduler_gamma': 0.5, 'patience': 3, 'optimizer': 'sgd', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.0012742749857031334, 'batch_size': 64}\n",
      "Epoch 1/30: Train Loss: 0.5530, Test Loss: 0.4490, Train Acc: 0.7675, Test Acc: 0.8880, LR: 0.001274\n",
      "Epoch 2/30: Train Loss: 0.4106, Test Loss: 0.3821, Train Acc: 0.8976, Test Acc: 0.9020, LR: 0.001274\n",
      "Epoch 3/30: Train Loss: 0.3634, Test Loss: 0.3473, Train Acc: 0.9086, Test Acc: 0.9140, LR: 0.001274\n",
      "Epoch 4/30: Train Loss: 0.3352, Test Loss: 0.3239, Train Acc: 0.9181, Test Acc: 0.9189, LR: 0.001274\n",
      "Epoch 5/30: Train Loss: 0.3152, Test Loss: 0.3066, Train Acc: 0.9253, Test Acc: 0.9261, LR: 0.001274\n",
      "Epoch 6/30: Train Loss: 0.3000, Test Loss: 0.2930, Train Acc: 0.9281, Test Acc: 0.9293, LR: 0.001274\n",
      "Epoch 7/30: Train Loss: 0.2877, Test Loss: 0.2819, Train Acc: 0.9313, Test Acc: 0.9314, LR: 0.001274\n",
      "Epoch 8/30: Train Loss: 0.2777, Test Loss: 0.2727, Train Acc: 0.9329, Test Acc: 0.9286, LR: 0.001274\n",
      "Epoch 9/30: Train Loss: 0.2692, Test Loss: 0.2649, Train Acc: 0.9341, Test Acc: 0.9325, LR: 0.001274\n",
      "Epoch 10/30: Train Loss: 0.2620, Test Loss: 0.2581, Train Acc: 0.9370, Test Acc: 0.9356, LR: 0.000637\n",
      "Epoch 11/30: Train Loss: 0.2572, Test Loss: 0.2550, Train Acc: 0.9370, Test Acc: 0.9357, LR: 0.000637\n",
      "Epoch 12/30: Train Loss: 0.2543, Test Loss: 0.2522, Train Acc: 0.9400, Test Acc: 0.9398, LR: 0.000637\n",
      "Epoch 13/30: Train Loss: 0.2516, Test Loss: 0.2495, Train Acc: 0.9407, Test Acc: 0.9399, LR: 0.000637\n",
      "Epoch 14/30: Train Loss: 0.2490, Test Loss: 0.2470, Train Acc: 0.9413, Test Acc: 0.9405, LR: 0.000637\n",
      "Epoch 15/30: Train Loss: 0.2466, Test Loss: 0.2446, Train Acc: 0.9429, Test Acc: 0.9433, LR: 0.000637\n",
      "Epoch 16/30: Train Loss: 0.2443, Test Loss: 0.2423, Train Acc: 0.9443, Test Acc: 0.9433, LR: 0.000637\n",
      "Epoch 17/30: Train Loss: 0.2422, Test Loss: 0.2402, Train Acc: 0.9446, Test Acc: 0.9438, LR: 0.000637\n",
      "Epoch 18/30: Train Loss: 0.2400, Test Loss: 0.2381, Train Acc: 0.9449, Test Acc: 0.9440, LR: 0.000637\n",
      "Epoch 19/30: Train Loss: 0.2382, Test Loss: 0.2362, Train Acc: 0.9446, Test Acc: 0.9436, LR: 0.000637\n",
      "Epoch 20/30: Train Loss: 0.2362, Test Loss: 0.2344, Train Acc: 0.9462, Test Acc: 0.9460, LR: 0.000319\n",
      "Epoch 21/30: Train Loss: 0.2348, Test Loss: 0.2335, Train Acc: 0.9471, Test Acc: 0.9467, LR: 0.000319\n",
      "Epoch 22/30: Train Loss: 0.2339, Test Loss: 0.2326, Train Acc: 0.9473, Test Acc: 0.9467, LR: 0.000319\n",
      "Epoch 23/30: Train Loss: 0.2331, Test Loss: 0.2318, Train Acc: 0.9474, Test Acc: 0.9467, LR: 0.000319\n",
      "Epoch 24/30: Train Loss: 0.2323, Test Loss: 0.2310, Train Acc: 0.9475, Test Acc: 0.9468, LR: 0.000319\n",
      "Epoch 25/30: Train Loss: 0.2315, Test Loss: 0.2301, Train Acc: 0.9476, Test Acc: 0.9468, LR: 0.000319\n",
      "Epoch 26/30: Train Loss: 0.2309, Test Loss: 0.2294, Train Acc: 0.9476, Test Acc: 0.9468, LR: 0.000319\n",
      "Epoch 27/30: Train Loss: 0.2300, Test Loss: 0.2286, Train Acc: 0.9476, Test Acc: 0.9468, LR: 0.000319\n",
      "Epoch 28/30: Train Loss: 0.2292, Test Loss: 0.2278, Train Acc: 0.9477, Test Acc: 0.9470, LR: 0.000319\n",
      "Epoch 29/30: Train Loss: 0.2284, Test Loss: 0.2271, Train Acc: 0.9480, Test Acc: 0.9470, LR: 0.000319\n",
      "Epoch 30/30: Train Loss: 0.2277, Test Loss: 0.2264, Train Acc: 0.9480, Test Acc: 0.9470, LR: 0.000159\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13090  1212]\n",
      " [  472 17020]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.92      0.94     14302\n",
      "         1.0       0.93      0.97      0.95     17492\n",
      "\n",
      "    accuracy                           0.95     31794\n",
      "   macro avg       0.95      0.94      0.95     31794\n",
      "weighted avg       0.95      0.95      0.95     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.002154434690031882, 'scheduler_step_size': 10, 'scheduler_gamma': 0.7, 'patience': 5, 'optimizer': 'sgd', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.01623776739188721, 'batch_size': 64}\n",
      "Epoch 1/100: Train Loss: 0.3163, Test Loss: 0.2384, Train Acc: 0.9158, Test Acc: 0.9471, LR: 0.016238\n",
      "Epoch 2/100: Train Loss: 0.2176, Test Loss: 0.2005, Train Acc: 0.9530, Test Acc: 0.9590, LR: 0.016238\n",
      "Epoch 3/100: Train Loss: 0.1925, Test Loss: 0.1833, Train Acc: 0.9599, Test Acc: 0.9624, LR: 0.016238\n",
      "Epoch 4/100: Train Loss: 0.1796, Test Loss: 0.1733, Train Acc: 0.9621, Test Acc: 0.9642, LR: 0.016238\n",
      "Epoch 5/100: Train Loss: 0.1716, Test Loss: 0.1668, Train Acc: 0.9642, Test Acc: 0.9646, LR: 0.016238\n",
      "Epoch 6/100: Train Loss: 0.1664, Test Loss: 0.1624, Train Acc: 0.9649, Test Acc: 0.9663, LR: 0.016238\n",
      "Epoch 7/100: Train Loss: 0.1627, Test Loss: 0.1591, Train Acc: 0.9664, Test Acc: 0.9663, LR: 0.016238\n",
      "Epoch 8/100: Train Loss: 0.1599, Test Loss: 0.1568, Train Acc: 0.9667, Test Acc: 0.9663, LR: 0.016238\n",
      "Epoch 9/100: Train Loss: 0.1579, Test Loss: 0.1550, Train Acc: 0.9671, Test Acc: 0.9674, LR: 0.016238\n",
      "Epoch 10/100: Train Loss: 0.1564, Test Loss: 0.1536, Train Acc: 0.9673, Test Acc: 0.9674, LR: 0.011366\n",
      "Epoch 11/100: Train Loss: 0.1553, Test Loss: 0.1529, Train Acc: 0.9682, Test Acc: 0.9674, LR: 0.011366\n",
      "Epoch 12/100: Train Loss: 0.1547, Test Loss: 0.1522, Train Acc: 0.9685, Test Acc: 0.9697, LR: 0.011366\n",
      "Epoch 13/100: Train Loss: 0.1541, Test Loss: 0.1516, Train Acc: 0.9694, Test Acc: 0.9698, LR: 0.011366\n",
      "Epoch 14/100: Train Loss: 0.1536, Test Loss: 0.1512, Train Acc: 0.9694, Test Acc: 0.9699, LR: 0.011366\n",
      "Epoch 15/100: Train Loss: 0.1532, Test Loss: 0.1508, Train Acc: 0.9695, Test Acc: 0.9698, LR: 0.011366\n",
      "Epoch 16/100: Train Loss: 0.1528, Test Loss: 0.1504, Train Acc: 0.9694, Test Acc: 0.9699, LR: 0.011366\n",
      "Epoch 17/100: Train Loss: 0.1526, Test Loss: 0.1501, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.011366\n",
      "Epoch 18/100: Train Loss: 0.1522, Test Loss: 0.1499, Train Acc: 0.9695, Test Acc: 0.9699, LR: 0.011366\n",
      "Epoch 19/100: Train Loss: 0.1521, Test Loss: 0.1497, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.011366\n",
      "Epoch 20/100: Train Loss: 0.1518, Test Loss: 0.1495, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.007957\n",
      "Epoch 21/100: Train Loss: 0.1517, Test Loss: 0.1494, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.007957\n",
      "Epoch 22/100: Train Loss: 0.1515, Test Loss: 0.1493, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.007957\n",
      "Epoch 23/100: Train Loss: 0.1514, Test Loss: 0.1492, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.007957\n",
      "Epoch 24/100: Train Loss: 0.1514, Test Loss: 0.1491, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.007957\n",
      "Epoch 25/100: Train Loss: 0.1513, Test Loss: 0.1490, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.007957\n",
      "Epoch 26/100: Train Loss: 0.1517, Test Loss: 0.1490, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.007957\n",
      "Epoch 27/100: Train Loss: 0.1512, Test Loss: 0.1489, Train Acc: 0.9697, Test Acc: 0.9699, LR: 0.007957\n",
      "Epoch 28/100: Train Loss: 0.1511, Test Loss: 0.1489, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.007957\n",
      "Epoch 29/100: Train Loss: 0.1511, Test Loss: 0.1488, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.007957\n",
      "Epoch 30/100: Train Loss: 0.1510, Test Loss: 0.1488, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.005570\n",
      "Epoch 31/100: Train Loss: 0.1510, Test Loss: 0.1487, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.005570\n",
      "Epoch 32/100: Train Loss: 0.1509, Test Loss: 0.1487, Train Acc: 0.9697, Test Acc: 0.9699, LR: 0.005570\n",
      "Epoch 33/100: Train Loss: 0.1509, Test Loss: 0.1487, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.005570\n",
      "Epoch 34/100: Train Loss: 0.1509, Test Loss: 0.1487, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.005570\n",
      "Epoch 35/100: Train Loss: 0.1509, Test Loss: 0.1486, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.005570\n",
      "Epoch 36/100: Train Loss: 0.1509, Test Loss: 0.1486, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.005570\n",
      "Epoch 37/100: Train Loss: 0.1508, Test Loss: 0.1486, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.005570\n",
      "Epoch 38/100: Train Loss: 0.1509, Test Loss: 0.1486, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.005570\n",
      "Epoch 39/100: Train Loss: 0.1508, Test Loss: 0.1486, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.005570\n",
      "Epoch 40/100: Train Loss: 0.1508, Test Loss: 0.1485, Train Acc: 0.9697, Test Acc: 0.9699, LR: 0.003899\n",
      "Epoch 41/100: Train Loss: 0.1508, Test Loss: 0.1485, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.003899\n",
      "Epoch 42/100: Train Loss: 0.1508, Test Loss: 0.1485, Train Acc: 0.9697, Test Acc: 0.9699, LR: 0.003899\n",
      "Early stopping triggered after 42 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13676   626]\n",
      " [  331 17161]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.96      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.00021544346900318823, 'scheduler_step_size': 15, 'scheduler_gamma': 0.7, 'patience': 7, 'optimizer': 'rmsprop', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.06951927961775606, 'batch_size': 64}\n",
      "Epoch 1/100: Train Loss: 0.1250, Test Loss: 0.1139, Train Acc: 0.9727, Test Acc: 0.9746, LR: 0.069519\n",
      "Epoch 2/100: Train Loss: 0.1195, Test Loss: 0.1190, Train Acc: 0.9744, Test Acc: 0.9748, LR: 0.069519\n",
      "Epoch 3/100: Train Loss: 0.1195, Test Loss: 0.1168, Train Acc: 0.9745, Test Acc: 0.9746, LR: 0.069519\n",
      "Epoch 4/100: Train Loss: 0.1194, Test Loss: 0.1325, Train Acc: 0.9745, Test Acc: 0.9661, LR: 0.069519\n",
      "Epoch 5/100: Train Loss: 0.1195, Test Loss: 0.1156, Train Acc: 0.9747, Test Acc: 0.9745, LR: 0.069519\n",
      "Epoch 6/100: Train Loss: 0.1196, Test Loss: 0.1172, Train Acc: 0.9745, Test Acc: 0.9760, LR: 0.069519\n",
      "Epoch 7/100: Train Loss: 0.1196, Test Loss: 0.1139, Train Acc: 0.9746, Test Acc: 0.9750, LR: 0.069519\n",
      "Epoch 8/100: Train Loss: 0.1193, Test Loss: 0.1314, Train Acc: 0.9745, Test Acc: 0.9693, LR: 0.069519\n",
      "Early stopping triggered after 8 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13539   763]\n",
      " [  214 17278]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.95      0.97     14302\n",
      "         1.0       0.96      0.99      0.97     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.0001, 'scheduler_step_size': 5, 'scheduler_gamma': 0.9, 'patience': 7, 'optimizer': 'sgd', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.01623776739188721, 'batch_size': 128}\n",
      "Epoch 1/30: Train Loss: 0.3720, Test Loss: 0.2834, Train Acc: 0.8932, Test Acc: 0.9299, LR: 0.016238\n",
      "Epoch 2/30: Train Loss: 0.2561, Test Loss: 0.2340, Train Acc: 0.9405, Test Acc: 0.9475, LR: 0.016238\n",
      "Epoch 3/30: Train Loss: 0.2216, Test Loss: 0.2090, Train Acc: 0.9525, Test Acc: 0.9563, LR: 0.016238\n",
      "Epoch 4/30: Train Loss: 0.2019, Test Loss: 0.1931, Train Acc: 0.9573, Test Acc: 0.9591, LR: 0.016238\n",
      "Epoch 5/30: Train Loss: 0.1887, Test Loss: 0.1819, Train Acc: 0.9601, Test Acc: 0.9623, LR: 0.014614\n",
      "Epoch 6/30: Train Loss: 0.1795, Test Loss: 0.1741, Train Acc: 0.9617, Test Acc: 0.9629, LR: 0.014614\n",
      "Epoch 7/30: Train Loss: 0.1727, Test Loss: 0.1679, Train Acc: 0.9630, Test Acc: 0.9643, LR: 0.014614\n",
      "Epoch 8/30: Train Loss: 0.1672, Test Loss: 0.1628, Train Acc: 0.9645, Test Acc: 0.9649, LR: 0.014614\n",
      "Epoch 9/30: Train Loss: 0.1626, Test Loss: 0.1585, Train Acc: 0.9648, Test Acc: 0.9663, LR: 0.014614\n",
      "Epoch 10/30: Train Loss: 0.1587, Test Loss: 0.1548, Train Acc: 0.9667, Test Acc: 0.9668, LR: 0.013153\n",
      "Epoch 11/30: Train Loss: 0.1555, Test Loss: 0.1519, Train Acc: 0.9668, Test Acc: 0.9668, LR: 0.013153\n",
      "Epoch 12/30: Train Loss: 0.1528, Test Loss: 0.1493, Train Acc: 0.9673, Test Acc: 0.9676, LR: 0.013153\n",
      "Epoch 13/30: Train Loss: 0.1505, Test Loss: 0.1470, Train Acc: 0.9677, Test Acc: 0.9700, LR: 0.013153\n",
      "Epoch 14/30: Train Loss: 0.1484, Test Loss: 0.1450, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.013153\n",
      "Epoch 15/30: Train Loss: 0.1465, Test Loss: 0.1431, Train Acc: 0.9697, Test Acc: 0.9701, LR: 0.011837\n",
      "Epoch 16/30: Train Loss: 0.1448, Test Loss: 0.1416, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.011837\n",
      "Epoch 17/30: Train Loss: 0.1434, Test Loss: 0.1402, Train Acc: 0.9699, Test Acc: 0.9705, LR: 0.011837\n",
      "Epoch 18/30: Train Loss: 0.1421, Test Loss: 0.1389, Train Acc: 0.9700, Test Acc: 0.9703, LR: 0.011837\n",
      "Epoch 19/30: Train Loss: 0.1409, Test Loss: 0.1377, Train Acc: 0.9695, Test Acc: 0.9704, LR: 0.011837\n",
      "Epoch 20/30: Train Loss: 0.1398, Test Loss: 0.1366, Train Acc: 0.9700, Test Acc: 0.9727, LR: 0.010654\n",
      "Epoch 21/30: Train Loss: 0.1388, Test Loss: 0.1356, Train Acc: 0.9717, Test Acc: 0.9727, LR: 0.010654\n",
      "Epoch 22/30: Train Loss: 0.1379, Test Loss: 0.1348, Train Acc: 0.9719, Test Acc: 0.9728, LR: 0.010654\n",
      "Epoch 23/30: Train Loss: 0.1371, Test Loss: 0.1339, Train Acc: 0.9719, Test Acc: 0.9728, LR: 0.010654\n",
      "Epoch 24/30: Train Loss: 0.1363, Test Loss: 0.1332, Train Acc: 0.9720, Test Acc: 0.9728, LR: 0.010654\n",
      "Epoch 25/30: Train Loss: 0.1356, Test Loss: 0.1324, Train Acc: 0.9721, Test Acc: 0.9729, LR: 0.009588\n",
      "Epoch 26/30: Train Loss: 0.1349, Test Loss: 0.1318, Train Acc: 0.9721, Test Acc: 0.9729, LR: 0.009588\n",
      "Epoch 27/30: Train Loss: 0.1343, Test Loss: 0.1312, Train Acc: 0.9721, Test Acc: 0.9730, LR: 0.009588\n",
      "Epoch 28/30: Train Loss: 0.1337, Test Loss: 0.1306, Train Acc: 0.9721, Test Acc: 0.9730, LR: 0.009588\n",
      "Epoch 29/30: Train Loss: 0.1332, Test Loss: 0.1301, Train Acc: 0.9721, Test Acc: 0.9730, LR: 0.009588\n",
      "Epoch 30/30: Train Loss: 0.1327, Test Loss: 0.1295, Train Acc: 0.9721, Test Acc: 0.9730, LR: 0.008629\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13758   544]\n",
      " [  315 17177]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.00046415888336127773, 'scheduler_step_size': 15, 'scheduler_gamma': 0.9, 'patience': 5, 'optimizer': 'sgd', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.1, 'batch_size': 128}\n",
      "Epoch 1/30: Train Loss: 0.2361, Test Loss: 0.1740, Train Acc: 0.9442, Test Acc: 0.9632, LR: 0.100000\n",
      "Epoch 2/30: Train Loss: 0.1613, Test Loss: 0.1488, Train Acc: 0.9657, Test Acc: 0.9700, LR: 0.100000\n",
      "Epoch 3/30: Train Loss: 0.1454, Test Loss: 0.1383, Train Acc: 0.9699, Test Acc: 0.9704, LR: 0.100000\n",
      "Epoch 4/30: Train Loss: 0.1379, Test Loss: 0.1325, Train Acc: 0.9720, Test Acc: 0.9728, LR: 0.100000\n",
      "Epoch 5/30: Train Loss: 0.1335, Test Loss: 0.1290, Train Acc: 0.9723, Test Acc: 0.9730, LR: 0.100000\n",
      "Epoch 6/30: Train Loss: 0.1308, Test Loss: 0.1267, Train Acc: 0.9725, Test Acc: 0.9739, LR: 0.100000\n",
      "Epoch 7/30: Train Loss: 0.1289, Test Loss: 0.1250, Train Acc: 0.9729, Test Acc: 0.9739, LR: 0.100000\n",
      "Epoch 8/30: Train Loss: 0.1277, Test Loss: 0.1239, Train Acc: 0.9733, Test Acc: 0.9736, LR: 0.100000\n",
      "Epoch 9/30: Train Loss: 0.1267, Test Loss: 0.1230, Train Acc: 0.9736, Test Acc: 0.9740, LR: 0.100000\n",
      "Epoch 10/30: Train Loss: 0.1259, Test Loss: 0.1223, Train Acc: 0.9737, Test Acc: 0.9746, LR: 0.100000\n",
      "Epoch 11/30: Train Loss: 0.1253, Test Loss: 0.1218, Train Acc: 0.9736, Test Acc: 0.9746, LR: 0.100000\n",
      "Epoch 12/30: Train Loss: 0.1249, Test Loss: 0.1214, Train Acc: 0.9738, Test Acc: 0.9746, LR: 0.100000\n",
      "Epoch 13/30: Train Loss: 0.1246, Test Loss: 0.1211, Train Acc: 0.9737, Test Acc: 0.9741, LR: 0.100000\n",
      "Epoch 14/30: Train Loss: 0.1243, Test Loss: 0.1209, Train Acc: 0.9738, Test Acc: 0.9742, LR: 0.100000\n",
      "Epoch 15/30: Train Loss: 0.1242, Test Loss: 0.1207, Train Acc: 0.9738, Test Acc: 0.9746, LR: 0.090000\n",
      "Epoch 16/30: Train Loss: 0.1240, Test Loss: 0.1205, Train Acc: 0.9738, Test Acc: 0.9742, LR: 0.090000\n",
      "Epoch 17/30: Train Loss: 0.1239, Test Loss: 0.1204, Train Acc: 0.9739, Test Acc: 0.9746, LR: 0.090000\n",
      "Epoch 18/30: Train Loss: 0.1237, Test Loss: 0.1203, Train Acc: 0.9737, Test Acc: 0.9742, LR: 0.090000\n",
      "Epoch 19/30: Train Loss: 0.1237, Test Loss: 0.1202, Train Acc: 0.9738, Test Acc: 0.9746, LR: 0.090000\n",
      "Epoch 20/30: Train Loss: 0.1236, Test Loss: 0.1201, Train Acc: 0.9738, Test Acc: 0.9742, LR: 0.090000\n",
      "Epoch 21/30: Train Loss: 0.1235, Test Loss: 0.1201, Train Acc: 0.9739, Test Acc: 0.9741, LR: 0.090000\n",
      "Epoch 22/30: Train Loss: 0.1235, Test Loss: 0.1200, Train Acc: 0.9737, Test Acc: 0.9746, LR: 0.090000\n",
      "Epoch 23/30: Train Loss: 0.1235, Test Loss: 0.1200, Train Acc: 0.9738, Test Acc: 0.9741, LR: 0.090000\n",
      "Epoch 24/30: Train Loss: 0.1234, Test Loss: 0.1200, Train Acc: 0.9739, Test Acc: 0.9741, LR: 0.090000\n",
      "Epoch 25/30: Train Loss: 0.1234, Test Loss: 0.1200, Train Acc: 0.9738, Test Acc: 0.9746, LR: 0.090000\n",
      "Epoch 26/30: Train Loss: 0.1234, Test Loss: 0.1200, Train Acc: 0.9738, Test Acc: 0.9746, LR: 0.090000\n",
      "Early stopping triggered after 26 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13780   522]\n",
      " [  285 17207]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.98      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.0001, 'scheduler_step_size': 5, 'scheduler_gamma': 0.5, 'patience': 5, 'optimizer': 'rmsprop', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.00042813323987193956, 'batch_size': 32}\n",
      "Epoch 1/100: Train Loss: 0.3711, Test Loss: 0.2432, Train Acc: 0.8847, Test Acc: 0.9434, LR: 0.000428\n",
      "Epoch 2/100: Train Loss: 0.2025, Test Loss: 0.1729, Train Acc: 0.9550, Test Acc: 0.9632, LR: 0.000428\n",
      "Epoch 3/100: Train Loss: 0.1592, Test Loss: 0.1457, Train Acc: 0.9655, Test Acc: 0.9684, LR: 0.000428\n",
      "Epoch 4/100: Train Loss: 0.1407, Test Loss: 0.1324, Train Acc: 0.9691, Test Acc: 0.9710, LR: 0.000428\n",
      "Epoch 5/100: Train Loss: 0.1312, Test Loss: 0.1253, Train Acc: 0.9716, Test Acc: 0.9737, LR: 0.000214\n",
      "Epoch 6/100: Train Loss: 0.1270, Test Loss: 0.1229, Train Acc: 0.9730, Test Acc: 0.9738, LR: 0.000214\n",
      "Epoch 7/100: Train Loss: 0.1250, Test Loss: 0.1210, Train Acc: 0.9730, Test Acc: 0.9738, LR: 0.000214\n",
      "Epoch 8/100: Train Loss: 0.1235, Test Loss: 0.1196, Train Acc: 0.9733, Test Acc: 0.9740, LR: 0.000214\n",
      "Epoch 9/100: Train Loss: 0.1223, Test Loss: 0.1184, Train Acc: 0.9734, Test Acc: 0.9747, LR: 0.000214\n",
      "Epoch 10/100: Train Loss: 0.1213, Test Loss: 0.1175, Train Acc: 0.9739, Test Acc: 0.9744, LR: 0.000107\n",
      "Epoch 11/100: Train Loss: 0.1207, Test Loss: 0.1171, Train Acc: 0.9741, Test Acc: 0.9752, LR: 0.000107\n",
      "Epoch 12/100: Train Loss: 0.1204, Test Loss: 0.1167, Train Acc: 0.9742, Test Acc: 0.9747, LR: 0.000107\n",
      "Epoch 13/100: Train Loss: 0.1201, Test Loss: 0.1164, Train Acc: 0.9741, Test Acc: 0.9747, LR: 0.000107\n",
      "Epoch 14/100: Train Loss: 0.1198, Test Loss: 0.1161, Train Acc: 0.9744, Test Acc: 0.9747, LR: 0.000107\n",
      "Epoch 15/100: Train Loss: 0.1195, Test Loss: 0.1158, Train Acc: 0.9741, Test Acc: 0.9752, LR: 0.000054\n",
      "Epoch 16/100: Train Loss: 0.1193, Test Loss: 0.1157, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000054\n",
      "Epoch 17/100: Train Loss: 0.1193, Test Loss: 0.1155, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000054\n",
      "Epoch 18/100: Train Loss: 0.1191, Test Loss: 0.1154, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000054\n",
      "Epoch 19/100: Train Loss: 0.1190, Test Loss: 0.1153, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000054\n",
      "Epoch 20/100: Train Loss: 0.1189, Test Loss: 0.1152, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000027\n",
      "Epoch 21/100: Train Loss: 0.1188, Test Loss: 0.1151, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000027\n",
      "Epoch 22/100: Train Loss: 0.1188, Test Loss: 0.1151, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000027\n",
      "Epoch 23/100: Train Loss: 0.1187, Test Loss: 0.1150, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000027\n",
      "Epoch 24/100: Train Loss: 0.1187, Test Loss: 0.1150, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000027\n",
      "Epoch 25/100: Train Loss: 0.1186, Test Loss: 0.1149, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000013\n",
      "Epoch 26/100: Train Loss: 0.1189, Test Loss: 0.1149, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000013\n",
      "Epoch 27/100: Train Loss: 0.1186, Test Loss: 0.1149, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000013\n",
      "Epoch 28/100: Train Loss: 0.1186, Test Loss: 0.1149, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000013\n",
      "Epoch 29/100: Train Loss: 0.1185, Test Loss: 0.1148, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000013\n",
      "Epoch 30/100: Train Loss: 0.1185, Test Loss: 0.1148, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000007\n",
      "Epoch 31/100: Train Loss: 0.1185, Test Loss: 0.1148, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000007\n",
      "Epoch 32/100: Train Loss: 0.1185, Test Loss: 0.1148, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000007\n",
      "Epoch 33/100: Train Loss: 0.1185, Test Loss: 0.1148, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000007\n",
      "Early stopping triggered after 33 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13780   522]\n",
      " [  268 17224]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.98     31794\n",
      "   macro avg       0.98      0.97      0.97     31794\n",
      "weighted avg       0.98      0.98      0.98     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 1e-05, 'scheduler_step_size': 15, 'scheduler_gamma': 0.7, 'patience': 3, 'optimizer': 'rmsprop', 'num_epochs': 75, 'model': 'lr', 'learning_rate': 0.007847599703514606, 'batch_size': 32}\n",
      "Epoch 1/75: Train Loss: 0.1410, Test Loss: 0.1107, Train Acc: 0.9684, Test Acc: 0.9756, LR: 0.007848\n",
      "Epoch 2/75: Train Loss: 0.1149, Test Loss: 0.1100, Train Acc: 0.9755, Test Acc: 0.9763, LR: 0.007848\n",
      "Epoch 3/75: Train Loss: 0.1148, Test Loss: 0.1101, Train Acc: 0.9757, Test Acc: 0.9761, LR: 0.007848\n",
      "Epoch 4/75: Train Loss: 0.1148, Test Loss: 0.1105, Train Acc: 0.9757, Test Acc: 0.9762, LR: 0.007848\n",
      "Epoch 5/75: Train Loss: 0.1148, Test Loss: 0.1104, Train Acc: 0.9758, Test Acc: 0.9756, LR: 0.007848\n",
      "Early stopping triggered after 5 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13825   477]\n",
      " [  300 17192]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.97      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.98     31794\n",
      "   macro avg       0.98      0.97      0.98     31794\n",
      "weighted avg       0.98      0.98      0.98     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 2.1544346900318823e-05, 'scheduler_step_size': 15, 'scheduler_gamma': 0.9, 'patience': 7, 'optimizer': 'rmsprop', 'num_epochs': 75, 'model': 'lr', 'learning_rate': 0.0001, 'batch_size': 64}\n",
      "Epoch 1/75: Train Loss: 0.6346, Test Loss: 0.5423, Train Acc: 0.6395, Test Acc: 0.7663, LR: 0.000100\n",
      "Epoch 2/75: Train Loss: 0.4860, Test Loss: 0.4407, Train Acc: 0.8463, Test Acc: 0.8741, LR: 0.000100\n",
      "Epoch 3/75: Train Loss: 0.4075, Test Loss: 0.3789, Train Acc: 0.8833, Test Acc: 0.8890, LR: 0.000100\n",
      "Epoch 4/75: Train Loss: 0.3561, Test Loss: 0.3355, Train Acc: 0.9029, Test Acc: 0.9122, LR: 0.000100\n",
      "Epoch 5/75: Train Loss: 0.3190, Test Loss: 0.3031, Train Acc: 0.9223, Test Acc: 0.9255, LR: 0.000100\n",
      "Epoch 6/75: Train Loss: 0.2903, Test Loss: 0.2777, Train Acc: 0.9302, Test Acc: 0.9316, LR: 0.000100\n",
      "Epoch 7/75: Train Loss: 0.2676, Test Loss: 0.2571, Train Acc: 0.9362, Test Acc: 0.9401, LR: 0.000100\n",
      "Epoch 8/75: Train Loss: 0.2490, Test Loss: 0.2400, Train Acc: 0.9422, Test Acc: 0.9411, LR: 0.000100\n",
      "Epoch 9/75: Train Loss: 0.2334, Test Loss: 0.2257, Train Acc: 0.9464, Test Acc: 0.9474, LR: 0.000100\n",
      "Epoch 10/75: Train Loss: 0.2204, Test Loss: 0.2136, Train Acc: 0.9505, Test Acc: 0.9513, LR: 0.000100\n",
      "Epoch 11/75: Train Loss: 0.2093, Test Loss: 0.2032, Train Acc: 0.9531, Test Acc: 0.9545, LR: 0.000100\n",
      "Epoch 12/75: Train Loss: 0.1996, Test Loss: 0.1941, Train Acc: 0.9555, Test Acc: 0.9552, LR: 0.000100\n",
      "Epoch 13/75: Train Loss: 0.1912, Test Loss: 0.1861, Train Acc: 0.9563, Test Acc: 0.9576, LR: 0.000100\n",
      "Epoch 14/75: Train Loss: 0.1838, Test Loss: 0.1791, Train Acc: 0.9583, Test Acc: 0.9624, LR: 0.000100\n",
      "Epoch 15/75: Train Loss: 0.1773, Test Loss: 0.1729, Train Acc: 0.9622, Test Acc: 0.9634, LR: 0.000090\n",
      "Epoch 16/75: Train Loss: 0.1718, Test Loss: 0.1679, Train Acc: 0.9635, Test Acc: 0.9647, LR: 0.000090\n",
      "Epoch 17/75: Train Loss: 0.1673, Test Loss: 0.1634, Train Acc: 0.9640, Test Acc: 0.9654, LR: 0.000090\n",
      "Epoch 18/75: Train Loss: 0.1629, Test Loss: 0.1593, Train Acc: 0.9644, Test Acc: 0.9655, LR: 0.000090\n",
      "Epoch 19/75: Train Loss: 0.1593, Test Loss: 0.1557, Train Acc: 0.9659, Test Acc: 0.9669, LR: 0.000090\n",
      "Epoch 20/75: Train Loss: 0.1558, Test Loss: 0.1523, Train Acc: 0.9664, Test Acc: 0.9669, LR: 0.000090\n",
      "Epoch 21/75: Train Loss: 0.1526, Test Loss: 0.1492, Train Acc: 0.9665, Test Acc: 0.9670, LR: 0.000090\n",
      "Epoch 22/75: Train Loss: 0.1498, Test Loss: 0.1465, Train Acc: 0.9666, Test Acc: 0.9681, LR: 0.000090\n",
      "Epoch 23/75: Train Loss: 0.1472, Test Loss: 0.1439, Train Acc: 0.9683, Test Acc: 0.9683, LR: 0.000090\n",
      "Epoch 24/75: Train Loss: 0.1448, Test Loss: 0.1416, Train Acc: 0.9683, Test Acc: 0.9683, LR: 0.000090\n",
      "Epoch 25/75: Train Loss: 0.1427, Test Loss: 0.1394, Train Acc: 0.9685, Test Acc: 0.9686, LR: 0.000090\n",
      "Epoch 26/75: Train Loss: 0.1412, Test Loss: 0.1375, Train Acc: 0.9685, Test Acc: 0.9686, LR: 0.000090\n",
      "Epoch 27/75: Train Loss: 0.1390, Test Loss: 0.1357, Train Acc: 0.9689, Test Acc: 0.9710, LR: 0.000090\n",
      "Epoch 28/75: Train Loss: 0.1373, Test Loss: 0.1341, Train Acc: 0.9706, Test Acc: 0.9710, LR: 0.000090\n",
      "Epoch 29/75: Train Loss: 0.1358, Test Loss: 0.1326, Train Acc: 0.9709, Test Acc: 0.9715, LR: 0.000090\n",
      "Epoch 30/75: Train Loss: 0.1344, Test Loss: 0.1311, Train Acc: 0.9708, Test Acc: 0.9713, LR: 0.000081\n",
      "Epoch 31/75: Train Loss: 0.1331, Test Loss: 0.1300, Train Acc: 0.9705, Test Acc: 0.9713, LR: 0.000081\n",
      "Epoch 32/75: Train Loss: 0.1320, Test Loss: 0.1289, Train Acc: 0.9706, Test Acc: 0.9714, LR: 0.000081\n",
      "Epoch 33/75: Train Loss: 0.1310, Test Loss: 0.1279, Train Acc: 0.9707, Test Acc: 0.9714, LR: 0.000081\n",
      "Epoch 34/75: Train Loss: 0.1301, Test Loss: 0.1269, Train Acc: 0.9722, Test Acc: 0.9737, LR: 0.000081\n",
      "Epoch 35/75: Train Loss: 0.1292, Test Loss: 0.1260, Train Acc: 0.9730, Test Acc: 0.9737, LR: 0.000081\n",
      "Epoch 36/75: Train Loss: 0.1285, Test Loss: 0.1252, Train Acc: 0.9730, Test Acc: 0.9738, LR: 0.000081\n",
      "Epoch 37/75: Train Loss: 0.1276, Test Loss: 0.1244, Train Acc: 0.9730, Test Acc: 0.9738, LR: 0.000081\n",
      "Epoch 38/75: Train Loss: 0.1270, Test Loss: 0.1237, Train Acc: 0.9730, Test Acc: 0.9738, LR: 0.000081\n",
      "Epoch 39/75: Train Loss: 0.1263, Test Loss: 0.1230, Train Acc: 0.9731, Test Acc: 0.9738, LR: 0.000081\n",
      "Epoch 40/75: Train Loss: 0.1257, Test Loss: 0.1224, Train Acc: 0.9731, Test Acc: 0.9738, LR: 0.000081\n",
      "Epoch 41/75: Train Loss: 0.1251, Test Loss: 0.1218, Train Acc: 0.9731, Test Acc: 0.9738, LR: 0.000081\n",
      "Epoch 42/75: Train Loss: 0.1246, Test Loss: 0.1212, Train Acc: 0.9731, Test Acc: 0.9738, LR: 0.000081\n",
      "Epoch 43/75: Train Loss: 0.1240, Test Loss: 0.1207, Train Acc: 0.9731, Test Acc: 0.9738, LR: 0.000081\n",
      "Epoch 44/75: Train Loss: 0.1235, Test Loss: 0.1202, Train Acc: 0.9731, Test Acc: 0.9738, LR: 0.000081\n",
      "Epoch 45/75: Train Loss: 0.1231, Test Loss: 0.1197, Train Acc: 0.9732, Test Acc: 0.9740, LR: 0.000073\n",
      "Epoch 46/75: Train Loss: 0.1227, Test Loss: 0.1193, Train Acc: 0.9734, Test Acc: 0.9740, LR: 0.000073\n",
      "Epoch 47/75: Train Loss: 0.1224, Test Loss: 0.1189, Train Acc: 0.9734, Test Acc: 0.9740, LR: 0.000073\n",
      "Epoch 48/75: Train Loss: 0.1220, Test Loss: 0.1185, Train Acc: 0.9734, Test Acc: 0.9740, LR: 0.000073\n",
      "Epoch 49/75: Train Loss: 0.1216, Test Loss: 0.1182, Train Acc: 0.9734, Test Acc: 0.9740, LR: 0.000073\n",
      "Epoch 50/75: Train Loss: 0.1214, Test Loss: 0.1179, Train Acc: 0.9734, Test Acc: 0.9740, LR: 0.000073\n",
      "Epoch 51/75: Train Loss: 0.1210, Test Loss: 0.1176, Train Acc: 0.9734, Test Acc: 0.9740, LR: 0.000073\n",
      "Epoch 52/75: Train Loss: 0.1208, Test Loss: 0.1173, Train Acc: 0.9734, Test Acc: 0.9740, LR: 0.000073\n",
      "Epoch 53/75: Train Loss: 0.1208, Test Loss: 0.1170, Train Acc: 0.9734, Test Acc: 0.9740, LR: 0.000073\n",
      "Epoch 54/75: Train Loss: 0.1203, Test Loss: 0.1167, Train Acc: 0.9738, Test Acc: 0.9747, LR: 0.000073\n",
      "Epoch 55/75: Train Loss: 0.1200, Test Loss: 0.1165, Train Acc: 0.9740, Test Acc: 0.9747, LR: 0.000073\n",
      "Epoch 56/75: Train Loss: 0.1198, Test Loss: 0.1162, Train Acc: 0.9741, Test Acc: 0.9747, LR: 0.000073\n",
      "Epoch 57/75: Train Loss: 0.1196, Test Loss: 0.1160, Train Acc: 0.9741, Test Acc: 0.9747, LR: 0.000073\n",
      "Epoch 58/75: Train Loss: 0.1194, Test Loss: 0.1158, Train Acc: 0.9742, Test Acc: 0.9747, LR: 0.000073\n",
      "Epoch 59/75: Train Loss: 0.1192, Test Loss: 0.1156, Train Acc: 0.9746, Test Acc: 0.9747, LR: 0.000073\n",
      "Epoch 60/75: Train Loss: 0.1191, Test Loss: 0.1154, Train Acc: 0.9744, Test Acc: 0.9747, LR: 0.000066\n",
      "Epoch 61/75: Train Loss: 0.1188, Test Loss: 0.1152, Train Acc: 0.9742, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 62/75: Train Loss: 0.1188, Test Loss: 0.1150, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 63/75: Train Loss: 0.1186, Test Loss: 0.1149, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 64/75: Train Loss: 0.1184, Test Loss: 0.1147, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 65/75: Train Loss: 0.1183, Test Loss: 0.1146, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 66/75: Train Loss: 0.1182, Test Loss: 0.1145, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 67/75: Train Loss: 0.1180, Test Loss: 0.1143, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 68/75: Train Loss: 0.1179, Test Loss: 0.1142, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 69/75: Train Loss: 0.1178, Test Loss: 0.1141, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 70/75: Train Loss: 0.1177, Test Loss: 0.1139, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 71/75: Train Loss: 0.1176, Test Loss: 0.1138, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 72/75: Train Loss: 0.1175, Test Loss: 0.1137, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 73/75: Train Loss: 0.1174, Test Loss: 0.1136, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 74/75: Train Loss: 0.1173, Test Loss: 0.1135, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000066\n",
      "Epoch 75/75: Train Loss: 0.1172, Test Loss: 0.1134, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000059\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13780   522]\n",
      " [  267 17225]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.98     31794\n",
      "   macro avg       0.98      0.97      0.97     31794\n",
      "weighted avg       0.98      0.98      0.98     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 1e-05, 'scheduler_step_size': 15, 'scheduler_gamma': 0.5, 'patience': 3, 'optimizer': 'sgd', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.0001, 'batch_size': 64}\n",
      "Epoch 1/30: Train Loss: 0.7377, Test Loss: 0.7075, Train Acc: 0.4511, Test Acc: 0.4846, LR: 0.000100\n",
      "Epoch 2/30: Train Loss: 0.6814, Test Loss: 0.6573, Train Acc: 0.5370, Test Acc: 0.5986, LR: 0.000100\n",
      "Epoch 3/30: Train Loss: 0.6362, Test Loss: 0.6170, Train Acc: 0.6623, Test Acc: 0.7191, LR: 0.000100\n",
      "Epoch 4/30: Train Loss: 0.5999, Test Loss: 0.5844, Train Acc: 0.7411, Test Acc: 0.7567, LR: 0.000100\n",
      "Epoch 5/30: Train Loss: 0.5703, Test Loss: 0.5577, Train Acc: 0.7601, Test Acc: 0.7662, LR: 0.000100\n",
      "Epoch 6/30: Train Loss: 0.5459, Test Loss: 0.5355, Train Acc: 0.7753, Test Acc: 0.7971, LR: 0.000100\n",
      "Epoch 7/30: Train Loss: 0.5255, Test Loss: 0.5168, Train Acc: 0.8164, Test Acc: 0.8392, LR: 0.000100\n",
      "Epoch 8/30: Train Loss: 0.5083, Test Loss: 0.5008, Train Acc: 0.8464, Test Acc: 0.8607, LR: 0.000100\n",
      "Epoch 9/30: Train Loss: 0.4933, Test Loss: 0.4870, Train Acc: 0.8684, Test Acc: 0.8814, LR: 0.000100\n",
      "Epoch 10/30: Train Loss: 0.4804, Test Loss: 0.4749, Train Acc: 0.8841, Test Acc: 0.8892, LR: 0.000100\n",
      "Epoch 11/30: Train Loss: 0.4691, Test Loss: 0.4642, Train Acc: 0.8884, Test Acc: 0.8878, LR: 0.000100\n",
      "Epoch 12/30: Train Loss: 0.4590, Test Loss: 0.4547, Train Acc: 0.8889, Test Acc: 0.8888, LR: 0.000100\n",
      "Epoch 13/30: Train Loss: 0.4500, Test Loss: 0.4461, Train Acc: 0.8883, Test Acc: 0.8880, LR: 0.000100\n",
      "Epoch 14/30: Train Loss: 0.4419, Test Loss: 0.4383, Train Acc: 0.8917, Test Acc: 0.8910, LR: 0.000100\n",
      "Epoch 15/30: Train Loss: 0.4344, Test Loss: 0.4311, Train Acc: 0.8930, Test Acc: 0.8920, LR: 0.000050\n",
      "Epoch 16/30: Train Loss: 0.4292, Test Loss: 0.4278, Train Acc: 0.8934, Test Acc: 0.8930, LR: 0.000050\n",
      "Epoch 17/30: Train Loss: 0.4261, Test Loss: 0.4246, Train Acc: 0.8943, Test Acc: 0.8937, LR: 0.000050\n",
      "Epoch 18/30: Train Loss: 0.4228, Test Loss: 0.4215, Train Acc: 0.8973, Test Acc: 0.8965, LR: 0.000050\n",
      "Epoch 19/30: Train Loss: 0.4199, Test Loss: 0.4186, Train Acc: 0.8977, Test Acc: 0.8961, LR: 0.000050\n",
      "Epoch 20/30: Train Loss: 0.4169, Test Loss: 0.4157, Train Acc: 0.8978, Test Acc: 0.8964, LR: 0.000050\n",
      "Epoch 21/30: Train Loss: 0.4141, Test Loss: 0.4129, Train Acc: 0.8973, Test Acc: 0.8953, LR: 0.000050\n",
      "Epoch 22/30: Train Loss: 0.4114, Test Loss: 0.4103, Train Acc: 0.8972, Test Acc: 0.8953, LR: 0.000050\n",
      "Epoch 23/30: Train Loss: 0.4088, Test Loss: 0.4077, Train Acc: 0.8974, Test Acc: 0.8960, LR: 0.000050\n",
      "Epoch 24/30: Train Loss: 0.4063, Test Loss: 0.4052, Train Acc: 0.8984, Test Acc: 0.9001, LR: 0.000050\n",
      "Epoch 25/30: Train Loss: 0.4039, Test Loss: 0.4028, Train Acc: 0.9011, Test Acc: 0.8988, LR: 0.000050\n",
      "Epoch 26/30: Train Loss: 0.4015, Test Loss: 0.4004, Train Acc: 0.9001, Test Acc: 0.8988, LR: 0.000050\n",
      "Epoch 27/30: Train Loss: 0.3992, Test Loss: 0.3982, Train Acc: 0.9003, Test Acc: 0.8992, LR: 0.000050\n",
      "Epoch 28/30: Train Loss: 0.3969, Test Loss: 0.3959, Train Acc: 0.9006, Test Acc: 0.8992, LR: 0.000050\n",
      "Epoch 29/30: Train Loss: 0.3948, Test Loss: 0.3938, Train Acc: 0.9006, Test Acc: 0.8992, LR: 0.000050\n",
      "Epoch 30/30: Train Loss: 0.3927, Test Loss: 0.3917, Train Acc: 0.9007, Test Acc: 0.8994, LR: 0.000025\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12018  2284]\n",
      " [  915 16577]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.84      0.88     14302\n",
      "         1.0       0.88      0.95      0.91     17492\n",
      "\n",
      "    accuracy                           0.90     31794\n",
      "   macro avg       0.90      0.89      0.90     31794\n",
      "weighted avg       0.90      0.90      0.90     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.01, 'scheduler_step_size': 10, 'scheduler_gamma': 0.7, 'patience': 7, 'optimizer': 'rmsprop', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.0001, 'batch_size': 32}\n",
      "Epoch 1/30: Train Loss: 0.5656, Test Loss: 0.4475, Train Acc: 0.7411, Test Acc: 0.8771, LR: 0.000100\n",
      "Epoch 2/30: Train Loss: 0.3901, Test Loss: 0.3456, Train Acc: 0.8931, Test Acc: 0.9090, LR: 0.000100\n",
      "Epoch 3/30: Train Loss: 0.3166, Test Loss: 0.2921, Train Acc: 0.9262, Test Acc: 0.9314, LR: 0.000100\n",
      "Epoch 4/30: Train Loss: 0.2755, Test Loss: 0.2609, Train Acc: 0.9352, Test Acc: 0.9356, LR: 0.000100\n",
      "Epoch 5/30: Train Loss: 0.2517, Test Loss: 0.2430, Train Acc: 0.9417, Test Acc: 0.9429, LR: 0.000100\n",
      "Epoch 6/30: Train Loss: 0.2380, Test Loss: 0.2323, Train Acc: 0.9463, Test Acc: 0.9486, LR: 0.000100\n",
      "Epoch 7/30: Train Loss: 0.2297, Test Loss: 0.2257, Train Acc: 0.9497, Test Acc: 0.9480, LR: 0.000100\n",
      "Epoch 8/30: Train Loss: 0.2243, Test Loss: 0.2213, Train Acc: 0.9508, Test Acc: 0.9511, LR: 0.000100\n",
      "Epoch 9/30: Train Loss: 0.2207, Test Loss: 0.2184, Train Acc: 0.9514, Test Acc: 0.9512, LR: 0.000100\n",
      "Epoch 10/30: Train Loss: 0.2184, Test Loss: 0.2163, Train Acc: 0.9536, Test Acc: 0.9527, LR: 0.000070\n",
      "Epoch 11/30: Train Loss: 0.2169, Test Loss: 0.2153, Train Acc: 0.9548, Test Acc: 0.9569, LR: 0.000070\n",
      "Epoch 12/30: Train Loss: 0.2161, Test Loss: 0.2145, Train Acc: 0.9565, Test Acc: 0.9569, LR: 0.000070\n",
      "Epoch 13/30: Train Loss: 0.2154, Test Loss: 0.2139, Train Acc: 0.9566, Test Acc: 0.9570, LR: 0.000070\n",
      "Epoch 14/30: Train Loss: 0.2148, Test Loss: 0.2134, Train Acc: 0.9566, Test Acc: 0.9571, LR: 0.000070\n",
      "Epoch 15/30: Train Loss: 0.2144, Test Loss: 0.2130, Train Acc: 0.9566, Test Acc: 0.9571, LR: 0.000070\n",
      "Epoch 16/30: Train Loss: 0.2140, Test Loss: 0.2126, Train Acc: 0.9571, Test Acc: 0.9583, LR: 0.000070\n",
      "Epoch 17/30: Train Loss: 0.2137, Test Loss: 0.2124, Train Acc: 0.9576, Test Acc: 0.9584, LR: 0.000070\n",
      "Epoch 18/30: Train Loss: 0.2135, Test Loss: 0.2121, Train Acc: 0.9577, Test Acc: 0.9584, LR: 0.000070\n",
      "Epoch 19/30: Train Loss: 0.2133, Test Loss: 0.2119, Train Acc: 0.9577, Test Acc: 0.9584, LR: 0.000070\n",
      "Epoch 20/30: Train Loss: 0.2130, Test Loss: 0.2118, Train Acc: 0.9578, Test Acc: 0.9584, LR: 0.000049\n",
      "Epoch 21/30: Train Loss: 0.2130, Test Loss: 0.2117, Train Acc: 0.9578, Test Acc: 0.9584, LR: 0.000049\n",
      "Epoch 22/30: Train Loss: 0.2129, Test Loss: 0.2117, Train Acc: 0.9578, Test Acc: 0.9584, LR: 0.000049\n",
      "Epoch 23/30: Train Loss: 0.2129, Test Loss: 0.2116, Train Acc: 0.9578, Test Acc: 0.9585, LR: 0.000049\n",
      "Epoch 24/30: Train Loss: 0.2129, Test Loss: 0.2115, Train Acc: 0.9578, Test Acc: 0.9585, LR: 0.000049\n",
      "Epoch 25/30: Train Loss: 0.2128, Test Loss: 0.2115, Train Acc: 0.9578, Test Acc: 0.9585, LR: 0.000049\n",
      "Epoch 26/30: Train Loss: 0.2128, Test Loss: 0.2115, Train Acc: 0.9578, Test Acc: 0.9584, LR: 0.000049\n",
      "Epoch 27/30: Train Loss: 0.2127, Test Loss: 0.2115, Train Acc: 0.9578, Test Acc: 0.9585, LR: 0.000049\n",
      "Epoch 28/30: Train Loss: 0.2127, Test Loss: 0.2114, Train Acc: 0.9578, Test Acc: 0.9585, LR: 0.000049\n",
      "Epoch 29/30: Train Loss: 0.2127, Test Loss: 0.2114, Train Acc: 0.9578, Test Acc: 0.9585, LR: 0.000049\n",
      "Epoch 30/30: Train Loss: 0.2126, Test Loss: 0.2113, Train Acc: 0.9578, Test Acc: 0.9585, LR: 0.000034\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13402   900]\n",
      " [  420 17072]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.94      0.95     14302\n",
      "         1.0       0.95      0.98      0.96     17492\n",
      "\n",
      "    accuracy                           0.96     31794\n",
      "   macro avg       0.96      0.96      0.96     31794\n",
      "weighted avg       0.96      0.96      0.96     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.00046415888336127773, 'scheduler_step_size': 5, 'scheduler_gamma': 0.9, 'patience': 3, 'optimizer': 'rmsprop', 'num_epochs': 50, 'model': 'lr', 'learning_rate': 0.06951927961775606, 'batch_size': 32}\n",
      "Epoch 1/50: Train Loss: 0.1283, Test Loss: 0.1195, Train Acc: 0.9722, Test Acc: 0.9742, LR: 0.069519\n",
      "Epoch 2/50: Train Loss: 0.1253, Test Loss: 0.1213, Train Acc: 0.9736, Test Acc: 0.9745, LR: 0.069519\n",
      "Epoch 3/50: Train Loss: 0.1253, Test Loss: 0.1242, Train Acc: 0.9736, Test Acc: 0.9724, LR: 0.069519\n",
      "Epoch 4/50: Train Loss: 0.1251, Test Loss: 0.1345, Train Acc: 0.9735, Test Acc: 0.9682, LR: 0.069519\n",
      "Early stopping triggered after 4 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13512   790]\n",
      " [  222 17270]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.94      0.96     14302\n",
      "         1.0       0.96      0.99      0.97     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.002154434690031882, 'scheduler_step_size': 10, 'scheduler_gamma': 0.9, 'patience': 5, 'optimizer': 'adam', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.1, 'batch_size': 128}\n",
      "Epoch 1/100: Train Loss: 0.1604, Test Loss: 0.1457, Train Acc: 0.9646, Test Acc: 0.9670, LR: 0.100000\n",
      "Epoch 2/100: Train Loss: 0.1517, Test Loss: 0.1486, Train Acc: 0.9688, Test Acc: 0.9692, LR: 0.100000\n",
      "Epoch 3/100: Train Loss: 0.1521, Test Loss: 0.1487, Train Acc: 0.9682, Test Acc: 0.9680, LR: 0.100000\n",
      "Epoch 4/100: Train Loss: 0.1518, Test Loss: 0.1487, Train Acc: 0.9683, Test Acc: 0.9736, LR: 0.100000\n",
      "Epoch 5/100: Train Loss: 0.1520, Test Loss: 0.1444, Train Acc: 0.9687, Test Acc: 0.9696, LR: 0.100000\n",
      "Epoch 6/100: Train Loss: 0.1520, Test Loss: 0.1497, Train Acc: 0.9688, Test Acc: 0.9704, LR: 0.100000\n",
      "Epoch 7/100: Train Loss: 0.1519, Test Loss: 0.1442, Train Acc: 0.9687, Test Acc: 0.9683, LR: 0.100000\n",
      "Epoch 8/100: Train Loss: 0.1520, Test Loss: 0.1507, Train Acc: 0.9684, Test Acc: 0.9699, LR: 0.100000\n",
      "Epoch 9/100: Train Loss: 0.1520, Test Loss: 0.1475, Train Acc: 0.9685, Test Acc: 0.9692, LR: 0.100000\n",
      "Epoch 10/100: Train Loss: 0.1518, Test Loss: 0.1485, Train Acc: 0.9687, Test Acc: 0.9690, LR: 0.090000\n",
      "Epoch 11/100: Train Loss: 0.1518, Test Loss: 0.1480, Train Acc: 0.9688, Test Acc: 0.9667, LR: 0.090000\n",
      "Epoch 12/100: Train Loss: 0.1519, Test Loss: 0.1489, Train Acc: 0.9685, Test Acc: 0.9734, LR: 0.090000\n",
      "Early stopping triggered after 12 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13813   489]\n",
      " [  356 17136]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.00046415888336127773, 'scheduler_step_size': 5, 'scheduler_gamma': 0.7, 'patience': 5, 'optimizer': 'sgd', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.0001, 'batch_size': 64}\n",
      "Epoch 1/30: Train Loss: 0.7377, Test Loss: 0.7075, Train Acc: 0.4511, Test Acc: 0.4846, LR: 0.000100\n",
      "Epoch 2/30: Train Loss: 0.6814, Test Loss: 0.6573, Train Acc: 0.5370, Test Acc: 0.5986, LR: 0.000100\n",
      "Epoch 3/30: Train Loss: 0.6362, Test Loss: 0.6170, Train Acc: 0.6623, Test Acc: 0.7191, LR: 0.000100\n",
      "Epoch 4/30: Train Loss: 0.5999, Test Loss: 0.5844, Train Acc: 0.7411, Test Acc: 0.7567, LR: 0.000100\n",
      "Epoch 5/30: Train Loss: 0.5703, Test Loss: 0.5577, Train Acc: 0.7601, Test Acc: 0.7662, LR: 0.000070\n",
      "Epoch 6/30: Train Loss: 0.5492, Test Loss: 0.5418, Train Acc: 0.7700, Test Acc: 0.7808, LR: 0.000070\n",
      "Epoch 7/30: Train Loss: 0.5342, Test Loss: 0.5277, Train Acc: 0.7976, Test Acc: 0.8121, LR: 0.000070\n",
      "Epoch 8/30: Train Loss: 0.5209, Test Loss: 0.5151, Train Acc: 0.8259, Test Acc: 0.8396, LR: 0.000070\n",
      "Epoch 9/30: Train Loss: 0.5090, Test Loss: 0.5039, Train Acc: 0.8444, Test Acc: 0.8598, LR: 0.000070\n",
      "Epoch 10/30: Train Loss: 0.4983, Test Loss: 0.4937, Train Acc: 0.8623, Test Acc: 0.8720, LR: 0.000049\n",
      "Epoch 11/30: Train Loss: 0.4900, Test Loss: 0.4872, Train Acc: 0.8722, Test Acc: 0.8814, LR: 0.000049\n",
      "Epoch 12/30: Train Loss: 0.4837, Test Loss: 0.4811, Train Acc: 0.8809, Test Acc: 0.8838, LR: 0.000049\n",
      "Epoch 13/30: Train Loss: 0.4778, Test Loss: 0.4753, Train Acc: 0.8882, Test Acc: 0.8889, LR: 0.000049\n",
      "Epoch 14/30: Train Loss: 0.4723, Test Loss: 0.4699, Train Acc: 0.8877, Test Acc: 0.8887, LR: 0.000049\n",
      "Epoch 15/30: Train Loss: 0.4669, Test Loss: 0.4648, Train Acc: 0.8888, Test Acc: 0.8878, LR: 0.000034\n",
      "Epoch 16/30: Train Loss: 0.4627, Test Loss: 0.4614, Train Acc: 0.8884, Test Acc: 0.8877, LR: 0.000034\n",
      "Epoch 17/30: Train Loss: 0.4595, Test Loss: 0.4581, Train Acc: 0.8887, Test Acc: 0.8880, LR: 0.000034\n",
      "Epoch 18/30: Train Loss: 0.4562, Test Loss: 0.4549, Train Acc: 0.8890, Test Acc: 0.8888, LR: 0.000034\n",
      "Epoch 19/30: Train Loss: 0.4532, Test Loss: 0.4519, Train Acc: 0.8891, Test Acc: 0.8863, LR: 0.000034\n",
      "Epoch 20/30: Train Loss: 0.4501, Test Loss: 0.4489, Train Acc: 0.8876, Test Acc: 0.8868, LR: 0.000024\n",
      "Epoch 21/30: Train Loss: 0.4477, Test Loss: 0.4469, Train Acc: 0.8882, Test Acc: 0.8880, LR: 0.000024\n",
      "Epoch 22/30: Train Loss: 0.4457, Test Loss: 0.4450, Train Acc: 0.8897, Test Acc: 0.8909, LR: 0.000024\n",
      "Epoch 23/30: Train Loss: 0.4437, Test Loss: 0.4431, Train Acc: 0.8922, Test Acc: 0.8912, LR: 0.000024\n",
      "Epoch 24/30: Train Loss: 0.4419, Test Loss: 0.4412, Train Acc: 0.8917, Test Acc: 0.8910, LR: 0.000024\n",
      "Epoch 25/30: Train Loss: 0.4400, Test Loss: 0.4394, Train Acc: 0.8922, Test Acc: 0.8909, LR: 0.000017\n",
      "Epoch 26/30: Train Loss: 0.4385, Test Loss: 0.4381, Train Acc: 0.8920, Test Acc: 0.8910, LR: 0.000017\n",
      "Epoch 27/30: Train Loss: 0.4373, Test Loss: 0.4369, Train Acc: 0.8926, Test Acc: 0.8917, LR: 0.000017\n",
      "Epoch 28/30: Train Loss: 0.4360, Test Loss: 0.4357, Train Acc: 0.8928, Test Acc: 0.8918, LR: 0.000017\n",
      "Epoch 29/30: Train Loss: 0.4348, Test Loss: 0.4345, Train Acc: 0.8930, Test Acc: 0.8920, LR: 0.000017\n",
      "Epoch 30/30: Train Loss: 0.4336, Test Loss: 0.4333, Train Acc: 0.8932, Test Acc: 0.8927, LR: 0.000012\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11942  2360]\n",
      " [ 1053 16439]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.83      0.87     14302\n",
      "         1.0       0.87      0.94      0.91     17492\n",
      "\n",
      "    accuracy                           0.89     31794\n",
      "   macro avg       0.90      0.89      0.89     31794\n",
      "weighted avg       0.89      0.89      0.89     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.01, 'scheduler_step_size': 10, 'scheduler_gamma': 0.9, 'patience': 7, 'optimizer': 'rmsprop', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.04832930238571752, 'batch_size': 256}\n",
      "Epoch 1/100: Train Loss: 0.2192, Test Loss: 0.2101, Train Acc: 0.9522, Test Acc: 0.9582, LR: 0.048329\n",
      "Epoch 2/100: Train Loss: 0.2132, Test Loss: 0.2103, Train Acc: 0.9551, Test Acc: 0.9475, LR: 0.048329\n",
      "Epoch 3/100: Train Loss: 0.2131, Test Loss: 0.2121, Train Acc: 0.9547, Test Acc: 0.9501, LR: 0.048329\n",
      "Epoch 4/100: Train Loss: 0.2130, Test Loss: 0.2124, Train Acc: 0.9546, Test Acc: 0.9520, LR: 0.048329\n",
      "Epoch 5/100: Train Loss: 0.2133, Test Loss: 0.2090, Train Acc: 0.9552, Test Acc: 0.9590, LR: 0.048329\n",
      "Epoch 6/100: Train Loss: 0.2131, Test Loss: 0.2094, Train Acc: 0.9549, Test Acc: 0.9611, LR: 0.048329\n",
      "Epoch 7/100: Train Loss: 0.2132, Test Loss: 0.2086, Train Acc: 0.9550, Test Acc: 0.9500, LR: 0.048329\n",
      "Epoch 8/100: Train Loss: 0.2130, Test Loss: 0.2137, Train Acc: 0.9553, Test Acc: 0.9511, LR: 0.048329\n",
      "Epoch 9/100: Train Loss: 0.2132, Test Loss: 0.2121, Train Acc: 0.9552, Test Acc: 0.9600, LR: 0.048329\n",
      "Epoch 10/100: Train Loss: 0.2132, Test Loss: 0.2140, Train Acc: 0.9550, Test Acc: 0.9568, LR: 0.043496\n",
      "Epoch 11/100: Train Loss: 0.2130, Test Loss: 0.2097, Train Acc: 0.9553, Test Acc: 0.9475, LR: 0.043496\n",
      "Epoch 12/100: Train Loss: 0.2131, Test Loss: 0.2112, Train Acc: 0.9551, Test Acc: 0.9585, LR: 0.043496\n",
      "Epoch 13/100: Train Loss: 0.2131, Test Loss: 0.2146, Train Acc: 0.9550, Test Acc: 0.9590, LR: 0.043496\n",
      "Epoch 14/100: Train Loss: 0.2130, Test Loss: 0.2104, Train Acc: 0.9553, Test Acc: 0.9600, LR: 0.043496\n",
      "Early stopping triggered after 14 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13530   772]\n",
      " [  499 16993]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.95      0.96     14302\n",
      "         1.0       0.96      0.97      0.96     17492\n",
      "\n",
      "    accuracy                           0.96     31794\n",
      "   macro avg       0.96      0.96      0.96     31794\n",
      "weighted avg       0.96      0.96      0.96     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 2.1544346900318823e-05, 'scheduler_step_size': 15, 'scheduler_gamma': 0.7, 'patience': 3, 'optimizer': 'rmsprop', 'num_epochs': 50, 'model': 'lr', 'learning_rate': 0.0001438449888287663, 'batch_size': 256}\n",
      "Epoch 1/50: Train Loss: 0.6969, Test Loss: 0.6493, Train Acc: 0.5196, Test Acc: 0.6793, LR: 0.000144\n",
      "Epoch 2/50: Train Loss: 0.6119, Test Loss: 0.5780, Train Acc: 0.7064, Test Acc: 0.7238, LR: 0.000144\n",
      "Epoch 3/50: Train Loss: 0.5495, Test Loss: 0.5244, Train Acc: 0.7396, Test Acc: 0.7851, LR: 0.000144\n",
      "Epoch 4/50: Train Loss: 0.5031, Test Loss: 0.4838, Train Acc: 0.8271, Test Acc: 0.8572, LR: 0.000144\n",
      "Epoch 5/50: Train Loss: 0.4668, Test Loss: 0.4510, Train Acc: 0.8632, Test Acc: 0.8686, LR: 0.000144\n",
      "Epoch 6/50: Train Loss: 0.4367, Test Loss: 0.4231, Train Acc: 0.8719, Test Acc: 0.8760, LR: 0.000144\n",
      "Epoch 7/50: Train Loss: 0.4109, Test Loss: 0.3991, Train Acc: 0.8801, Test Acc: 0.8813, LR: 0.000144\n",
      "Epoch 8/50: Train Loss: 0.3886, Test Loss: 0.3781, Train Acc: 0.8874, Test Acc: 0.8894, LR: 0.000144\n",
      "Epoch 9/50: Train Loss: 0.3690, Test Loss: 0.3597, Train Acc: 0.8955, Test Acc: 0.9007, LR: 0.000144\n",
      "Epoch 10/50: Train Loss: 0.3519, Test Loss: 0.3435, Train Acc: 0.9058, Test Acc: 0.9076, LR: 0.000144\n",
      "Epoch 11/50: Train Loss: 0.3368, Test Loss: 0.3291, Train Acc: 0.9132, Test Acc: 0.9143, LR: 0.000144\n",
      "Epoch 12/50: Train Loss: 0.3232, Test Loss: 0.3162, Train Acc: 0.9173, Test Acc: 0.9182, LR: 0.000144\n",
      "Epoch 13/50: Train Loss: 0.3111, Test Loss: 0.3045, Train Acc: 0.9202, Test Acc: 0.9201, LR: 0.000144\n",
      "Epoch 14/50: Train Loss: 0.2999, Test Loss: 0.2938, Train Acc: 0.9218, Test Acc: 0.9217, LR: 0.000144\n",
      "Epoch 15/50: Train Loss: 0.2897, Test Loss: 0.2840, Train Acc: 0.9249, Test Acc: 0.9260, LR: 0.000101\n",
      "Epoch 16/50: Train Loss: 0.2817, Test Loss: 0.2776, Train Acc: 0.9286, Test Acc: 0.9295, LR: 0.000101\n",
      "Epoch 17/50: Train Loss: 0.2755, Test Loss: 0.2715, Train Acc: 0.9321, Test Acc: 0.9311, LR: 0.000101\n",
      "Epoch 18/50: Train Loss: 0.2696, Test Loss: 0.2657, Train Acc: 0.9337, Test Acc: 0.9323, LR: 0.000101\n",
      "Epoch 19/50: Train Loss: 0.2640, Test Loss: 0.2602, Train Acc: 0.9345, Test Acc: 0.9365, LR: 0.000101\n",
      "Epoch 20/50: Train Loss: 0.2586, Test Loss: 0.2550, Train Acc: 0.9376, Test Acc: 0.9366, LR: 0.000101\n",
      "Epoch 21/50: Train Loss: 0.2536, Test Loss: 0.2501, Train Acc: 0.9385, Test Acc: 0.9401, LR: 0.000101\n",
      "Epoch 22/50: Train Loss: 0.2488, Test Loss: 0.2453, Train Acc: 0.9408, Test Acc: 0.9411, LR: 0.000101\n",
      "Epoch 23/50: Train Loss: 0.2441, Test Loss: 0.2408, Train Acc: 0.9419, Test Acc: 0.9420, LR: 0.000101\n",
      "Epoch 24/50: Train Loss: 0.2397, Test Loss: 0.2364, Train Acc: 0.9436, Test Acc: 0.9445, LR: 0.000101\n",
      "Epoch 25/50: Train Loss: 0.2355, Test Loss: 0.2322, Train Acc: 0.9448, Test Acc: 0.9454, LR: 0.000101\n",
      "Epoch 26/50: Train Loss: 0.2314, Test Loss: 0.2283, Train Acc: 0.9455, Test Acc: 0.9457, LR: 0.000101\n",
      "Epoch 27/50: Train Loss: 0.2275, Test Loss: 0.2245, Train Acc: 0.9466, Test Acc: 0.9485, LR: 0.000101\n",
      "Epoch 28/50: Train Loss: 0.2238, Test Loss: 0.2208, Train Acc: 0.9487, Test Acc: 0.9490, LR: 0.000101\n",
      "Epoch 29/50: Train Loss: 0.2202, Test Loss: 0.2172, Train Acc: 0.9493, Test Acc: 0.9495, LR: 0.000101\n",
      "Epoch 30/50: Train Loss: 0.2168, Test Loss: 0.2139, Train Acc: 0.9497, Test Acc: 0.9497, LR: 0.000070\n",
      "Epoch 31/50: Train Loss: 0.2140, Test Loss: 0.2116, Train Acc: 0.9502, Test Acc: 0.9500, LR: 0.000070\n",
      "Epoch 32/50: Train Loss: 0.2118, Test Loss: 0.2093, Train Acc: 0.9507, Test Acc: 0.9520, LR: 0.000070\n",
      "Epoch 33/50: Train Loss: 0.2096, Test Loss: 0.2072, Train Acc: 0.9522, Test Acc: 0.9521, LR: 0.000070\n",
      "Epoch 34/50: Train Loss: 0.2075, Test Loss: 0.2051, Train Acc: 0.9526, Test Acc: 0.9524, LR: 0.000070\n",
      "Epoch 35/50: Train Loss: 0.2054, Test Loss: 0.2030, Train Acc: 0.9529, Test Acc: 0.9527, LR: 0.000070\n",
      "Epoch 36/50: Train Loss: 0.2034, Test Loss: 0.2010, Train Acc: 0.9530, Test Acc: 0.9527, LR: 0.000070\n",
      "Epoch 37/50: Train Loss: 0.2014, Test Loss: 0.1990, Train Acc: 0.9533, Test Acc: 0.9533, LR: 0.000070\n",
      "Epoch 38/50: Train Loss: 0.1995, Test Loss: 0.1971, Train Acc: 0.9538, Test Acc: 0.9533, LR: 0.000070\n",
      "Epoch 39/50: Train Loss: 0.1976, Test Loss: 0.1953, Train Acc: 0.9538, Test Acc: 0.9534, LR: 0.000070\n",
      "Epoch 40/50: Train Loss: 0.1958, Test Loss: 0.1935, Train Acc: 0.9542, Test Acc: 0.9551, LR: 0.000070\n",
      "Epoch 41/50: Train Loss: 0.1940, Test Loss: 0.1917, Train Acc: 0.9555, Test Acc: 0.9551, LR: 0.000070\n",
      "Epoch 42/50: Train Loss: 0.1923, Test Loss: 0.1900, Train Acc: 0.9558, Test Acc: 0.9559, LR: 0.000070\n",
      "Epoch 43/50: Train Loss: 0.1906, Test Loss: 0.1883, Train Acc: 0.9571, Test Acc: 0.9602, LR: 0.000070\n",
      "Epoch 44/50: Train Loss: 0.1890, Test Loss: 0.1866, Train Acc: 0.9589, Test Acc: 0.9607, LR: 0.000070\n",
      "Epoch 45/50: Train Loss: 0.1873, Test Loss: 0.1850, Train Acc: 0.9603, Test Acc: 0.9608, LR: 0.000049\n",
      "Epoch 46/50: Train Loss: 0.1860, Test Loss: 0.1839, Train Acc: 0.9604, Test Acc: 0.9609, LR: 0.000049\n",
      "Epoch 47/50: Train Loss: 0.1849, Test Loss: 0.1828, Train Acc: 0.9608, Test Acc: 0.9613, LR: 0.000049\n",
      "Epoch 48/50: Train Loss: 0.1839, Test Loss: 0.1818, Train Acc: 0.9609, Test Acc: 0.9613, LR: 0.000049\n",
      "Epoch 49/50: Train Loss: 0.1828, Test Loss: 0.1807, Train Acc: 0.9611, Test Acc: 0.9616, LR: 0.000049\n",
      "Epoch 50/50: Train Loss: 0.1818, Test Loss: 0.1797, Train Acc: 0.9613, Test Acc: 0.9617, LR: 0.000049\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13468   834]\n",
      " [  385 17107]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.94      0.96     14302\n",
      "         1.0       0.95      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.96     31794\n",
      "   macro avg       0.96      0.96      0.96     31794\n",
      "weighted avg       0.96      0.96      0.96     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 2.1544346900318823e-05, 'scheduler_step_size': 5, 'scheduler_gamma': 0.5, 'patience': 7, 'optimizer': 'adam', 'num_epochs': 75, 'model': 'lr', 'learning_rate': 0.0001438449888287663, 'batch_size': 32}\n",
      "Epoch 1/75: Train Loss: 0.5427, Test Loss: 0.4220, Train Acc: 0.7642, Test Acc: 0.8792, LR: 0.000144\n",
      "Epoch 2/75: Train Loss: 0.3649, Test Loss: 0.3187, Train Acc: 0.9023, Test Acc: 0.9234, LR: 0.000144\n",
      "Epoch 3/75: Train Loss: 0.2876, Test Loss: 0.2607, Train Acc: 0.9326, Test Acc: 0.9379, LR: 0.000144\n",
      "Epoch 4/75: Train Loss: 0.2416, Test Loss: 0.2238, Train Acc: 0.9452, Test Acc: 0.9468, LR: 0.000144\n",
      "Epoch 5/75: Train Loss: 0.2114, Test Loss: 0.1986, Train Acc: 0.9526, Test Acc: 0.9554, LR: 0.000072\n",
      "Epoch 6/75: Train Loss: 0.1946, Test Loss: 0.1887, Train Acc: 0.9562, Test Acc: 0.9564, LR: 0.000072\n",
      "Epoch 7/75: Train Loss: 0.1855, Test Loss: 0.1802, Train Acc: 0.9594, Test Acc: 0.9632, LR: 0.000072\n",
      "Epoch 8/75: Train Loss: 0.1778, Test Loss: 0.1729, Train Acc: 0.9625, Test Acc: 0.9630, LR: 0.000072\n",
      "Epoch 9/75: Train Loss: 0.1711, Test Loss: 0.1666, Train Acc: 0.9636, Test Acc: 0.9650, LR: 0.000072\n",
      "Epoch 10/75: Train Loss: 0.1653, Test Loss: 0.1611, Train Acc: 0.9643, Test Acc: 0.9655, LR: 0.000036\n",
      "Epoch 11/75: Train Loss: 0.1615, Test Loss: 0.1586, Train Acc: 0.9645, Test Acc: 0.9656, LR: 0.000036\n",
      "Epoch 12/75: Train Loss: 0.1591, Test Loss: 0.1562, Train Acc: 0.9660, Test Acc: 0.9667, LR: 0.000036\n",
      "Epoch 13/75: Train Loss: 0.1569, Test Loss: 0.1541, Train Acc: 0.9663, Test Acc: 0.9668, LR: 0.000036\n",
      "Epoch 14/75: Train Loss: 0.1548, Test Loss: 0.1520, Train Acc: 0.9664, Test Acc: 0.9670, LR: 0.000036\n",
      "Epoch 15/75: Train Loss: 0.1529, Test Loss: 0.1501, Train Acc: 0.9666, Test Acc: 0.9671, LR: 0.000018\n",
      "Epoch 16/75: Train Loss: 0.1515, Test Loss: 0.1491, Train Acc: 0.9666, Test Acc: 0.9671, LR: 0.000018\n",
      "Epoch 17/75: Train Loss: 0.1507, Test Loss: 0.1483, Train Acc: 0.9666, Test Acc: 0.9671, LR: 0.000018\n",
      "Epoch 18/75: Train Loss: 0.1498, Test Loss: 0.1474, Train Acc: 0.9666, Test Acc: 0.9672, LR: 0.000018\n",
      "Epoch 19/75: Train Loss: 0.1490, Test Loss: 0.1465, Train Acc: 0.9667, Test Acc: 0.9672, LR: 0.000018\n",
      "Epoch 20/75: Train Loss: 0.1481, Test Loss: 0.1457, Train Acc: 0.9670, Test Acc: 0.9684, LR: 0.000009\n",
      "Epoch 21/75: Train Loss: 0.1476, Test Loss: 0.1453, Train Acc: 0.9683, Test Acc: 0.9684, LR: 0.000009\n",
      "Epoch 22/75: Train Loss: 0.1471, Test Loss: 0.1449, Train Acc: 0.9683, Test Acc: 0.9684, LR: 0.000009\n",
      "Epoch 23/75: Train Loss: 0.1468, Test Loss: 0.1445, Train Acc: 0.9684, Test Acc: 0.9684, LR: 0.000009\n",
      "Epoch 24/75: Train Loss: 0.1464, Test Loss: 0.1441, Train Acc: 0.9684, Test Acc: 0.9684, LR: 0.000009\n",
      "Epoch 25/75: Train Loss: 0.1460, Test Loss: 0.1438, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000004\n",
      "Epoch 26/75: Train Loss: 0.1460, Test Loss: 0.1436, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000004\n",
      "Epoch 27/75: Train Loss: 0.1456, Test Loss: 0.1434, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000004\n",
      "Epoch 28/75: Train Loss: 0.1454, Test Loss: 0.1432, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000004\n",
      "Epoch 29/75: Train Loss: 0.1452, Test Loss: 0.1430, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000004\n",
      "Epoch 30/75: Train Loss: 0.1450, Test Loss: 0.1428, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000002\n",
      "Epoch 31/75: Train Loss: 0.1449, Test Loss: 0.1428, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000002\n",
      "Epoch 32/75: Train Loss: 0.1448, Test Loss: 0.1427, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000002\n",
      "Epoch 33/75: Train Loss: 0.1447, Test Loss: 0.1426, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000002\n",
      "Epoch 34/75: Train Loss: 0.1447, Test Loss: 0.1425, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000002\n",
      "Epoch 35/75: Train Loss: 0.1446, Test Loss: 0.1424, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000001\n",
      "Epoch 36/75: Train Loss: 0.1445, Test Loss: 0.1424, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000001\n",
      "Epoch 37/75: Train Loss: 0.1444, Test Loss: 0.1423, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000001\n",
      "Epoch 38/75: Train Loss: 0.1444, Test Loss: 0.1423, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000001\n",
      "Epoch 39/75: Train Loss: 0.1444, Test Loss: 0.1422, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000001\n",
      "Epoch 40/75: Train Loss: 0.1443, Test Loss: 0.1422, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000001\n",
      "Epoch 41/75: Train Loss: 0.1443, Test Loss: 0.1422, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000001\n",
      "Epoch 42/75: Train Loss: 0.1443, Test Loss: 0.1421, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000001\n",
      "Epoch 43/75: Train Loss: 0.1442, Test Loss: 0.1421, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000001\n",
      "Epoch 44/75: Train Loss: 0.1442, Test Loss: 0.1421, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000001\n",
      "Epoch 45/75: Train Loss: 0.1442, Test Loss: 0.1421, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000000\n",
      "Epoch 46/75: Train Loss: 0.1442, Test Loss: 0.1421, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000000\n",
      "Epoch 47/75: Train Loss: 0.1442, Test Loss: 0.1420, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000000\n",
      "Epoch 48/75: Train Loss: 0.1442, Test Loss: 0.1420, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000000\n",
      "Epoch 49/75: Train Loss: 0.1442, Test Loss: 0.1420, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000000\n",
      "Epoch 50/75: Train Loss: 0.1442, Test Loss: 0.1420, Train Acc: 0.9684, Test Acc: 0.9685, LR: 0.000000\n",
      "Early stopping triggered after 50 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13585   717]\n",
      " [  286 17206]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.95      0.96     14302\n",
      "         1.0       0.96      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.00046415888336127773, 'scheduler_step_size': 5, 'scheduler_gamma': 0.7, 'patience': 7, 'optimizer': 'adam', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.005455594781168515, 'batch_size': 64}\n",
      "Epoch 1/100: Train Loss: 0.2279, Test Loss: 0.1475, Train Acc: 0.9413, Test Acc: 0.9674, LR: 0.005456\n",
      "Epoch 2/100: Train Loss: 0.1356, Test Loss: 0.1243, Train Acc: 0.9718, Test Acc: 0.9741, LR: 0.005456\n",
      "Epoch 3/100: Train Loss: 0.1252, Test Loss: 0.1205, Train Acc: 0.9737, Test Acc: 0.9740, LR: 0.005456\n",
      "Epoch 4/100: Train Loss: 0.1237, Test Loss: 0.1198, Train Acc: 0.9737, Test Acc: 0.9738, LR: 0.005456\n",
      "Epoch 5/100: Train Loss: 0.1232, Test Loss: 0.1203, Train Acc: 0.9739, Test Acc: 0.9737, LR: 0.003819\n",
      "Epoch 6/100: Train Loss: 0.1235, Test Loss: 0.1197, Train Acc: 0.9738, Test Acc: 0.9742, LR: 0.003819\n",
      "Epoch 7/100: Train Loss: 0.1234, Test Loss: 0.1196, Train Acc: 0.9739, Test Acc: 0.9740, LR: 0.003819\n",
      "Epoch 8/100: Train Loss: 0.1231, Test Loss: 0.1200, Train Acc: 0.9739, Test Acc: 0.9749, LR: 0.003819\n",
      "Epoch 9/100: Train Loss: 0.1233, Test Loss: 0.1199, Train Acc: 0.9739, Test Acc: 0.9741, LR: 0.003819\n",
      "Epoch 10/100: Train Loss: 0.1233, Test Loss: 0.1200, Train Acc: 0.9739, Test Acc: 0.9746, LR: 0.002673\n",
      "Epoch 11/100: Train Loss: 0.1233, Test Loss: 0.1202, Train Acc: 0.9738, Test Acc: 0.9744, LR: 0.002673\n",
      "Epoch 12/100: Train Loss: 0.1233, Test Loss: 0.1201, Train Acc: 0.9739, Test Acc: 0.9746, LR: 0.002673\n",
      "Epoch 13/100: Train Loss: 0.1233, Test Loss: 0.1199, Train Acc: 0.9739, Test Acc: 0.9742, LR: 0.002673\n",
      "Epoch 14/100: Train Loss: 0.1232, Test Loss: 0.1202, Train Acc: 0.9739, Test Acc: 0.9742, LR: 0.002673\n",
      "Early stopping triggered after 14 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13785   517]\n",
      " [  304 17188]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.001, 'scheduler_step_size': 15, 'scheduler_gamma': 0.9, 'patience': 3, 'optimizer': 'sgd', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.007847599703514606, 'batch_size': 256}\n",
      "Epoch 1/100: Train Loss: 0.5080, Test Loss: 0.4047, Train Acc: 0.8125, Test Acc: 0.8962, LR: 0.007848\n",
      "Epoch 2/100: Train Loss: 0.3699, Test Loss: 0.3428, Train Acc: 0.9065, Test Acc: 0.9150, LR: 0.007848\n",
      "Epoch 3/100: Train Loss: 0.3253, Test Loss: 0.3094, Train Acc: 0.9217, Test Acc: 0.9255, LR: 0.007848\n",
      "Epoch 4/100: Train Loss: 0.2981, Test Loss: 0.2869, Train Acc: 0.9290, Test Acc: 0.9294, LR: 0.007848\n",
      "Epoch 5/100: Train Loss: 0.2789, Test Loss: 0.2702, Train Acc: 0.9328, Test Acc: 0.9329, LR: 0.007848\n",
      "Epoch 6/100: Train Loss: 0.2643, Test Loss: 0.2572, Train Acc: 0.9380, Test Acc: 0.9397, LR: 0.007848\n",
      "Epoch 7/100: Train Loss: 0.2527, Test Loss: 0.2467, Train Acc: 0.9424, Test Acc: 0.9445, LR: 0.007848\n",
      "Epoch 8/100: Train Loss: 0.2432, Test Loss: 0.2379, Train Acc: 0.9464, Test Acc: 0.9436, LR: 0.007848\n",
      "Epoch 9/100: Train Loss: 0.2352, Test Loss: 0.2305, Train Acc: 0.9479, Test Acc: 0.9507, LR: 0.007848\n",
      "Epoch 10/100: Train Loss: 0.2283, Test Loss: 0.2240, Train Acc: 0.9505, Test Acc: 0.9511, LR: 0.007848\n",
      "Epoch 11/100: Train Loss: 0.2224, Test Loss: 0.2184, Train Acc: 0.9520, Test Acc: 0.9533, LR: 0.007848\n",
      "Epoch 12/100: Train Loss: 0.2171, Test Loss: 0.2134, Train Acc: 0.9537, Test Acc: 0.9545, LR: 0.007848\n",
      "Epoch 13/100: Train Loss: 0.2125, Test Loss: 0.2089, Train Acc: 0.9557, Test Acc: 0.9563, LR: 0.007848\n",
      "Epoch 14/100: Train Loss: 0.2083, Test Loss: 0.2049, Train Acc: 0.9562, Test Acc: 0.9561, LR: 0.007848\n",
      "Epoch 15/100: Train Loss: 0.2045, Test Loss: 0.2013, Train Acc: 0.9560, Test Acc: 0.9561, LR: 0.007063\n",
      "Epoch 16/100: Train Loss: 0.2012, Test Loss: 0.1983, Train Acc: 0.9575, Test Acc: 0.9588, LR: 0.007063\n",
      "Epoch 17/100: Train Loss: 0.1984, Test Loss: 0.1955, Train Acc: 0.9587, Test Acc: 0.9590, LR: 0.007063\n",
      "Epoch 18/100: Train Loss: 0.1958, Test Loss: 0.1929, Train Acc: 0.9588, Test Acc: 0.9595, LR: 0.007063\n",
      "Epoch 19/100: Train Loss: 0.1933, Test Loss: 0.1905, Train Acc: 0.9593, Test Acc: 0.9598, LR: 0.007063\n",
      "Epoch 20/100: Train Loss: 0.1910, Test Loss: 0.1883, Train Acc: 0.9596, Test Acc: 0.9602, LR: 0.007063\n",
      "Epoch 21/100: Train Loss: 0.1889, Test Loss: 0.1862, Train Acc: 0.9598, Test Acc: 0.9614, LR: 0.007063\n",
      "Epoch 22/100: Train Loss: 0.1869, Test Loss: 0.1842, Train Acc: 0.9609, Test Acc: 0.9623, LR: 0.007063\n",
      "Epoch 23/100: Train Loss: 0.1851, Test Loss: 0.1824, Train Acc: 0.9613, Test Acc: 0.9623, LR: 0.007063\n",
      "Epoch 24/100: Train Loss: 0.1833, Test Loss: 0.1807, Train Acc: 0.9614, Test Acc: 0.9624, LR: 0.007063\n",
      "Epoch 25/100: Train Loss: 0.1816, Test Loss: 0.1790, Train Acc: 0.9614, Test Acc: 0.9624, LR: 0.007063\n",
      "Epoch 26/100: Train Loss: 0.1800, Test Loss: 0.1775, Train Acc: 0.9619, Test Acc: 0.9629, LR: 0.007063\n",
      "Epoch 27/100: Train Loss: 0.1786, Test Loss: 0.1760, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.007063\n",
      "Epoch 28/100: Train Loss: 0.1772, Test Loss: 0.1746, Train Acc: 0.9620, Test Acc: 0.9629, LR: 0.007063\n",
      "Epoch 29/100: Train Loss: 0.1758, Test Loss: 0.1733, Train Acc: 0.9621, Test Acc: 0.9631, LR: 0.007063\n",
      "Epoch 30/100: Train Loss: 0.1745, Test Loss: 0.1720, Train Acc: 0.9631, Test Acc: 0.9642, LR: 0.006357\n",
      "Epoch 31/100: Train Loss: 0.1734, Test Loss: 0.1709, Train Acc: 0.9638, Test Acc: 0.9643, LR: 0.006357\n",
      "Epoch 32/100: Train Loss: 0.1723, Test Loss: 0.1699, Train Acc: 0.9639, Test Acc: 0.9643, LR: 0.006357\n",
      "Epoch 33/100: Train Loss: 0.1714, Test Loss: 0.1689, Train Acc: 0.9639, Test Acc: 0.9644, LR: 0.006357\n",
      "Epoch 34/100: Train Loss: 0.1704, Test Loss: 0.1679, Train Acc: 0.9641, Test Acc: 0.9646, LR: 0.006357\n",
      "Epoch 35/100: Train Loss: 0.1695, Test Loss: 0.1670, Train Acc: 0.9644, Test Acc: 0.9646, LR: 0.006357\n",
      "Epoch 36/100: Train Loss: 0.1686, Test Loss: 0.1661, Train Acc: 0.9645, Test Acc: 0.9648, LR: 0.006357\n",
      "Epoch 37/100: Train Loss: 0.1677, Test Loss: 0.1653, Train Acc: 0.9646, Test Acc: 0.9648, LR: 0.006357\n",
      "Epoch 38/100: Train Loss: 0.1669, Test Loss: 0.1645, Train Acc: 0.9646, Test Acc: 0.9648, LR: 0.006357\n",
      "Epoch 39/100: Train Loss: 0.1661, Test Loss: 0.1637, Train Acc: 0.9646, Test Acc: 0.9649, LR: 0.006357\n",
      "Epoch 40/100: Train Loss: 0.1654, Test Loss: 0.1629, Train Acc: 0.9646, Test Acc: 0.9650, LR: 0.006357\n",
      "Epoch 41/100: Train Loss: 0.1647, Test Loss: 0.1622, Train Acc: 0.9647, Test Acc: 0.9651, LR: 0.006357\n",
      "Epoch 42/100: Train Loss: 0.1639, Test Loss: 0.1615, Train Acc: 0.9647, Test Acc: 0.9651, LR: 0.006357\n",
      "Epoch 43/100: Train Loss: 0.1632, Test Loss: 0.1608, Train Acc: 0.9647, Test Acc: 0.9651, LR: 0.006357\n",
      "Epoch 44/100: Train Loss: 0.1626, Test Loss: 0.1601, Train Acc: 0.9660, Test Acc: 0.9663, LR: 0.006357\n",
      "Epoch 45/100: Train Loss: 0.1619, Test Loss: 0.1595, Train Acc: 0.9663, Test Acc: 0.9663, LR: 0.005721\n",
      "Epoch 46/100: Train Loss: 0.1614, Test Loss: 0.1589, Train Acc: 0.9664, Test Acc: 0.9663, LR: 0.005721\n",
      "Epoch 47/100: Train Loss: 0.1608, Test Loss: 0.1584, Train Acc: 0.9664, Test Acc: 0.9663, LR: 0.005721\n",
      "Epoch 48/100: Train Loss: 0.1603, Test Loss: 0.1578, Train Acc: 0.9666, Test Acc: 0.9666, LR: 0.005721\n",
      "Epoch 49/100: Train Loss: 0.1598, Test Loss: 0.1573, Train Acc: 0.9666, Test Acc: 0.9666, LR: 0.005721\n",
      "Epoch 50/100: Train Loss: 0.1593, Test Loss: 0.1568, Train Acc: 0.9667, Test Acc: 0.9668, LR: 0.005721\n",
      "Epoch 51/100: Train Loss: 0.1588, Test Loss: 0.1563, Train Acc: 0.9667, Test Acc: 0.9668, LR: 0.005721\n",
      "Epoch 52/100: Train Loss: 0.1583, Test Loss: 0.1559, Train Acc: 0.9667, Test Acc: 0.9668, LR: 0.005721\n",
      "Epoch 53/100: Train Loss: 0.1579, Test Loss: 0.1554, Train Acc: 0.9667, Test Acc: 0.9668, LR: 0.005721\n",
      "Epoch 54/100: Train Loss: 0.1574, Test Loss: 0.1549, Train Acc: 0.9667, Test Acc: 0.9668, LR: 0.005721\n",
      "Epoch 55/100: Train Loss: 0.1570, Test Loss: 0.1545, Train Acc: 0.9667, Test Acc: 0.9668, LR: 0.005721\n",
      "Epoch 56/100: Train Loss: 0.1566, Test Loss: 0.1541, Train Acc: 0.9667, Test Acc: 0.9668, LR: 0.005721\n",
      "Epoch 57/100: Train Loss: 0.1562, Test Loss: 0.1537, Train Acc: 0.9668, Test Acc: 0.9670, LR: 0.005721\n",
      "Epoch 58/100: Train Loss: 0.1558, Test Loss: 0.1533, Train Acc: 0.9669, Test Acc: 0.9670, LR: 0.005721\n",
      "Epoch 59/100: Train Loss: 0.1554, Test Loss: 0.1529, Train Acc: 0.9669, Test Acc: 0.9674, LR: 0.005721\n",
      "Epoch 60/100: Train Loss: 0.1550, Test Loss: 0.1525, Train Acc: 0.9674, Test Acc: 0.9674, LR: 0.005149\n",
      "Epoch 61/100: Train Loss: 0.1546, Test Loss: 0.1521, Train Acc: 0.9674, Test Acc: 0.9674, LR: 0.005149\n",
      "Epoch 62/100: Train Loss: 0.1543, Test Loss: 0.1518, Train Acc: 0.9674, Test Acc: 0.9674, LR: 0.005149\n",
      "Epoch 63/100: Train Loss: 0.1540, Test Loss: 0.1515, Train Acc: 0.9674, Test Acc: 0.9674, LR: 0.005149\n",
      "Epoch 64/100: Train Loss: 0.1537, Test Loss: 0.1512, Train Acc: 0.9674, Test Acc: 0.9674, LR: 0.005149\n",
      "Epoch 65/100: Train Loss: 0.1534, Test Loss: 0.1509, Train Acc: 0.9674, Test Acc: 0.9674, LR: 0.005149\n",
      "Epoch 66/100: Train Loss: 0.1531, Test Loss: 0.1505, Train Acc: 0.9674, Test Acc: 0.9675, LR: 0.005149\n",
      "Epoch 67/100: Train Loss: 0.1528, Test Loss: 0.1502, Train Acc: 0.9674, Test Acc: 0.9676, LR: 0.005149\n",
      "Epoch 68/100: Train Loss: 0.1525, Test Loss: 0.1500, Train Acc: 0.9676, Test Acc: 0.9676, LR: 0.005149\n",
      "Epoch 69/100: Train Loss: 0.1522, Test Loss: 0.1497, Train Acc: 0.9676, Test Acc: 0.9676, LR: 0.005149\n",
      "Epoch 70/100: Train Loss: 0.1519, Test Loss: 0.1494, Train Acc: 0.9677, Test Acc: 0.9699, LR: 0.005149\n",
      "Epoch 71/100: Train Loss: 0.1517, Test Loss: 0.1491, Train Acc: 0.9695, Test Acc: 0.9699, LR: 0.005149\n",
      "Epoch 72/100: Train Loss: 0.1514, Test Loss: 0.1488, Train Acc: 0.9696, Test Acc: 0.9700, LR: 0.005149\n",
      "Epoch 73/100: Train Loss: 0.1511, Test Loss: 0.1486, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.005149\n",
      "Epoch 74/100: Train Loss: 0.1509, Test Loss: 0.1483, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.005149\n",
      "Epoch 75/100: Train Loss: 0.1506, Test Loss: 0.1481, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 76/100: Train Loss: 0.1504, Test Loss: 0.1478, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 77/100: Train Loss: 0.1502, Test Loss: 0.1476, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 78/100: Train Loss: 0.1500, Test Loss: 0.1474, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 79/100: Train Loss: 0.1498, Test Loss: 0.1472, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 80/100: Train Loss: 0.1496, Test Loss: 0.1470, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 81/100: Train Loss: 0.1494, Test Loss: 0.1468, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 82/100: Train Loss: 0.1492, Test Loss: 0.1466, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 83/100: Train Loss: 0.1490, Test Loss: 0.1464, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 84/100: Train Loss: 0.1488, Test Loss: 0.1462, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 85/100: Train Loss: 0.1486, Test Loss: 0.1460, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 86/100: Train Loss: 0.1484, Test Loss: 0.1458, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 87/100: Train Loss: 0.1482, Test Loss: 0.1456, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 88/100: Train Loss: 0.1480, Test Loss: 0.1454, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.004634\n",
      "Epoch 89/100: Train Loss: 0.1479, Test Loss: 0.1452, Train Acc: 0.9697, Test Acc: 0.9701, LR: 0.004634\n",
      "Epoch 90/100: Train Loss: 0.1477, Test Loss: 0.1451, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.004171\n",
      "Epoch 91/100: Train Loss: 0.1475, Test Loss: 0.1449, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.004171\n",
      "Epoch 92/100: Train Loss: 0.1474, Test Loss: 0.1447, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.004171\n",
      "Epoch 93/100: Train Loss: 0.1472, Test Loss: 0.1446, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.004171\n",
      "Epoch 94/100: Train Loss: 0.1470, Test Loss: 0.1444, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.004171\n",
      "Epoch 95/100: Train Loss: 0.1469, Test Loss: 0.1443, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.004171\n",
      "Epoch 96/100: Train Loss: 0.1468, Test Loss: 0.1441, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.004171\n",
      "Epoch 97/100: Train Loss: 0.1466, Test Loss: 0.1440, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.004171\n",
      "Epoch 98/100: Train Loss: 0.1465, Test Loss: 0.1439, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.004171\n",
      "Epoch 99/100: Train Loss: 0.1464, Test Loss: 0.1437, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.004171\n",
      "Epoch 100/100: Train Loss: 0.1462, Test Loss: 0.1436, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.004171\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13676   626]\n",
      " [  324 17168]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.96      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.001, 'scheduler_step_size': 5, 'scheduler_gamma': 0.9, 'patience': 3, 'optimizer': 'adam', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.0008858667904100823, 'batch_size': 256}\n",
      "Epoch 1/30: Train Loss: 0.5679, Test Loss: 0.4584, Train Acc: 0.7271, Test Acc: 0.8680, LR: 0.000886\n",
      "Epoch 2/30: Train Loss: 0.4104, Test Loss: 0.3714, Train Acc: 0.8812, Test Acc: 0.8942, LR: 0.000886\n",
      "Epoch 3/30: Train Loss: 0.3443, Test Loss: 0.3197, Train Acc: 0.9126, Test Acc: 0.9200, LR: 0.000886\n",
      "Epoch 4/30: Train Loss: 0.3015, Test Loss: 0.2838, Train Acc: 0.9256, Test Acc: 0.9288, LR: 0.000886\n",
      "Epoch 5/30: Train Loss: 0.2708, Test Loss: 0.2570, Train Acc: 0.9372, Test Acc: 0.9422, LR: 0.000797\n",
      "Epoch 6/30: Train Loss: 0.2481, Test Loss: 0.2376, Train Acc: 0.9450, Test Acc: 0.9492, LR: 0.000797\n",
      "Epoch 7/30: Train Loss: 0.2305, Test Loss: 0.2213, Train Acc: 0.9496, Test Acc: 0.9490, LR: 0.000797\n",
      "Epoch 8/30: Train Loss: 0.2156, Test Loss: 0.2075, Train Acc: 0.9527, Test Acc: 0.9558, LR: 0.000797\n",
      "Epoch 9/30: Train Loss: 0.2030, Test Loss: 0.1956, Train Acc: 0.9567, Test Acc: 0.9589, LR: 0.000797\n",
      "Epoch 10/30: Train Loss: 0.1920, Test Loss: 0.1854, Train Acc: 0.9593, Test Acc: 0.9611, LR: 0.000718\n",
      "Epoch 11/30: Train Loss: 0.1830, Test Loss: 0.1774, Train Acc: 0.9610, Test Acc: 0.9621, LR: 0.000718\n",
      "Epoch 12/30: Train Loss: 0.1756, Test Loss: 0.1702, Train Acc: 0.9625, Test Acc: 0.9642, LR: 0.000718\n",
      "Epoch 13/30: Train Loss: 0.1689, Test Loss: 0.1639, Train Acc: 0.9643, Test Acc: 0.9645, LR: 0.000718\n",
      "Epoch 14/30: Train Loss: 0.1632, Test Loss: 0.1585, Train Acc: 0.9655, Test Acc: 0.9664, LR: 0.000718\n",
      "Epoch 15/30: Train Loss: 0.1582, Test Loss: 0.1537, Train Acc: 0.9669, Test Acc: 0.9668, LR: 0.000646\n",
      "Epoch 16/30: Train Loss: 0.1540, Test Loss: 0.1499, Train Acc: 0.9671, Test Acc: 0.9672, LR: 0.000646\n",
      "Epoch 17/30: Train Loss: 0.1506, Test Loss: 0.1467, Train Acc: 0.9685, Test Acc: 0.9704, LR: 0.000646\n",
      "Epoch 18/30: Train Loss: 0.1477, Test Loss: 0.1438, Train Acc: 0.9699, Test Acc: 0.9705, LR: 0.000646\n",
      "Epoch 19/30: Train Loss: 0.1451, Test Loss: 0.1414, Train Acc: 0.9701, Test Acc: 0.9706, LR: 0.000646\n",
      "Epoch 20/30: Train Loss: 0.1430, Test Loss: 0.1394, Train Acc: 0.9702, Test Acc: 0.9706, LR: 0.000581\n",
      "Epoch 21/30: Train Loss: 0.1413, Test Loss: 0.1379, Train Acc: 0.9703, Test Acc: 0.9707, LR: 0.000581\n",
      "Epoch 22/30: Train Loss: 0.1400, Test Loss: 0.1367, Train Acc: 0.9711, Test Acc: 0.9730, LR: 0.000581\n",
      "Epoch 23/30: Train Loss: 0.1389, Test Loss: 0.1356, Train Acc: 0.9723, Test Acc: 0.9730, LR: 0.000581\n",
      "Epoch 24/30: Train Loss: 0.1380, Test Loss: 0.1347, Train Acc: 0.9725, Test Acc: 0.9731, LR: 0.000581\n",
      "Epoch 25/30: Train Loss: 0.1372, Test Loss: 0.1340, Train Acc: 0.9725, Test Acc: 0.9732, LR: 0.000523\n",
      "Epoch 26/30: Train Loss: 0.1366, Test Loss: 0.1334, Train Acc: 0.9724, Test Acc: 0.9732, LR: 0.000523\n",
      "Epoch 27/30: Train Loss: 0.1361, Test Loss: 0.1330, Train Acc: 0.9724, Test Acc: 0.9730, LR: 0.000523\n",
      "Epoch 28/30: Train Loss: 0.1357, Test Loss: 0.1326, Train Acc: 0.9723, Test Acc: 0.9730, LR: 0.000523\n",
      "Epoch 29/30: Train Loss: 0.1354, Test Loss: 0.1322, Train Acc: 0.9723, Test Acc: 0.9730, LR: 0.000523\n",
      "Epoch 30/30: Train Loss: 0.1350, Test Loss: 0.1320, Train Acc: 0.9723, Test Acc: 0.9730, LR: 0.000471\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13758   544]\n",
      " [  315 17177]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.01, 'scheduler_step_size': 5, 'scheduler_gamma': 0.5, 'patience': 5, 'optimizer': 'sgd', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.04832930238571752, 'batch_size': 128}\n",
      "Epoch 1/30: Train Loss: 0.2988, Test Loss: 0.2369, Train Acc: 0.9233, Test Acc: 0.9461, LR: 0.048329\n",
      "Epoch 2/30: Train Loss: 0.2265, Test Loss: 0.2180, Train Acc: 0.9518, Test Acc: 0.9569, LR: 0.048329\n",
      "Epoch 3/30: Train Loss: 0.2164, Test Loss: 0.2132, Train Acc: 0.9557, Test Acc: 0.9583, LR: 0.048329\n",
      "Epoch 4/30: Train Loss: 0.2137, Test Loss: 0.2116, Train Acc: 0.9570, Test Acc: 0.9584, LR: 0.048329\n",
      "Epoch 5/30: Train Loss: 0.2127, Test Loss: 0.2111, Train Acc: 0.9578, Test Acc: 0.9583, LR: 0.024165\n",
      "Epoch 6/30: Train Loss: 0.2125, Test Loss: 0.2111, Train Acc: 0.9577, Test Acc: 0.9585, LR: 0.024165\n",
      "Epoch 7/30: Train Loss: 0.2124, Test Loss: 0.2110, Train Acc: 0.9578, Test Acc: 0.9585, LR: 0.024165\n",
      "Epoch 8/30: Train Loss: 0.2123, Test Loss: 0.2110, Train Acc: 0.9578, Test Acc: 0.9584, LR: 0.024165\n",
      "Epoch 9/30: Train Loss: 0.2123, Test Loss: 0.2110, Train Acc: 0.9579, Test Acc: 0.9585, LR: 0.024165\n",
      "Epoch 10/30: Train Loss: 0.2123, Test Loss: 0.2110, Train Acc: 0.9580, Test Acc: 0.9585, LR: 0.012082\n",
      "Epoch 11/30: Train Loss: 0.2122, Test Loss: 0.2110, Train Acc: 0.9578, Test Acc: 0.9585, LR: 0.012082\n",
      "Epoch 12/30: Train Loss: 0.2123, Test Loss: 0.2110, Train Acc: 0.9578, Test Acc: 0.9585, LR: 0.012082\n",
      "Early stopping triggered after 12 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13402   900]\n",
      " [  420 17072]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.94      0.95     14302\n",
      "         1.0       0.95      0.98      0.96     17492\n",
      "\n",
      "    accuracy                           0.96     31794\n",
      "   macro avg       0.96      0.96      0.96     31794\n",
      "weighted avg       0.96      0.96      0.96     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.00046415888336127773, 'scheduler_step_size': 10, 'scheduler_gamma': 0.9, 'patience': 7, 'optimizer': 'sgd', 'num_epochs': 50, 'model': 'lr', 'learning_rate': 0.0008858667904100823, 'batch_size': 64}\n",
      "Epoch 1/50: Train Loss: 0.5904, Test Loss: 0.4889, Train Acc: 0.7146, Test Acc: 0.8705, LR: 0.000886\n",
      "Epoch 2/50: Train Loss: 0.4461, Test Loss: 0.4146, Train Acc: 0.8907, Test Acc: 0.8953, LR: 0.000886\n",
      "Epoch 3/50: Train Loss: 0.3940, Test Loss: 0.3767, Train Acc: 0.9010, Test Acc: 0.9020, LR: 0.000886\n",
      "Epoch 4/50: Train Loss: 0.3634, Test Loss: 0.3514, Train Acc: 0.9079, Test Acc: 0.9103, LR: 0.000886\n",
      "Epoch 5/50: Train Loss: 0.3417, Test Loss: 0.3325, Train Acc: 0.9152, Test Acc: 0.9158, LR: 0.000886\n",
      "Epoch 6/50: Train Loss: 0.3250, Test Loss: 0.3174, Train Acc: 0.9210, Test Acc: 0.9243, LR: 0.000886\n",
      "Epoch 7/50: Train Loss: 0.3113, Test Loss: 0.3049, Train Acc: 0.9264, Test Acc: 0.9256, LR: 0.000886\n",
      "Epoch 8/50: Train Loss: 0.2999, Test Loss: 0.2944, Train Acc: 0.9288, Test Acc: 0.9290, LR: 0.000886\n",
      "Epoch 9/50: Train Loss: 0.2901, Test Loss: 0.2853, Train Acc: 0.9305, Test Acc: 0.9294, LR: 0.000886\n",
      "Epoch 10/50: Train Loss: 0.2817, Test Loss: 0.2773, Train Acc: 0.9321, Test Acc: 0.9320, LR: 0.000797\n",
      "Epoch 11/50: Train Loss: 0.2746, Test Loss: 0.2709, Train Acc: 0.9338, Test Acc: 0.9329, LR: 0.000797\n",
      "Epoch 12/50: Train Loss: 0.2685, Test Loss: 0.2651, Train Acc: 0.9368, Test Acc: 0.9366, LR: 0.000797\n",
      "Epoch 13/50: Train Loss: 0.2631, Test Loss: 0.2598, Train Acc: 0.9384, Test Acc: 0.9395, LR: 0.000797\n",
      "Epoch 14/50: Train Loss: 0.2581, Test Loss: 0.2549, Train Acc: 0.9403, Test Acc: 0.9397, LR: 0.000797\n",
      "Epoch 15/50: Train Loss: 0.2534, Test Loss: 0.2505, Train Acc: 0.9423, Test Acc: 0.9439, LR: 0.000797\n",
      "Epoch 16/50: Train Loss: 0.2492, Test Loss: 0.2464, Train Acc: 0.9443, Test Acc: 0.9445, LR: 0.000797\n",
      "Epoch 17/50: Train Loss: 0.2453, Test Loss: 0.2426, Train Acc: 0.9451, Test Acc: 0.9472, LR: 0.000797\n",
      "Epoch 18/50: Train Loss: 0.2415, Test Loss: 0.2390, Train Acc: 0.9478, Test Acc: 0.9478, LR: 0.000797\n",
      "Epoch 19/50: Train Loss: 0.2383, Test Loss: 0.2357, Train Acc: 0.9480, Test Acc: 0.9471, LR: 0.000797\n",
      "Epoch 20/50: Train Loss: 0.2349, Test Loss: 0.2325, Train Acc: 0.9483, Test Acc: 0.9499, LR: 0.000718\n",
      "Epoch 21/50: Train Loss: 0.2321, Test Loss: 0.2299, Train Acc: 0.9504, Test Acc: 0.9507, LR: 0.000718\n",
      "Epoch 22/50: Train Loss: 0.2295, Test Loss: 0.2273, Train Acc: 0.9510, Test Acc: 0.9508, LR: 0.000718\n",
      "Epoch 23/50: Train Loss: 0.2271, Test Loss: 0.2250, Train Acc: 0.9512, Test Acc: 0.9510, LR: 0.000718\n",
      "Epoch 24/50: Train Loss: 0.2248, Test Loss: 0.2227, Train Acc: 0.9516, Test Acc: 0.9531, LR: 0.000718\n",
      "Epoch 25/50: Train Loss: 0.2226, Test Loss: 0.2205, Train Acc: 0.9531, Test Acc: 0.9533, LR: 0.000718\n",
      "Epoch 26/50: Train Loss: 0.2207, Test Loss: 0.2184, Train Acc: 0.9532, Test Acc: 0.9533, LR: 0.000718\n",
      "Epoch 27/50: Train Loss: 0.2185, Test Loss: 0.2164, Train Acc: 0.9533, Test Acc: 0.9534, LR: 0.000718\n",
      "Epoch 28/50: Train Loss: 0.2166, Test Loss: 0.2145, Train Acc: 0.9537, Test Acc: 0.9544, LR: 0.000718\n",
      "Epoch 29/50: Train Loss: 0.2147, Test Loss: 0.2127, Train Acc: 0.9543, Test Acc: 0.9548, LR: 0.000718\n",
      "Epoch 30/50: Train Loss: 0.2130, Test Loss: 0.2110, Train Acc: 0.9551, Test Acc: 0.9562, LR: 0.000646\n",
      "Epoch 31/50: Train Loss: 0.2113, Test Loss: 0.2094, Train Acc: 0.9561, Test Acc: 0.9563, LR: 0.000646\n",
      "Epoch 32/50: Train Loss: 0.2098, Test Loss: 0.2080, Train Acc: 0.9562, Test Acc: 0.9563, LR: 0.000646\n",
      "Epoch 33/50: Train Loss: 0.2084, Test Loss: 0.2066, Train Acc: 0.9562, Test Acc: 0.9564, LR: 0.000646\n",
      "Epoch 34/50: Train Loss: 0.2071, Test Loss: 0.2052, Train Acc: 0.9561, Test Acc: 0.9564, LR: 0.000646\n",
      "Epoch 35/50: Train Loss: 0.2057, Test Loss: 0.2039, Train Acc: 0.9559, Test Acc: 0.9561, LR: 0.000646\n",
      "Epoch 36/50: Train Loss: 0.2045, Test Loss: 0.2026, Train Acc: 0.9558, Test Acc: 0.9561, LR: 0.000646\n",
      "Epoch 37/50: Train Loss: 0.2032, Test Loss: 0.2014, Train Acc: 0.9559, Test Acc: 0.9565, LR: 0.000646\n",
      "Epoch 38/50: Train Loss: 0.2020, Test Loss: 0.2001, Train Acc: 0.9565, Test Acc: 0.9577, LR: 0.000646\n",
      "Epoch 39/50: Train Loss: 0.2008, Test Loss: 0.1990, Train Acc: 0.9576, Test Acc: 0.9587, LR: 0.000646\n",
      "Epoch 40/50: Train Loss: 0.1997, Test Loss: 0.1978, Train Acc: 0.9586, Test Acc: 0.9589, LR: 0.000581\n",
      "Epoch 41/50: Train Loss: 0.1986, Test Loss: 0.1968, Train Acc: 0.9587, Test Acc: 0.9589, LR: 0.000581\n",
      "Epoch 42/50: Train Loss: 0.1977, Test Loss: 0.1959, Train Acc: 0.9587, Test Acc: 0.9589, LR: 0.000581\n",
      "Epoch 43/50: Train Loss: 0.1967, Test Loss: 0.1949, Train Acc: 0.9587, Test Acc: 0.9590, LR: 0.000581\n",
      "Epoch 44/50: Train Loss: 0.1958, Test Loss: 0.1940, Train Acc: 0.9588, Test Acc: 0.9591, LR: 0.000581\n",
      "Epoch 45/50: Train Loss: 0.1950, Test Loss: 0.1931, Train Acc: 0.9591, Test Acc: 0.9595, LR: 0.000581\n",
      "Epoch 46/50: Train Loss: 0.1940, Test Loss: 0.1922, Train Acc: 0.9592, Test Acc: 0.9597, LR: 0.000581\n",
      "Epoch 47/50: Train Loss: 0.1932, Test Loss: 0.1914, Train Acc: 0.9593, Test Acc: 0.9597, LR: 0.000581\n",
      "Epoch 48/50: Train Loss: 0.1924, Test Loss: 0.1905, Train Acc: 0.9593, Test Acc: 0.9597, LR: 0.000581\n",
      "Epoch 49/50: Train Loss: 0.1915, Test Loss: 0.1897, Train Acc: 0.9595, Test Acc: 0.9601, LR: 0.000581\n",
      "Epoch 50/50: Train Loss: 0.1908, Test Loss: 0.1889, Train Acc: 0.9597, Test Acc: 0.9601, LR: 0.000523\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13448   854]\n",
      " [  413 17079]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.94      0.96     14302\n",
      "         1.0       0.95      0.98      0.96     17492\n",
      "\n",
      "    accuracy                           0.96     31794\n",
      "   macro avg       0.96      0.96      0.96     31794\n",
      "weighted avg       0.96      0.96      0.96     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.00046415888336127773, 'scheduler_step_size': 10, 'scheduler_gamma': 0.5, 'patience': 7, 'optimizer': 'sgd', 'num_epochs': 50, 'model': 'lr', 'learning_rate': 0.0026366508987303583, 'batch_size': 128}\n",
      "Epoch 1/50: Train Loss: 0.5493, Test Loss: 0.4446, Train Acc: 0.7713, Test Acc: 0.8887, LR: 0.002637\n",
      "Epoch 2/50: Train Loss: 0.4062, Test Loss: 0.3774, Train Acc: 0.8984, Test Acc: 0.9020, LR: 0.002637\n",
      "Epoch 3/50: Train Loss: 0.3585, Test Loss: 0.3420, Train Acc: 0.9099, Test Acc: 0.9150, LR: 0.002637\n",
      "Epoch 4/50: Train Loss: 0.3296, Test Loss: 0.3180, Train Acc: 0.9191, Test Acc: 0.9242, LR: 0.002637\n",
      "Epoch 5/50: Train Loss: 0.3090, Test Loss: 0.3000, Train Acc: 0.9266, Test Acc: 0.9280, LR: 0.002637\n",
      "Epoch 6/50: Train Loss: 0.2932, Test Loss: 0.2858, Train Acc: 0.9300, Test Acc: 0.9294, LR: 0.002637\n",
      "Epoch 7/50: Train Loss: 0.2804, Test Loss: 0.2741, Train Acc: 0.9326, Test Acc: 0.9324, LR: 0.002637\n",
      "Epoch 8/50: Train Loss: 0.2698, Test Loss: 0.2643, Train Acc: 0.9362, Test Acc: 0.9366, LR: 0.002637\n",
      "Epoch 9/50: Train Loss: 0.2608, Test Loss: 0.2560, Train Acc: 0.9392, Test Acc: 0.9397, LR: 0.002637\n",
      "Epoch 10/50: Train Loss: 0.2530, Test Loss: 0.2486, Train Acc: 0.9426, Test Acc: 0.9440, LR: 0.001318\n",
      "Epoch 11/50: Train Loss: 0.2478, Test Loss: 0.2453, Train Acc: 0.9445, Test Acc: 0.9445, LR: 0.001318\n",
      "Epoch 12/50: Train Loss: 0.2446, Test Loss: 0.2422, Train Acc: 0.9454, Test Acc: 0.9472, LR: 0.001318\n",
      "Epoch 13/50: Train Loss: 0.2416, Test Loss: 0.2393, Train Acc: 0.9478, Test Acc: 0.9474, LR: 0.001318\n",
      "Epoch 14/50: Train Loss: 0.2388, Test Loss: 0.2365, Train Acc: 0.9481, Test Acc: 0.9471, LR: 0.001318\n",
      "Epoch 15/50: Train Loss: 0.2361, Test Loss: 0.2338, Train Acc: 0.9479, Test Acc: 0.9474, LR: 0.001318\n",
      "Epoch 16/50: Train Loss: 0.2335, Test Loss: 0.2313, Train Acc: 0.9495, Test Acc: 0.9501, LR: 0.001318\n",
      "Epoch 17/50: Train Loss: 0.2311, Test Loss: 0.2289, Train Acc: 0.9508, Test Acc: 0.9507, LR: 0.001318\n",
      "Epoch 18/50: Train Loss: 0.2288, Test Loss: 0.2266, Train Acc: 0.9510, Test Acc: 0.9508, LR: 0.001318\n",
      "Epoch 19/50: Train Loss: 0.2266, Test Loss: 0.2245, Train Acc: 0.9514, Test Acc: 0.9511, LR: 0.001318\n",
      "Epoch 20/50: Train Loss: 0.2245, Test Loss: 0.2224, Train Acc: 0.9518, Test Acc: 0.9531, LR: 0.000659\n",
      "Epoch 21/50: Train Loss: 0.2229, Test Loss: 0.2214, Train Acc: 0.9531, Test Acc: 0.9532, LR: 0.000659\n",
      "Epoch 22/50: Train Loss: 0.2220, Test Loss: 0.2204, Train Acc: 0.9532, Test Acc: 0.9533, LR: 0.000659\n",
      "Epoch 23/50: Train Loss: 0.2210, Test Loss: 0.2194, Train Acc: 0.9532, Test Acc: 0.9533, LR: 0.000659\n",
      "Epoch 24/50: Train Loss: 0.2201, Test Loss: 0.2185, Train Acc: 0.9532, Test Acc: 0.9533, LR: 0.000659\n",
      "Epoch 25/50: Train Loss: 0.2191, Test Loss: 0.2176, Train Acc: 0.9532, Test Acc: 0.9533, LR: 0.000659\n",
      "Epoch 26/50: Train Loss: 0.2183, Test Loss: 0.2167, Train Acc: 0.9533, Test Acc: 0.9533, LR: 0.000659\n",
      "Epoch 27/50: Train Loss: 0.2173, Test Loss: 0.2158, Train Acc: 0.9533, Test Acc: 0.9534, LR: 0.000659\n",
      "Epoch 28/50: Train Loss: 0.2165, Test Loss: 0.2149, Train Acc: 0.9539, Test Acc: 0.9544, LR: 0.000659\n",
      "Epoch 29/50: Train Loss: 0.2156, Test Loss: 0.2140, Train Acc: 0.9541, Test Acc: 0.9544, LR: 0.000659\n",
      "Epoch 30/50: Train Loss: 0.2148, Test Loss: 0.2132, Train Acc: 0.9543, Test Acc: 0.9548, LR: 0.000330\n",
      "Epoch 31/50: Train Loss: 0.2142, Test Loss: 0.2128, Train Acc: 0.9547, Test Acc: 0.9548, LR: 0.000330\n",
      "Epoch 32/50: Train Loss: 0.2138, Test Loss: 0.2124, Train Acc: 0.9547, Test Acc: 0.9548, LR: 0.000330\n",
      "Epoch 33/50: Train Loss: 0.2134, Test Loss: 0.2120, Train Acc: 0.9547, Test Acc: 0.9548, LR: 0.000330\n",
      "Epoch 34/50: Train Loss: 0.2130, Test Loss: 0.2116, Train Acc: 0.9548, Test Acc: 0.9562, LR: 0.000330\n",
      "Epoch 35/50: Train Loss: 0.2125, Test Loss: 0.2112, Train Acc: 0.9561, Test Acc: 0.9562, LR: 0.000330\n",
      "Epoch 36/50: Train Loss: 0.2122, Test Loss: 0.2108, Train Acc: 0.9561, Test Acc: 0.9562, LR: 0.000330\n",
      "Epoch 37/50: Train Loss: 0.2118, Test Loss: 0.2104, Train Acc: 0.9561, Test Acc: 0.9562, LR: 0.000330\n",
      "Epoch 38/50: Train Loss: 0.2114, Test Loss: 0.2100, Train Acc: 0.9561, Test Acc: 0.9562, LR: 0.000330\n",
      "Epoch 39/50: Train Loss: 0.2110, Test Loss: 0.2096, Train Acc: 0.9562, Test Acc: 0.9563, LR: 0.000330\n",
      "Epoch 40/50: Train Loss: 0.2106, Test Loss: 0.2092, Train Acc: 0.9562, Test Acc: 0.9563, LR: 0.000165\n",
      "Epoch 41/50: Train Loss: 0.2103, Test Loss: 0.2090, Train Acc: 0.9562, Test Acc: 0.9563, LR: 0.000165\n",
      "Epoch 42/50: Train Loss: 0.2102, Test Loss: 0.2089, Train Acc: 0.9562, Test Acc: 0.9563, LR: 0.000165\n",
      "Epoch 43/50: Train Loss: 0.2100, Test Loss: 0.2087, Train Acc: 0.9562, Test Acc: 0.9563, LR: 0.000165\n",
      "Epoch 44/50: Train Loss: 0.2098, Test Loss: 0.2085, Train Acc: 0.9562, Test Acc: 0.9563, LR: 0.000165\n",
      "Epoch 45/50: Train Loss: 0.2096, Test Loss: 0.2083, Train Acc: 0.9562, Test Acc: 0.9563, LR: 0.000165\n",
      "Epoch 46/50: Train Loss: 0.2094, Test Loss: 0.2081, Train Acc: 0.9562, Test Acc: 0.9563, LR: 0.000165\n",
      "Epoch 47/50: Train Loss: 0.2093, Test Loss: 0.2079, Train Acc: 0.9562, Test Acc: 0.9563, LR: 0.000165\n",
      "Epoch 48/50: Train Loss: 0.2091, Test Loss: 0.2077, Train Acc: 0.9562, Test Acc: 0.9563, LR: 0.000165\n",
      "Epoch 49/50: Train Loss: 0.2089, Test Loss: 0.2076, Train Acc: 0.9562, Test Acc: 0.9563, LR: 0.000165\n",
      "Epoch 50/50: Train Loss: 0.2087, Test Loss: 0.2074, Train Acc: 0.9562, Test Acc: 0.9563, LR: 0.000082\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13365   937]\n",
      " [  451 17041]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.93      0.95     14302\n",
      "         1.0       0.95      0.97      0.96     17492\n",
      "\n",
      "    accuracy                           0.96     31794\n",
      "   macro avg       0.96      0.95      0.96     31794\n",
      "weighted avg       0.96      0.96      0.96     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.00021544346900318823, 'scheduler_step_size': 15, 'scheduler_gamma': 0.7, 'patience': 7, 'optimizer': 'adam', 'num_epochs': 75, 'model': 'lr', 'learning_rate': 0.00020691380811147902, 'batch_size': 256}\n",
      "Epoch 1/75: Train Loss: 0.6989, Test Loss: 0.6377, Train Acc: 0.5261, Test Acc: 0.7036, LR: 0.000207\n",
      "Epoch 2/75: Train Loss: 0.5931, Test Loss: 0.5552, Train Acc: 0.7151, Test Acc: 0.7271, LR: 0.000207\n",
      "Epoch 3/75: Train Loss: 0.5266, Test Loss: 0.5017, Train Acc: 0.7746, Test Acc: 0.8312, LR: 0.000207\n",
      "Epoch 4/75: Train Loss: 0.4816, Test Loss: 0.4635, Train Acc: 0.8532, Test Acc: 0.8661, LR: 0.000207\n",
      "Epoch 5/75: Train Loss: 0.4478, Test Loss: 0.4331, Train Acc: 0.8703, Test Acc: 0.8712, LR: 0.000207\n",
      "Epoch 6/75: Train Loss: 0.4201, Test Loss: 0.4074, Train Acc: 0.8786, Test Acc: 0.8788, LR: 0.000207\n",
      "Epoch 7/75: Train Loss: 0.3962, Test Loss: 0.3849, Train Acc: 0.8868, Test Acc: 0.8904, LR: 0.000207\n",
      "Epoch 8/75: Train Loss: 0.3750, Test Loss: 0.3649, Train Acc: 0.8962, Test Acc: 0.8979, LR: 0.000207\n",
      "Epoch 9/75: Train Loss: 0.3562, Test Loss: 0.3469, Train Acc: 0.9067, Test Acc: 0.9105, LR: 0.000207\n",
      "Epoch 10/75: Train Loss: 0.3392, Test Loss: 0.3307, Train Acc: 0.9152, Test Acc: 0.9181, LR: 0.000207\n",
      "Epoch 11/75: Train Loss: 0.3238, Test Loss: 0.3160, Train Acc: 0.9204, Test Acc: 0.9209, LR: 0.000207\n",
      "Epoch 12/75: Train Loss: 0.3099, Test Loss: 0.3026, Train Acc: 0.9226, Test Acc: 0.9215, LR: 0.000207\n",
      "Epoch 13/75: Train Loss: 0.2972, Test Loss: 0.2904, Train Acc: 0.9241, Test Acc: 0.9260, LR: 0.000207\n",
      "Epoch 14/75: Train Loss: 0.2857, Test Loss: 0.2793, Train Acc: 0.9290, Test Acc: 0.9302, LR: 0.000207\n",
      "Epoch 15/75: Train Loss: 0.2750, Test Loss: 0.2691, Train Acc: 0.9340, Test Acc: 0.9367, LR: 0.000145\n",
      "Epoch 16/75: Train Loss: 0.2667, Test Loss: 0.2623, Train Acc: 0.9372, Test Acc: 0.9399, LR: 0.000145\n",
      "Epoch 17/75: Train Loss: 0.2601, Test Loss: 0.2558, Train Acc: 0.9405, Test Acc: 0.9368, LR: 0.000145\n",
      "Epoch 18/75: Train Loss: 0.2538, Test Loss: 0.2496, Train Acc: 0.9388, Test Acc: 0.9403, LR: 0.000145\n",
      "Epoch 19/75: Train Loss: 0.2478, Test Loss: 0.2437, Train Acc: 0.9412, Test Acc: 0.9416, LR: 0.000145\n",
      "Epoch 20/75: Train Loss: 0.2421, Test Loss: 0.2381, Train Acc: 0.9425, Test Acc: 0.9441, LR: 0.000145\n",
      "Epoch 21/75: Train Loss: 0.2366, Test Loss: 0.2328, Train Acc: 0.9443, Test Acc: 0.9446, LR: 0.000145\n",
      "Epoch 22/75: Train Loss: 0.2315, Test Loss: 0.2278, Train Acc: 0.9454, Test Acc: 0.9462, LR: 0.000145\n",
      "Epoch 23/75: Train Loss: 0.2266, Test Loss: 0.2229, Train Acc: 0.9476, Test Acc: 0.9485, LR: 0.000145\n",
      "Epoch 24/75: Train Loss: 0.2219, Test Loss: 0.2184, Train Acc: 0.9486, Test Acc: 0.9489, LR: 0.000145\n",
      "Epoch 25/75: Train Loss: 0.2175, Test Loss: 0.2140, Train Acc: 0.9493, Test Acc: 0.9495, LR: 0.000145\n",
      "Epoch 26/75: Train Loss: 0.2132, Test Loss: 0.2098, Train Acc: 0.9508, Test Acc: 0.9518, LR: 0.000145\n",
      "Epoch 27/75: Train Loss: 0.2092, Test Loss: 0.2059, Train Acc: 0.9520, Test Acc: 0.9521, LR: 0.000145\n",
      "Epoch 28/75: Train Loss: 0.2054, Test Loss: 0.2021, Train Acc: 0.9524, Test Acc: 0.9525, LR: 0.000145\n",
      "Epoch 29/75: Train Loss: 0.2017, Test Loss: 0.1985, Train Acc: 0.9530, Test Acc: 0.9526, LR: 0.000145\n",
      "Epoch 30/75: Train Loss: 0.1982, Test Loss: 0.1950, Train Acc: 0.9547, Test Acc: 0.9586, LR: 0.000101\n",
      "Epoch 31/75: Train Loss: 0.1953, Test Loss: 0.1927, Train Acc: 0.9585, Test Acc: 0.9593, LR: 0.000101\n",
      "Epoch 32/75: Train Loss: 0.1930, Test Loss: 0.1904, Train Acc: 0.9592, Test Acc: 0.9599, LR: 0.000101\n",
      "Epoch 33/75: Train Loss: 0.1908, Test Loss: 0.1882, Train Acc: 0.9599, Test Acc: 0.9603, LR: 0.000101\n",
      "Epoch 34/75: Train Loss: 0.1887, Test Loss: 0.1860, Train Acc: 0.9601, Test Acc: 0.9606, LR: 0.000101\n",
      "Epoch 35/75: Train Loss: 0.1865, Test Loss: 0.1839, Train Acc: 0.9602, Test Acc: 0.9608, LR: 0.000101\n",
      "Epoch 36/75: Train Loss: 0.1845, Test Loss: 0.1819, Train Acc: 0.9607, Test Acc: 0.9613, LR: 0.000101\n",
      "Epoch 37/75: Train Loss: 0.1825, Test Loss: 0.1799, Train Acc: 0.9616, Test Acc: 0.9626, LR: 0.000101\n",
      "Epoch 38/75: Train Loss: 0.1806, Test Loss: 0.1780, Train Acc: 0.9619, Test Acc: 0.9626, LR: 0.000101\n",
      "Epoch 39/75: Train Loss: 0.1787, Test Loss: 0.1761, Train Acc: 0.9623, Test Acc: 0.9629, LR: 0.000101\n",
      "Epoch 40/75: Train Loss: 0.1769, Test Loss: 0.1743, Train Acc: 0.9624, Test Acc: 0.9630, LR: 0.000101\n",
      "Epoch 41/75: Train Loss: 0.1752, Test Loss: 0.1725, Train Acc: 0.9625, Test Acc: 0.9630, LR: 0.000101\n",
      "Epoch 42/75: Train Loss: 0.1735, Test Loss: 0.1708, Train Acc: 0.9627, Test Acc: 0.9635, LR: 0.000101\n",
      "Epoch 43/75: Train Loss: 0.1718, Test Loss: 0.1692, Train Acc: 0.9628, Test Acc: 0.9636, LR: 0.000101\n",
      "Epoch 44/75: Train Loss: 0.1702, Test Loss: 0.1676, Train Acc: 0.9629, Test Acc: 0.9639, LR: 0.000101\n",
      "Epoch 45/75: Train Loss: 0.1686, Test Loss: 0.1660, Train Acc: 0.9638, Test Acc: 0.9651, LR: 0.000071\n",
      "Epoch 46/75: Train Loss: 0.1674, Test Loss: 0.1650, Train Acc: 0.9649, Test Acc: 0.9652, LR: 0.000071\n",
      "Epoch 47/75: Train Loss: 0.1663, Test Loss: 0.1639, Train Acc: 0.9650, Test Acc: 0.9653, LR: 0.000071\n",
      "Epoch 48/75: Train Loss: 0.1653, Test Loss: 0.1629, Train Acc: 0.9650, Test Acc: 0.9654, LR: 0.000071\n",
      "Epoch 49/75: Train Loss: 0.1643, Test Loss: 0.1619, Train Acc: 0.9652, Test Acc: 0.9655, LR: 0.000071\n",
      "Epoch 50/75: Train Loss: 0.1633, Test Loss: 0.1609, Train Acc: 0.9653, Test Acc: 0.9655, LR: 0.000071\n",
      "Epoch 51/75: Train Loss: 0.1623, Test Loss: 0.1599, Train Acc: 0.9654, Test Acc: 0.9660, LR: 0.000071\n",
      "Epoch 52/75: Train Loss: 0.1614, Test Loss: 0.1589, Train Acc: 0.9655, Test Acc: 0.9660, LR: 0.000071\n",
      "Epoch 53/75: Train Loss: 0.1605, Test Loss: 0.1580, Train Acc: 0.9656, Test Acc: 0.9661, LR: 0.000071\n",
      "Epoch 54/75: Train Loss: 0.1595, Test Loss: 0.1571, Train Acc: 0.9656, Test Acc: 0.9662, LR: 0.000071\n",
      "Epoch 55/75: Train Loss: 0.1587, Test Loss: 0.1562, Train Acc: 0.9656, Test Acc: 0.9663, LR: 0.000071\n",
      "Epoch 56/75: Train Loss: 0.1578, Test Loss: 0.1553, Train Acc: 0.9657, Test Acc: 0.9663, LR: 0.000071\n",
      "Epoch 57/75: Train Loss: 0.1570, Test Loss: 0.1545, Train Acc: 0.9657, Test Acc: 0.9663, LR: 0.000071\n",
      "Epoch 58/75: Train Loss: 0.1561, Test Loss: 0.1536, Train Acc: 0.9659, Test Acc: 0.9664, LR: 0.000071\n",
      "Epoch 59/75: Train Loss: 0.1553, Test Loss: 0.1528, Train Acc: 0.9662, Test Acc: 0.9675, LR: 0.000071\n",
      "Epoch 60/75: Train Loss: 0.1545, Test Loss: 0.1520, Train Acc: 0.9676, Test Acc: 0.9675, LR: 0.000050\n",
      "Epoch 61/75: Train Loss: 0.1538, Test Loss: 0.1514, Train Acc: 0.9676, Test Acc: 0.9676, LR: 0.000050\n",
      "Epoch 62/75: Train Loss: 0.1533, Test Loss: 0.1509, Train Acc: 0.9676, Test Acc: 0.9676, LR: 0.000050\n",
      "Epoch 63/75: Train Loss: 0.1528, Test Loss: 0.1503, Train Acc: 0.9676, Test Acc: 0.9676, LR: 0.000050\n",
      "Epoch 64/75: Train Loss: 0.1522, Test Loss: 0.1498, Train Acc: 0.9676, Test Acc: 0.9676, LR: 0.000050\n",
      "Epoch 65/75: Train Loss: 0.1517, Test Loss: 0.1493, Train Acc: 0.9676, Test Acc: 0.9678, LR: 0.000050\n",
      "Epoch 66/75: Train Loss: 0.1512, Test Loss: 0.1487, Train Acc: 0.9678, Test Acc: 0.9678, LR: 0.000050\n",
      "Epoch 67/75: Train Loss: 0.1507, Test Loss: 0.1482, Train Acc: 0.9678, Test Acc: 0.9678, LR: 0.000050\n",
      "Epoch 68/75: Train Loss: 0.1502, Test Loss: 0.1477, Train Acc: 0.9678, Test Acc: 0.9678, LR: 0.000050\n",
      "Epoch 69/75: Train Loss: 0.1497, Test Loss: 0.1472, Train Acc: 0.9678, Test Acc: 0.9678, LR: 0.000050\n",
      "Epoch 70/75: Train Loss: 0.1492, Test Loss: 0.1467, Train Acc: 0.9678, Test Acc: 0.9679, LR: 0.000050\n",
      "Epoch 71/75: Train Loss: 0.1488, Test Loss: 0.1462, Train Acc: 0.9679, Test Acc: 0.9681, LR: 0.000050\n",
      "Epoch 72/75: Train Loss: 0.1483, Test Loss: 0.1458, Train Acc: 0.9679, Test Acc: 0.9681, LR: 0.000050\n",
      "Epoch 73/75: Train Loss: 0.1478, Test Loss: 0.1453, Train Acc: 0.9679, Test Acc: 0.9681, LR: 0.000050\n",
      "Epoch 74/75: Train Loss: 0.1474, Test Loss: 0.1448, Train Acc: 0.9680, Test Acc: 0.9681, LR: 0.000050\n",
      "Epoch 75/75: Train Loss: 0.1469, Test Loss: 0.1444, Train Acc: 0.9680, Test Acc: 0.9681, LR: 0.000035\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13591   711]\n",
      " [  303 17189]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.95      0.96     14302\n",
      "         1.0       0.96      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.00046415888336127773, 'scheduler_step_size': 10, 'scheduler_gamma': 0.7, 'patience': 5, 'optimizer': 'adam', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.1, 'batch_size': 256}\n",
      "Epoch 1/100: Train Loss: 0.1487, Test Loss: 0.1210, Train Acc: 0.9643, Test Acc: 0.9746, LR: 0.100000\n",
      "Epoch 2/100: Train Loss: 0.1242, Test Loss: 0.1198, Train Acc: 0.9738, Test Acc: 0.9744, LR: 0.100000\n",
      "Epoch 3/100: Train Loss: 0.1242, Test Loss: 0.1204, Train Acc: 0.9738, Test Acc: 0.9743, LR: 0.100000\n",
      "Epoch 4/100: Train Loss: 0.1242, Test Loss: 0.1209, Train Acc: 0.9738, Test Acc: 0.9746, LR: 0.100000\n",
      "Epoch 5/100: Train Loss: 0.1239, Test Loss: 0.1233, Train Acc: 0.9740, Test Acc: 0.9724, LR: 0.100000\n",
      "Epoch 6/100: Train Loss: 0.1241, Test Loss: 0.1197, Train Acc: 0.9737, Test Acc: 0.9745, LR: 0.100000\n",
      "Epoch 7/100: Train Loss: 0.1240, Test Loss: 0.1174, Train Acc: 0.9738, Test Acc: 0.9742, LR: 0.100000\n",
      "Epoch 8/100: Train Loss: 0.1240, Test Loss: 0.1201, Train Acc: 0.9738, Test Acc: 0.9730, LR: 0.100000\n",
      "Epoch 9/100: Train Loss: 0.1240, Test Loss: 0.1200, Train Acc: 0.9738, Test Acc: 0.9742, LR: 0.100000\n",
      "Epoch 10/100: Train Loss: 0.1241, Test Loss: 0.1200, Train Acc: 0.9739, Test Acc: 0.9743, LR: 0.070000\n",
      "Epoch 11/100: Train Loss: 0.1238, Test Loss: 0.1196, Train Acc: 0.9739, Test Acc: 0.9747, LR: 0.070000\n",
      "Epoch 12/100: Train Loss: 0.1239, Test Loss: 0.1224, Train Acc: 0.9739, Test Acc: 0.9746, LR: 0.070000\n",
      "Early stopping triggered after 12 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13724   578]\n",
      " [  231 17261]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.99      0.98     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.98      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.004641588833612777, 'scheduler_step_size': 10, 'scheduler_gamma': 0.9, 'patience': 7, 'optimizer': 'adam', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.0026366508987303583, 'batch_size': 256}\n",
      "Epoch 1/30: Train Loss: 0.4426, Test Loss: 0.3269, Train Acc: 0.8390, Test Acc: 0.9196, LR: 0.002637\n",
      "Epoch 2/30: Train Loss: 0.2858, Test Loss: 0.2539, Train Acc: 0.9324, Test Acc: 0.9438, LR: 0.002637\n",
      "Epoch 3/30: Train Loss: 0.2362, Test Loss: 0.2196, Train Acc: 0.9484, Test Acc: 0.9512, LR: 0.002637\n",
      "Epoch 4/30: Train Loss: 0.2105, Test Loss: 0.2000, Train Acc: 0.9546, Test Acc: 0.9585, LR: 0.002637\n",
      "Epoch 5/30: Train Loss: 0.1955, Test Loss: 0.1886, Train Acc: 0.9598, Test Acc: 0.9607, LR: 0.002637\n",
      "Epoch 6/30: Train Loss: 0.1867, Test Loss: 0.1817, Train Acc: 0.9611, Test Acc: 0.9635, LR: 0.002637\n",
      "Epoch 7/30: Train Loss: 0.1816, Test Loss: 0.1780, Train Acc: 0.9630, Test Acc: 0.9638, LR: 0.002637\n",
      "Epoch 8/30: Train Loss: 0.1788, Test Loss: 0.1763, Train Acc: 0.9637, Test Acc: 0.9639, LR: 0.002637\n",
      "Epoch 9/30: Train Loss: 0.1777, Test Loss: 0.1754, Train Acc: 0.9639, Test Acc: 0.9640, LR: 0.002637\n",
      "Epoch 10/30: Train Loss: 0.1771, Test Loss: 0.1750, Train Acc: 0.9640, Test Acc: 0.9649, LR: 0.002373\n",
      "Epoch 11/30: Train Loss: 0.1769, Test Loss: 0.1750, Train Acc: 0.9640, Test Acc: 0.9639, LR: 0.002373\n",
      "Epoch 12/30: Train Loss: 0.1768, Test Loss: 0.1749, Train Acc: 0.9639, Test Acc: 0.9643, LR: 0.002373\n",
      "Epoch 13/30: Train Loss: 0.1767, Test Loss: 0.1747, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.002373\n",
      "Epoch 14/30: Train Loss: 0.1766, Test Loss: 0.1752, Train Acc: 0.9641, Test Acc: 0.9644, LR: 0.002373\n",
      "Epoch 15/30: Train Loss: 0.1769, Test Loss: 0.1746, Train Acc: 0.9642, Test Acc: 0.9650, LR: 0.002373\n",
      "Epoch 16/30: Train Loss: 0.1766, Test Loss: 0.1748, Train Acc: 0.9640, Test Acc: 0.9646, LR: 0.002373\n",
      "Epoch 17/30: Train Loss: 0.1769, Test Loss: 0.1743, Train Acc: 0.9640, Test Acc: 0.9646, LR: 0.002373\n",
      "Epoch 18/30: Train Loss: 0.1766, Test Loss: 0.1746, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.002373\n",
      "Epoch 19/30: Train Loss: 0.1767, Test Loss: 0.1746, Train Acc: 0.9640, Test Acc: 0.9646, LR: 0.002373\n",
      "Epoch 20/30: Train Loss: 0.1766, Test Loss: 0.1749, Train Acc: 0.9641, Test Acc: 0.9646, LR: 0.002136\n",
      "Epoch 21/30: Train Loss: 0.1768, Test Loss: 0.1749, Train Acc: 0.9642, Test Acc: 0.9640, LR: 0.002136\n",
      "Epoch 22/30: Train Loss: 0.1766, Test Loss: 0.1750, Train Acc: 0.9639, Test Acc: 0.9645, LR: 0.002136\n",
      "Epoch 23/30: Train Loss: 0.1768, Test Loss: 0.1747, Train Acc: 0.9640, Test Acc: 0.9645, LR: 0.002136\n",
      "Epoch 24/30: Train Loss: 0.1768, Test Loss: 0.1744, Train Acc: 0.9640, Test Acc: 0.9648, LR: 0.002136\n",
      "Early stopping triggered after 24 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13540   762]\n",
      " [  357 17135]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.95      0.96     14302\n",
      "         1.0       0.96      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.96     31794\n",
      "   macro avg       0.97      0.96      0.96     31794\n",
      "weighted avg       0.97      0.96      0.96     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.004641588833612777, 'scheduler_step_size': 10, 'scheduler_gamma': 0.5, 'patience': 7, 'optimizer': 'sgd', 'num_epochs': 50, 'model': 'lr', 'learning_rate': 0.0001, 'batch_size': 128}\n",
      "Epoch 1/50: Train Loss: 0.7534, Test Loss: 0.7370, Train Acc: 0.4374, Test Acc: 0.4530, LR: 0.000100\n",
      "Epoch 2/50: Train Loss: 0.7220, Test Loss: 0.7074, Train Acc: 0.4644, Test Acc: 0.4846, LR: 0.000100\n",
      "Epoch 3/50: Train Loss: 0.6939, Test Loss: 0.6809, Train Acc: 0.5172, Test Acc: 0.5339, LR: 0.000100\n",
      "Epoch 4/50: Train Loss: 0.6687, Test Loss: 0.6572, Train Acc: 0.5566, Test Acc: 0.5986, LR: 0.000100\n",
      "Epoch 5/50: Train Loss: 0.6463, Test Loss: 0.6360, Train Acc: 0.6348, Test Acc: 0.6709, LR: 0.000100\n",
      "Epoch 6/50: Train Loss: 0.6261, Test Loss: 0.6170, Train Acc: 0.6908, Test Acc: 0.7191, LR: 0.000100\n",
      "Epoch 7/50: Train Loss: 0.6080, Test Loss: 0.5999, Train Acc: 0.7325, Test Acc: 0.7499, LR: 0.000100\n",
      "Epoch 8/50: Train Loss: 0.5918, Test Loss: 0.5845, Train Acc: 0.7506, Test Acc: 0.7567, LR: 0.000100\n",
      "Epoch 9/50: Train Loss: 0.5770, Test Loss: 0.5705, Train Acc: 0.7573, Test Acc: 0.7597, LR: 0.000100\n",
      "Epoch 10/50: Train Loss: 0.5637, Test Loss: 0.5578, Train Acc: 0.7632, Test Acc: 0.7665, LR: 0.000050\n",
      "Epoch 11/50: Train Loss: 0.5545, Test Loss: 0.5519, Train Acc: 0.7660, Test Acc: 0.7677, LR: 0.000050\n",
      "Epoch 12/50: Train Loss: 0.5487, Test Loss: 0.5463, Train Acc: 0.7684, Test Acc: 0.7808, LR: 0.000050\n",
      "Epoch 13/50: Train Loss: 0.5432, Test Loss: 0.5409, Train Acc: 0.7782, Test Acc: 0.7817, LR: 0.000050\n",
      "Epoch 14/50: Train Loss: 0.5379, Test Loss: 0.5357, Train Acc: 0.7915, Test Acc: 0.7972, LR: 0.000050\n",
      "Epoch 15/50: Train Loss: 0.5329, Test Loss: 0.5308, Train Acc: 0.8023, Test Acc: 0.8110, LR: 0.000050\n",
      "Epoch 16/50: Train Loss: 0.5280, Test Loss: 0.5260, Train Acc: 0.8113, Test Acc: 0.8264, LR: 0.000050\n",
      "Epoch 17/50: Train Loss: 0.5234, Test Loss: 0.5215, Train Acc: 0.8231, Test Acc: 0.8267, LR: 0.000050\n",
      "Epoch 18/50: Train Loss: 0.5189, Test Loss: 0.5171, Train Acc: 0.8305, Test Acc: 0.8392, LR: 0.000050\n",
      "Epoch 19/50: Train Loss: 0.5147, Test Loss: 0.5129, Train Acc: 0.8358, Test Acc: 0.8397, LR: 0.000050\n",
      "Epoch 20/50: Train Loss: 0.5105, Test Loss: 0.5089, Train Acc: 0.8467, Test Acc: 0.8499, LR: 0.000025\n",
      "Epoch 21/50: Train Loss: 0.5075, Test Loss: 0.5069, Train Acc: 0.8470, Test Acc: 0.8505, LR: 0.000025\n",
      "Epoch 22/50: Train Loss: 0.5056, Test Loss: 0.5050, Train Acc: 0.8483, Test Acc: 0.8611, LR: 0.000025\n",
      "Epoch 23/50: Train Loss: 0.5037, Test Loss: 0.5031, Train Acc: 0.8575, Test Acc: 0.8604, LR: 0.000025\n",
      "Epoch 24/50: Train Loss: 0.5018, Test Loss: 0.5012, Train Acc: 0.8583, Test Acc: 0.8638, LR: 0.000025\n",
      "Epoch 25/50: Train Loss: 0.5000, Test Loss: 0.4994, Train Acc: 0.8616, Test Acc: 0.8644, LR: 0.000025\n",
      "Epoch 26/50: Train Loss: 0.4981, Test Loss: 0.4976, Train Acc: 0.8592, Test Acc: 0.8606, LR: 0.000025\n",
      "Epoch 27/50: Train Loss: 0.4964, Test Loss: 0.4958, Train Acc: 0.8686, Test Acc: 0.8715, LR: 0.000025\n",
      "Epoch 28/50: Train Loss: 0.4946, Test Loss: 0.4941, Train Acc: 0.8707, Test Acc: 0.8720, LR: 0.000025\n",
      "Epoch 29/50: Train Loss: 0.4929, Test Loss: 0.4924, Train Acc: 0.8697, Test Acc: 0.8699, LR: 0.000025\n",
      "Epoch 30/50: Train Loss: 0.4912, Test Loss: 0.4907, Train Acc: 0.8694, Test Acc: 0.8705, LR: 0.000013\n",
      "Epoch 31/50: Train Loss: 0.4900, Test Loss: 0.4899, Train Acc: 0.8696, Test Acc: 0.8705, LR: 0.000013\n",
      "Epoch 32/50: Train Loss: 0.4892, Test Loss: 0.4891, Train Acc: 0.8746, Test Acc: 0.8814, LR: 0.000013\n",
      "Epoch 33/50: Train Loss: 0.4884, Test Loss: 0.4883, Train Acc: 0.8803, Test Acc: 0.8814, LR: 0.000013\n",
      "Epoch 34/50: Train Loss: 0.4876, Test Loss: 0.4875, Train Acc: 0.8802, Test Acc: 0.8793, LR: 0.000013\n",
      "Epoch 35/50: Train Loss: 0.4867, Test Loss: 0.4867, Train Acc: 0.8781, Test Acc: 0.8794, LR: 0.000013\n",
      "Epoch 36/50: Train Loss: 0.4859, Test Loss: 0.4859, Train Acc: 0.8789, Test Acc: 0.8827, LR: 0.000013\n",
      "Epoch 37/50: Train Loss: 0.4852, Test Loss: 0.4851, Train Acc: 0.8813, Test Acc: 0.8833, LR: 0.000013\n",
      "Epoch 38/50: Train Loss: 0.4844, Test Loss: 0.4843, Train Acc: 0.8816, Test Acc: 0.8834, LR: 0.000013\n",
      "Epoch 39/50: Train Loss: 0.4836, Test Loss: 0.4836, Train Acc: 0.8817, Test Acc: 0.8838, LR: 0.000013\n",
      "Epoch 40/50: Train Loss: 0.4829, Test Loss: 0.4828, Train Acc: 0.8819, Test Acc: 0.8838, LR: 0.000006\n",
      "Epoch 41/50: Train Loss: 0.4823, Test Loss: 0.4824, Train Acc: 0.8820, Test Acc: 0.8838, LR: 0.000006\n",
      "Epoch 42/50: Train Loss: 0.4819, Test Loss: 0.4820, Train Acc: 0.8820, Test Acc: 0.8838, LR: 0.000006\n",
      "Epoch 43/50: Train Loss: 0.4815, Test Loss: 0.4817, Train Acc: 0.8820, Test Acc: 0.8839, LR: 0.000006\n",
      "Epoch 44/50: Train Loss: 0.4812, Test Loss: 0.4813, Train Acc: 0.8816, Test Acc: 0.8884, LR: 0.000006\n",
      "Epoch 45/50: Train Loss: 0.4808, Test Loss: 0.4809, Train Acc: 0.8877, Test Acc: 0.8870, LR: 0.000006\n",
      "Epoch 46/50: Train Loss: 0.4804, Test Loss: 0.4805, Train Acc: 0.8876, Test Acc: 0.8882, LR: 0.000006\n",
      "Epoch 47/50: Train Loss: 0.4801, Test Loss: 0.4802, Train Acc: 0.8883, Test Acc: 0.8884, LR: 0.000006\n",
      "Epoch 48/50: Train Loss: 0.4797, Test Loss: 0.4798, Train Acc: 0.8886, Test Acc: 0.8885, LR: 0.000006\n",
      "Epoch 49/50: Train Loss: 0.4793, Test Loss: 0.4794, Train Acc: 0.8886, Test Acc: 0.8885, LR: 0.000006\n",
      "Epoch 50/50: Train Loss: 0.4789, Test Loss: 0.4791, Train Acc: 0.8886, Test Acc: 0.8885, LR: 0.000003\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12138  2164]\n",
      " [ 1380 16112]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.85      0.87     14302\n",
      "         1.0       0.88      0.92      0.90     17492\n",
      "\n",
      "    accuracy                           0.89     31794\n",
      "   macro avg       0.89      0.88      0.89     31794\n",
      "weighted avg       0.89      0.89      0.89     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 4.641588833612782e-05, 'scheduler_step_size': 15, 'scheduler_gamma': 0.9, 'patience': 5, 'optimizer': 'rmsprop', 'num_epochs': 50, 'model': 'lr', 'learning_rate': 0.011288378916846883, 'batch_size': 256}\n",
      "Epoch 1/50: Train Loss: 0.2113, Test Loss: 0.1407, Train Acc: 0.9494, Test Acc: 0.9698, LR: 0.011288\n",
      "Epoch 2/50: Train Loss: 0.1271, Test Loss: 0.1146, Train Acc: 0.9731, Test Acc: 0.9752, LR: 0.011288\n",
      "Epoch 3/50: Train Loss: 0.1166, Test Loss: 0.1110, Train Acc: 0.9749, Test Acc: 0.9755, LR: 0.011288\n",
      "Epoch 4/50: Train Loss: 0.1154, Test Loss: 0.1105, Train Acc: 0.9752, Test Acc: 0.9755, LR: 0.011288\n",
      "Epoch 5/50: Train Loss: 0.1152, Test Loss: 0.1105, Train Acc: 0.9754, Test Acc: 0.9755, LR: 0.011288\n",
      "Epoch 6/50: Train Loss: 0.1152, Test Loss: 0.1102, Train Acc: 0.9753, Test Acc: 0.9759, LR: 0.011288\n",
      "Epoch 7/50: Train Loss: 0.1152, Test Loss: 0.1102, Train Acc: 0.9752, Test Acc: 0.9755, LR: 0.011288\n",
      "Epoch 8/50: Train Loss: 0.1151, Test Loss: 0.1105, Train Acc: 0.9754, Test Acc: 0.9755, LR: 0.011288\n",
      "Epoch 9/50: Train Loss: 0.1151, Test Loss: 0.1103, Train Acc: 0.9754, Test Acc: 0.9761, LR: 0.011288\n",
      "Epoch 10/50: Train Loss: 0.1152, Test Loss: 0.1103, Train Acc: 0.9752, Test Acc: 0.9763, LR: 0.011288\n",
      "Epoch 11/50: Train Loss: 0.1151, Test Loss: 0.1105, Train Acc: 0.9752, Test Acc: 0.9755, LR: 0.011288\n",
      "Early stopping triggered after 11 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13790   512]\n",
      " [  267 17225]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.98     31794\n",
      "   macro avg       0.98      0.97      0.98     31794\n",
      "weighted avg       0.98      0.98      0.98     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 1e-05, 'scheduler_step_size': 10, 'scheduler_gamma': 0.7, 'patience': 5, 'optimizer': 'adam', 'num_epochs': 50, 'model': 'lr', 'learning_rate': 0.0018329807108324356, 'batch_size': 128}\n",
      "Epoch 1/50: Train Loss: 0.4046, Test Loss: 0.2865, Train Acc: 0.8629, Test Acc: 0.9271, LR: 0.001833\n",
      "Epoch 2/50: Train Loss: 0.2452, Test Loss: 0.2123, Train Acc: 0.9445, Test Acc: 0.9562, LR: 0.001833\n",
      "Epoch 3/50: Train Loss: 0.1936, Test Loss: 0.1753, Train Acc: 0.9585, Test Acc: 0.9625, LR: 0.001833\n",
      "Epoch 4/50: Train Loss: 0.1650, Test Loss: 0.1526, Train Acc: 0.9646, Test Acc: 0.9670, LR: 0.001833\n",
      "Epoch 5/50: Train Loss: 0.1471, Test Loss: 0.1379, Train Acc: 0.9690, Test Acc: 0.9706, LR: 0.001833\n",
      "Epoch 6/50: Train Loss: 0.1353, Test Loss: 0.1280, Train Acc: 0.9715, Test Acc: 0.9732, LR: 0.001833\n",
      "Epoch 7/50: Train Loss: 0.1275, Test Loss: 0.1214, Train Acc: 0.9728, Test Acc: 0.9735, LR: 0.001833\n",
      "Epoch 8/50: Train Loss: 0.1227, Test Loss: 0.1173, Train Acc: 0.9738, Test Acc: 0.9751, LR: 0.001833\n",
      "Epoch 9/50: Train Loss: 0.1195, Test Loss: 0.1145, Train Acc: 0.9744, Test Acc: 0.9751, LR: 0.001833\n",
      "Epoch 10/50: Train Loss: 0.1175, Test Loss: 0.1129, Train Acc: 0.9748, Test Acc: 0.9755, LR: 0.001283\n",
      "Epoch 11/50: Train Loss: 0.1164, Test Loss: 0.1121, Train Acc: 0.9751, Test Acc: 0.9755, LR: 0.001283\n",
      "Epoch 12/50: Train Loss: 0.1158, Test Loss: 0.1114, Train Acc: 0.9751, Test Acc: 0.9755, LR: 0.001283\n",
      "Epoch 13/50: Train Loss: 0.1154, Test Loss: 0.1110, Train Acc: 0.9751, Test Acc: 0.9755, LR: 0.001283\n",
      "Epoch 14/50: Train Loss: 0.1152, Test Loss: 0.1107, Train Acc: 0.9751, Test Acc: 0.9755, LR: 0.001283\n",
      "Epoch 15/50: Train Loss: 0.1150, Test Loss: 0.1105, Train Acc: 0.9752, Test Acc: 0.9755, LR: 0.001283\n",
      "Epoch 16/50: Train Loss: 0.1149, Test Loss: 0.1103, Train Acc: 0.9751, Test Acc: 0.9755, LR: 0.001283\n",
      "Epoch 17/50: Train Loss: 0.1148, Test Loss: 0.1102, Train Acc: 0.9752, Test Acc: 0.9755, LR: 0.001283\n",
      "Epoch 18/50: Train Loss: 0.1147, Test Loss: 0.1101, Train Acc: 0.9751, Test Acc: 0.9755, LR: 0.001283\n",
      "Epoch 19/50: Train Loss: 0.1147, Test Loss: 0.1101, Train Acc: 0.9753, Test Acc: 0.9755, LR: 0.001283\n",
      "Epoch 20/50: Train Loss: 0.1146, Test Loss: 0.1101, Train Acc: 0.9755, Test Acc: 0.9755, LR: 0.000898\n",
      "Epoch 21/50: Train Loss: 0.1146, Test Loss: 0.1100, Train Acc: 0.9754, Test Acc: 0.9764, LR: 0.000898\n",
      "Epoch 22/50: Train Loss: 0.1146, Test Loss: 0.1100, Train Acc: 0.9755, Test Acc: 0.9755, LR: 0.000898\n",
      "Epoch 23/50: Train Loss: 0.1147, Test Loss: 0.1100, Train Acc: 0.9754, Test Acc: 0.9764, LR: 0.000898\n",
      "Epoch 24/50: Train Loss: 0.1146, Test Loss: 0.1100, Train Acc: 0.9756, Test Acc: 0.9755, LR: 0.000898\n",
      "Epoch 25/50: Train Loss: 0.1146, Test Loss: 0.1100, Train Acc: 0.9758, Test Acc: 0.9755, LR: 0.000898\n",
      "Epoch 26/50: Train Loss: 0.1146, Test Loss: 0.1100, Train Acc: 0.9756, Test Acc: 0.9755, LR: 0.000898\n",
      "Early stopping triggered after 26 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13789   513]\n",
      " [  265 17227]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.98     31794\n",
      "   macro avg       0.98      0.97      0.98     31794\n",
      "weighted avg       0.98      0.98      0.98     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.004641588833612777, 'scheduler_step_size': 15, 'scheduler_gamma': 0.7, 'patience': 7, 'optimizer': 'sgd', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.0008858667904100823, 'batch_size': 64}\n",
      "Epoch 1/30: Train Loss: 0.5905, Test Loss: 0.4893, Train Acc: 0.7149, Test Acc: 0.8814, LR: 0.000886\n",
      "Epoch 2/30: Train Loss: 0.4469, Test Loss: 0.4157, Train Acc: 0.8909, Test Acc: 0.8953, LR: 0.000886\n",
      "Epoch 3/30: Train Loss: 0.3954, Test Loss: 0.3783, Train Acc: 0.9005, Test Acc: 0.9020, LR: 0.000886\n",
      "Epoch 4/30: Train Loss: 0.3653, Test Loss: 0.3536, Train Acc: 0.9077, Test Acc: 0.9104, LR: 0.000886\n",
      "Epoch 5/30: Train Loss: 0.3442, Test Loss: 0.3352, Train Acc: 0.9152, Test Acc: 0.9178, LR: 0.000886\n",
      "Epoch 6/30: Train Loss: 0.3279, Test Loss: 0.3206, Train Acc: 0.9204, Test Acc: 0.9241, LR: 0.000886\n",
      "Epoch 7/30: Train Loss: 0.3148, Test Loss: 0.3087, Train Acc: 0.9261, Test Acc: 0.9256, LR: 0.000886\n",
      "Epoch 8/30: Train Loss: 0.3039, Test Loss: 0.2986, Train Acc: 0.9279, Test Acc: 0.9286, LR: 0.000886\n",
      "Epoch 9/30: Train Loss: 0.2945, Test Loss: 0.2899, Train Acc: 0.9303, Test Acc: 0.9294, LR: 0.000886\n",
      "Epoch 10/30: Train Loss: 0.2865, Test Loss: 0.2824, Train Acc: 0.9313, Test Acc: 0.9302, LR: 0.000886\n",
      "Epoch 11/30: Train Loss: 0.2795, Test Loss: 0.2758, Train Acc: 0.9323, Test Acc: 0.9324, LR: 0.000886\n",
      "Epoch 12/30: Train Loss: 0.2734, Test Loss: 0.2699, Train Acc: 0.9322, Test Acc: 0.9323, LR: 0.000886\n",
      "Epoch 13/30: Train Loss: 0.2678, Test Loss: 0.2646, Train Acc: 0.9359, Test Acc: 0.9325, LR: 0.000886\n",
      "Epoch 14/30: Train Loss: 0.2629, Test Loss: 0.2598, Train Acc: 0.9361, Test Acc: 0.9356, LR: 0.000886\n",
      "Epoch 15/30: Train Loss: 0.2583, Test Loss: 0.2555, Train Acc: 0.9386, Test Acc: 0.9357, LR: 0.000620\n",
      "Epoch 16/30: Train Loss: 0.2548, Test Loss: 0.2527, Train Acc: 0.9396, Test Acc: 0.9398, LR: 0.000620\n",
      "Epoch 17/30: Train Loss: 0.2522, Test Loss: 0.2500, Train Acc: 0.9407, Test Acc: 0.9399, LR: 0.000620\n",
      "Epoch 18/30: Train Loss: 0.2495, Test Loss: 0.2475, Train Acc: 0.9411, Test Acc: 0.9405, LR: 0.000620\n",
      "Epoch 19/30: Train Loss: 0.2472, Test Loss: 0.2452, Train Acc: 0.9421, Test Acc: 0.9433, LR: 0.000620\n",
      "Epoch 20/30: Train Loss: 0.2448, Test Loss: 0.2429, Train Acc: 0.9443, Test Acc: 0.9433, LR: 0.000620\n",
      "Epoch 21/30: Train Loss: 0.2427, Test Loss: 0.2408, Train Acc: 0.9444, Test Acc: 0.9438, LR: 0.000620\n",
      "Epoch 22/30: Train Loss: 0.2406, Test Loss: 0.2388, Train Acc: 0.9449, Test Acc: 0.9438, LR: 0.000620\n",
      "Epoch 23/30: Train Loss: 0.2387, Test Loss: 0.2369, Train Acc: 0.9447, Test Acc: 0.9435, LR: 0.000620\n",
      "Epoch 24/30: Train Loss: 0.2369, Test Loss: 0.2351, Train Acc: 0.9456, Test Acc: 0.9460, LR: 0.000620\n",
      "Epoch 25/30: Train Loss: 0.2351, Test Loss: 0.2333, Train Acc: 0.9471, Test Acc: 0.9467, LR: 0.000620\n",
      "Epoch 26/30: Train Loss: 0.2336, Test Loss: 0.2317, Train Acc: 0.9474, Test Acc: 0.9467, LR: 0.000620\n",
      "Epoch 27/30: Train Loss: 0.2319, Test Loss: 0.2301, Train Acc: 0.9476, Test Acc: 0.9468, LR: 0.000620\n",
      "Epoch 28/30: Train Loss: 0.2303, Test Loss: 0.2286, Train Acc: 0.9476, Test Acc: 0.9468, LR: 0.000620\n",
      "Epoch 29/30: Train Loss: 0.2288, Test Loss: 0.2271, Train Acc: 0.9478, Test Acc: 0.9470, LR: 0.000620\n",
      "Epoch 30/30: Train Loss: 0.2274, Test Loss: 0.2257, Train Acc: 0.9483, Test Acc: 0.9490, LR: 0.000434\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13154  1148]\n",
      " [  472 17020]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.92      0.94     14302\n",
      "         1.0       0.94      0.97      0.95     17492\n",
      "\n",
      "    accuracy                           0.95     31794\n",
      "   macro avg       0.95      0.95      0.95     31794\n",
      "weighted avg       0.95      0.95      0.95     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.002154434690031882, 'scheduler_step_size': 10, 'scheduler_gamma': 0.7, 'patience': 7, 'optimizer': 'sgd', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.04832930238571752, 'batch_size': 64}\n",
      "Epoch 1/100: Train Loss: 0.2426, Test Loss: 0.1837, Train Acc: 0.9430, Test Acc: 0.9622, LR: 0.048329\n",
      "Epoch 2/100: Train Loss: 0.1729, Test Loss: 0.1625, Train Acc: 0.9637, Test Acc: 0.9666, LR: 0.048329\n",
      "Epoch 3/100: Train Loss: 0.1603, Test Loss: 0.1551, Train Acc: 0.9667, Test Acc: 0.9670, LR: 0.048329\n",
      "Epoch 4/100: Train Loss: 0.1555, Test Loss: 0.1517, Train Acc: 0.9682, Test Acc: 0.9697, LR: 0.048329\n",
      "Epoch 5/100: Train Loss: 0.1530, Test Loss: 0.1501, Train Acc: 0.9694, Test Acc: 0.9698, LR: 0.048329\n",
      "Epoch 6/100: Train Loss: 0.1520, Test Loss: 0.1492, Train Acc: 0.9695, Test Acc: 0.9697, LR: 0.048329\n",
      "Epoch 7/100: Train Loss: 0.1514, Test Loss: 0.1487, Train Acc: 0.9696, Test Acc: 0.9698, LR: 0.048329\n",
      "Epoch 8/100: Train Loss: 0.1509, Test Loss: 0.1487, Train Acc: 0.9696, Test Acc: 0.9703, LR: 0.048329\n",
      "Epoch 9/100: Train Loss: 0.1508, Test Loss: 0.1484, Train Acc: 0.9697, Test Acc: 0.9699, LR: 0.048329\n",
      "Epoch 10/100: Train Loss: 0.1507, Test Loss: 0.1484, Train Acc: 0.9696, Test Acc: 0.9699, LR: 0.033831\n",
      "Epoch 11/100: Train Loss: 0.1506, Test Loss: 0.1484, Train Acc: 0.9697, Test Acc: 0.9698, LR: 0.033831\n",
      "Epoch 12/100: Train Loss: 0.1507, Test Loss: 0.1483, Train Acc: 0.9697, Test Acc: 0.9699, LR: 0.033831\n",
      "Epoch 13/100: Train Loss: 0.1507, Test Loss: 0.1483, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.033831\n",
      "Epoch 14/100: Train Loss: 0.1506, Test Loss: 0.1484, Train Acc: 0.9696, Test Acc: 0.9700, LR: 0.033831\n",
      "Epoch 15/100: Train Loss: 0.1507, Test Loss: 0.1483, Train Acc: 0.9697, Test Acc: 0.9699, LR: 0.033831\n",
      "Epoch 16/100: Train Loss: 0.1506, Test Loss: 0.1484, Train Acc: 0.9696, Test Acc: 0.9700, LR: 0.033831\n",
      "Epoch 17/100: Train Loss: 0.1508, Test Loss: 0.1483, Train Acc: 0.9697, Test Acc: 0.9699, LR: 0.033831\n",
      "Epoch 18/100: Train Loss: 0.1506, Test Loss: 0.1483, Train Acc: 0.9696, Test Acc: 0.9701, LR: 0.033831\n",
      "Epoch 19/100: Train Loss: 0.1507, Test Loss: 0.1483, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.033831\n",
      "Early stopping triggered after 19 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13676   626]\n",
      " [  327 17165]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.96      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.0001, 'scheduler_step_size': 10, 'scheduler_gamma': 0.5, 'patience': 7, 'optimizer': 'adam', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.0008858667904100823, 'batch_size': 128}\n",
      "Epoch 1/30: Train Loss: 0.4902, Test Loss: 0.3726, Train Acc: 0.8056, Test Acc: 0.8924, LR: 0.000886\n",
      "Epoch 2/30: Train Loss: 0.3239, Test Loss: 0.2847, Train Acc: 0.9185, Test Acc: 0.9302, LR: 0.000886\n",
      "Epoch 3/30: Train Loss: 0.2594, Test Loss: 0.2362, Train Acc: 0.9404, Test Acc: 0.9458, LR: 0.000886\n",
      "Epoch 4/30: Train Loss: 0.2203, Test Loss: 0.2040, Train Acc: 0.9507, Test Acc: 0.9563, LR: 0.000886\n",
      "Epoch 5/30: Train Loss: 0.1933, Test Loss: 0.1809, Train Acc: 0.9589, Test Acc: 0.9619, LR: 0.000886\n",
      "Epoch 6/30: Train Loss: 0.1735, Test Loss: 0.1635, Train Acc: 0.9628, Test Acc: 0.9650, LR: 0.000886\n",
      "Epoch 7/30: Train Loss: 0.1587, Test Loss: 0.1505, Train Acc: 0.9659, Test Acc: 0.9673, LR: 0.000886\n",
      "Epoch 8/30: Train Loss: 0.1477, Test Loss: 0.1407, Train Acc: 0.9686, Test Acc: 0.9707, LR: 0.000886\n",
      "Epoch 9/30: Train Loss: 0.1393, Test Loss: 0.1332, Train Acc: 0.9704, Test Acc: 0.9732, LR: 0.000886\n",
      "Epoch 10/30: Train Loss: 0.1331, Test Loss: 0.1277, Train Acc: 0.9725, Test Acc: 0.9732, LR: 0.000443\n",
      "Epoch 11/30: Train Loss: 0.1294, Test Loss: 0.1254, Train Acc: 0.9726, Test Acc: 0.9734, LR: 0.000443\n",
      "Epoch 12/30: Train Loss: 0.1273, Test Loss: 0.1233, Train Acc: 0.9727, Test Acc: 0.9735, LR: 0.000443\n",
      "Epoch 13/30: Train Loss: 0.1255, Test Loss: 0.1216, Train Acc: 0.9730, Test Acc: 0.9738, LR: 0.000443\n",
      "Epoch 14/30: Train Loss: 0.1241, Test Loss: 0.1201, Train Acc: 0.9733, Test Acc: 0.9739, LR: 0.000443\n",
      "Epoch 15/30: Train Loss: 0.1228, Test Loss: 0.1189, Train Acc: 0.9738, Test Acc: 0.9744, LR: 0.000443\n",
      "Epoch 16/30: Train Loss: 0.1218, Test Loss: 0.1178, Train Acc: 0.9740, Test Acc: 0.9747, LR: 0.000443\n",
      "Epoch 17/30: Train Loss: 0.1209, Test Loss: 0.1170, Train Acc: 0.9743, Test Acc: 0.9752, LR: 0.000443\n",
      "Epoch 18/30: Train Loss: 0.1201, Test Loss: 0.1162, Train Acc: 0.9744, Test Acc: 0.9747, LR: 0.000443\n",
      "Epoch 19/30: Train Loss: 0.1196, Test Loss: 0.1156, Train Acc: 0.9743, Test Acc: 0.9752, LR: 0.000443\n",
      "Epoch 20/30: Train Loss: 0.1190, Test Loss: 0.1151, Train Acc: 0.9744, Test Acc: 0.9752, LR: 0.000221\n",
      "Epoch 21/30: Train Loss: 0.1187, Test Loss: 0.1149, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000221\n",
      "Epoch 22/30: Train Loss: 0.1185, Test Loss: 0.1147, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000221\n",
      "Epoch 23/30: Train Loss: 0.1184, Test Loss: 0.1145, Train Acc: 0.9746, Test Acc: 0.9752, LR: 0.000221\n",
      "Epoch 24/30: Train Loss: 0.1182, Test Loss: 0.1143, Train Acc: 0.9748, Test Acc: 0.9752, LR: 0.000221\n",
      "Epoch 25/30: Train Loss: 0.1180, Test Loss: 0.1142, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000221\n",
      "Epoch 26/30: Train Loss: 0.1180, Test Loss: 0.1140, Train Acc: 0.9749, Test Acc: 0.9755, LR: 0.000221\n",
      "Epoch 27/30: Train Loss: 0.1178, Test Loss: 0.1139, Train Acc: 0.9750, Test Acc: 0.9755, LR: 0.000221\n",
      "Epoch 28/30: Train Loss: 0.1176, Test Loss: 0.1137, Train Acc: 0.9750, Test Acc: 0.9755, LR: 0.000221\n",
      "Epoch 29/30: Train Loss: 0.1176, Test Loss: 0.1136, Train Acc: 0.9750, Test Acc: 0.9755, LR: 0.000221\n",
      "Epoch 30/30: Train Loss: 0.1174, Test Loss: 0.1135, Train Acc: 0.9750, Test Acc: 0.9755, LR: 0.000111\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13792   510]\n",
      " [  269 17223]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.98     31794\n",
      "   macro avg       0.98      0.97      0.98     31794\n",
      "weighted avg       0.98      0.98      0.98     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.0001, 'scheduler_step_size': 15, 'scheduler_gamma': 0.5, 'patience': 5, 'optimizer': 'sgd', 'num_epochs': 50, 'model': 'lr', 'learning_rate': 0.0001, 'batch_size': 128}\n",
      "Epoch 1/50: Train Loss: 0.7534, Test Loss: 0.7370, Train Acc: 0.4374, Test Acc: 0.4530, LR: 0.000100\n",
      "Epoch 2/50: Train Loss: 0.7221, Test Loss: 0.7074, Train Acc: 0.4644, Test Acc: 0.4846, LR: 0.000100\n",
      "Epoch 3/50: Train Loss: 0.6940, Test Loss: 0.6810, Train Acc: 0.5171, Test Acc: 0.5339, LR: 0.000100\n",
      "Epoch 4/50: Train Loss: 0.6688, Test Loss: 0.6573, Train Acc: 0.5564, Test Acc: 0.5984, LR: 0.000100\n",
      "Epoch 5/50: Train Loss: 0.6463, Test Loss: 0.6361, Train Acc: 0.6345, Test Acc: 0.6707, LR: 0.000100\n",
      "Epoch 6/50: Train Loss: 0.6261, Test Loss: 0.6170, Train Acc: 0.6905, Test Acc: 0.7190, LR: 0.000100\n",
      "Epoch 7/50: Train Loss: 0.6080, Test Loss: 0.5999, Train Acc: 0.7321, Test Acc: 0.7496, LR: 0.000100\n",
      "Epoch 8/50: Train Loss: 0.5917, Test Loss: 0.5844, Train Acc: 0.7505, Test Acc: 0.7567, LR: 0.000100\n",
      "Epoch 9/50: Train Loss: 0.5770, Test Loss: 0.5704, Train Acc: 0.7572, Test Acc: 0.7595, LR: 0.000100\n",
      "Epoch 10/50: Train Loss: 0.5636, Test Loss: 0.5577, Train Acc: 0.7630, Test Acc: 0.7664, LR: 0.000100\n",
      "Epoch 11/50: Train Loss: 0.5514, Test Loss: 0.5461, Train Acc: 0.7668, Test Acc: 0.7804, LR: 0.000100\n",
      "Epoch 12/50: Train Loss: 0.5403, Test Loss: 0.5355, Train Acc: 0.7843, Test Acc: 0.7967, LR: 0.000100\n",
      "Epoch 13/50: Train Loss: 0.5302, Test Loss: 0.5258, Train Acc: 0.8060, Test Acc: 0.8261, LR: 0.000100\n",
      "Epoch 14/50: Train Loss: 0.5209, Test Loss: 0.5168, Train Acc: 0.8264, Test Acc: 0.8392, LR: 0.000100\n",
      "Epoch 15/50: Train Loss: 0.5122, Test Loss: 0.5085, Train Acc: 0.8404, Test Acc: 0.8499, LR: 0.000050\n",
      "Epoch 16/50: Train Loss: 0.5062, Test Loss: 0.5046, Train Acc: 0.8472, Test Acc: 0.8508, LR: 0.000050\n",
      "Epoch 17/50: Train Loss: 0.5023, Test Loss: 0.5008, Train Acc: 0.8568, Test Acc: 0.8607, LR: 0.000050\n",
      "Epoch 18/50: Train Loss: 0.4986, Test Loss: 0.4972, Train Acc: 0.8607, Test Acc: 0.8606, LR: 0.000050\n",
      "Epoch 19/50: Train Loss: 0.4951, Test Loss: 0.4937, Train Acc: 0.8681, Test Acc: 0.8720, LR: 0.000050\n",
      "Epoch 20/50: Train Loss: 0.4916, Test Loss: 0.4903, Train Acc: 0.8698, Test Acc: 0.8703, LR: 0.000050\n",
      "Epoch 21/50: Train Loss: 0.4883, Test Loss: 0.4870, Train Acc: 0.8745, Test Acc: 0.8814, LR: 0.000050\n",
      "Epoch 22/50: Train Loss: 0.4851, Test Loss: 0.4838, Train Acc: 0.8798, Test Acc: 0.8833, LR: 0.000050\n",
      "Epoch 23/50: Train Loss: 0.4819, Test Loss: 0.4807, Train Acc: 0.8818, Test Acc: 0.8838, LR: 0.000050\n",
      "Epoch 24/50: Train Loss: 0.4789, Test Loss: 0.4778, Train Acc: 0.8874, Test Acc: 0.8886, LR: 0.000050\n",
      "Epoch 25/50: Train Loss: 0.4760, Test Loss: 0.4749, Train Acc: 0.8889, Test Acc: 0.8892, LR: 0.000050\n",
      "Epoch 26/50: Train Loss: 0.4732, Test Loss: 0.4721, Train Acc: 0.8883, Test Acc: 0.8878, LR: 0.000050\n",
      "Epoch 27/50: Train Loss: 0.4704, Test Loss: 0.4694, Train Acc: 0.8873, Test Acc: 0.8888, LR: 0.000050\n",
      "Epoch 28/50: Train Loss: 0.4678, Test Loss: 0.4668, Train Acc: 0.8893, Test Acc: 0.8874, LR: 0.000050\n",
      "Epoch 29/50: Train Loss: 0.4652, Test Loss: 0.4642, Train Acc: 0.8880, Test Acc: 0.8878, LR: 0.000050\n",
      "Epoch 30/50: Train Loss: 0.4627, Test Loss: 0.4617, Train Acc: 0.8884, Test Acc: 0.8871, LR: 0.000025\n",
      "Epoch 31/50: Train Loss: 0.4608, Test Loss: 0.4605, Train Acc: 0.8885, Test Acc: 0.8878, LR: 0.000025\n",
      "Epoch 32/50: Train Loss: 0.4596, Test Loss: 0.4593, Train Acc: 0.8886, Test Acc: 0.8879, LR: 0.000025\n",
      "Epoch 33/50: Train Loss: 0.4584, Test Loss: 0.4581, Train Acc: 0.8888, Test Acc: 0.8880, LR: 0.000025\n",
      "Epoch 34/50: Train Loss: 0.4573, Test Loss: 0.4569, Train Acc: 0.8889, Test Acc: 0.8882, LR: 0.000025\n",
      "Epoch 35/50: Train Loss: 0.4561, Test Loss: 0.4558, Train Acc: 0.8888, Test Acc: 0.8882, LR: 0.000025\n",
      "Epoch 36/50: Train Loss: 0.4549, Test Loss: 0.4546, Train Acc: 0.8895, Test Acc: 0.8888, LR: 0.000025\n",
      "Epoch 37/50: Train Loss: 0.4538, Test Loss: 0.4535, Train Acc: 0.8901, Test Acc: 0.8890, LR: 0.000025\n",
      "Epoch 38/50: Train Loss: 0.4527, Test Loss: 0.4524, Train Acc: 0.8895, Test Acc: 0.8866, LR: 0.000025\n",
      "Epoch 39/50: Train Loss: 0.4516, Test Loss: 0.4513, Train Acc: 0.8876, Test Acc: 0.8863, LR: 0.000025\n",
      "Epoch 40/50: Train Loss: 0.4505, Test Loss: 0.4502, Train Acc: 0.8875, Test Acc: 0.8866, LR: 0.000025\n",
      "Epoch 41/50: Train Loss: 0.4494, Test Loss: 0.4492, Train Acc: 0.8877, Test Acc: 0.8868, LR: 0.000025\n",
      "Epoch 42/50: Train Loss: 0.4484, Test Loss: 0.4481, Train Acc: 0.8878, Test Acc: 0.8873, LR: 0.000025\n",
      "Epoch 43/50: Train Loss: 0.4474, Test Loss: 0.4471, Train Acc: 0.8882, Test Acc: 0.8874, LR: 0.000025\n",
      "Epoch 44/50: Train Loss: 0.4463, Test Loss: 0.4460, Train Acc: 0.8889, Test Acc: 0.8880, LR: 0.000025\n",
      "Epoch 45/50: Train Loss: 0.4453, Test Loss: 0.4450, Train Acc: 0.8899, Test Acc: 0.8909, LR: 0.000013\n",
      "Epoch 46/50: Train Loss: 0.4446, Test Loss: 0.4445, Train Acc: 0.8921, Test Acc: 0.8909, LR: 0.000013\n",
      "Epoch 47/50: Train Loss: 0.4441, Test Loss: 0.4440, Train Acc: 0.8921, Test Acc: 0.8909, LR: 0.000013\n",
      "Epoch 48/50: Train Loss: 0.4436, Test Loss: 0.4435, Train Acc: 0.8922, Test Acc: 0.8909, LR: 0.000013\n",
      "Epoch 49/50: Train Loss: 0.4431, Test Loss: 0.4430, Train Acc: 0.8922, Test Acc: 0.8911, LR: 0.000013\n",
      "Epoch 50/50: Train Loss: 0.4426, Test Loss: 0.4425, Train Acc: 0.8922, Test Acc: 0.8903, LR: 0.000013\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11963  2339]\n",
      " [ 1148 16344]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.84      0.87     14302\n",
      "         1.0       0.87      0.93      0.90     17492\n",
      "\n",
      "    accuracy                           0.89     31794\n",
      "   macro avg       0.89      0.89      0.89     31794\n",
      "weighted avg       0.89      0.89      0.89     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.002154434690031882, 'scheduler_step_size': 10, 'scheduler_gamma': 0.7, 'patience': 3, 'optimizer': 'adam', 'num_epochs': 100, 'model': 'lr', 'learning_rate': 0.0001, 'batch_size': 32}\n",
      "Epoch 1/100: Train Loss: 0.5842, Test Loss: 0.4704, Train Acc: 0.7159, Test Acc: 0.8710, LR: 0.000100\n",
      "Epoch 2/100: Train Loss: 0.4134, Test Loss: 0.3673, Train Acc: 0.8847, Test Acc: 0.8987, LR: 0.000100\n",
      "Epoch 3/100: Train Loss: 0.3348, Test Loss: 0.3066, Train Acc: 0.9172, Test Acc: 0.9275, LR: 0.000100\n",
      "Epoch 4/100: Train Loss: 0.2856, Test Loss: 0.2665, Train Acc: 0.9336, Test Acc: 0.9352, LR: 0.000100\n",
      "Epoch 5/100: Train Loss: 0.2523, Test Loss: 0.2386, Train Acc: 0.9422, Test Acc: 0.9432, LR: 0.000100\n",
      "Epoch 6/100: Train Loss: 0.2286, Test Loss: 0.2183, Train Acc: 0.9486, Test Acc: 0.9498, LR: 0.000100\n",
      "Epoch 7/100: Train Loss: 0.2113, Test Loss: 0.2033, Train Acc: 0.9528, Test Acc: 0.9538, LR: 0.000100\n",
      "Epoch 8/100: Train Loss: 0.1985, Test Loss: 0.1921, Train Acc: 0.9557, Test Acc: 0.9558, LR: 0.000100\n",
      "Epoch 9/100: Train Loss: 0.1888, Test Loss: 0.1835, Train Acc: 0.9597, Test Acc: 0.9623, LR: 0.000100\n",
      "Epoch 10/100: Train Loss: 0.1815, Test Loss: 0.1770, Train Acc: 0.9620, Test Acc: 0.9636, LR: 0.000070\n",
      "Epoch 11/100: Train Loss: 0.1766, Test Loss: 0.1734, Train Acc: 0.9629, Test Acc: 0.9642, LR: 0.000070\n",
      "Epoch 12/100: Train Loss: 0.1733, Test Loss: 0.1703, Train Acc: 0.9634, Test Acc: 0.9644, LR: 0.000070\n",
      "Epoch 13/100: Train Loss: 0.1706, Test Loss: 0.1677, Train Acc: 0.9635, Test Acc: 0.9646, LR: 0.000070\n",
      "Epoch 14/100: Train Loss: 0.1682, Test Loss: 0.1654, Train Acc: 0.9651, Test Acc: 0.9657, LR: 0.000070\n",
      "Epoch 15/100: Train Loss: 0.1663, Test Loss: 0.1636, Train Acc: 0.9652, Test Acc: 0.9657, LR: 0.000070\n",
      "Epoch 16/100: Train Loss: 0.1645, Test Loss: 0.1619, Train Acc: 0.9653, Test Acc: 0.9657, LR: 0.000070\n",
      "Epoch 17/100: Train Loss: 0.1631, Test Loss: 0.1605, Train Acc: 0.9654, Test Acc: 0.9660, LR: 0.000070\n",
      "Epoch 18/100: Train Loss: 0.1617, Test Loss: 0.1592, Train Acc: 0.9655, Test Acc: 0.9660, LR: 0.000070\n",
      "Epoch 19/100: Train Loss: 0.1606, Test Loss: 0.1581, Train Acc: 0.9663, Test Acc: 0.9671, LR: 0.000070\n",
      "Epoch 20/100: Train Loss: 0.1595, Test Loss: 0.1571, Train Acc: 0.9671, Test Acc: 0.9671, LR: 0.000049\n",
      "Epoch 21/100: Train Loss: 0.1588, Test Loss: 0.1565, Train Acc: 0.9671, Test Acc: 0.9671, LR: 0.000049\n",
      "Epoch 22/100: Train Loss: 0.1582, Test Loss: 0.1560, Train Acc: 0.9671, Test Acc: 0.9671, LR: 0.000049\n",
      "Epoch 23/100: Train Loss: 0.1577, Test Loss: 0.1555, Train Acc: 0.9672, Test Acc: 0.9671, LR: 0.000049\n",
      "Epoch 24/100: Train Loss: 0.1573, Test Loss: 0.1550, Train Acc: 0.9672, Test Acc: 0.9671, LR: 0.000049\n",
      "Epoch 25/100: Train Loss: 0.1568, Test Loss: 0.1545, Train Acc: 0.9672, Test Acc: 0.9672, LR: 0.000049\n",
      "Epoch 26/100: Train Loss: 0.1566, Test Loss: 0.1541, Train Acc: 0.9672, Test Acc: 0.9671, LR: 0.000049\n",
      "Epoch 27/100: Train Loss: 0.1560, Test Loss: 0.1538, Train Acc: 0.9672, Test Acc: 0.9671, LR: 0.000049\n",
      "Epoch 28/100: Train Loss: 0.1557, Test Loss: 0.1534, Train Acc: 0.9672, Test Acc: 0.9673, LR: 0.000049\n",
      "Epoch 29/100: Train Loss: 0.1554, Test Loss: 0.1531, Train Acc: 0.9673, Test Acc: 0.9673, LR: 0.000049\n",
      "Epoch 30/100: Train Loss: 0.1550, Test Loss: 0.1528, Train Acc: 0.9674, Test Acc: 0.9677, LR: 0.000034\n",
      "Epoch 31/100: Train Loss: 0.1548, Test Loss: 0.1526, Train Acc: 0.9677, Test Acc: 0.9677, LR: 0.000034\n",
      "Epoch 32/100: Train Loss: 0.1546, Test Loss: 0.1524, Train Acc: 0.9678, Test Acc: 0.9677, LR: 0.000034\n",
      "Epoch 33/100: Train Loss: 0.1544, Test Loss: 0.1522, Train Acc: 0.9678, Test Acc: 0.9677, LR: 0.000034\n",
      "Epoch 34/100: Train Loss: 0.1543, Test Loss: 0.1521, Train Acc: 0.9678, Test Acc: 0.9677, LR: 0.000034\n",
      "Epoch 35/100: Train Loss: 0.1541, Test Loss: 0.1519, Train Acc: 0.9678, Test Acc: 0.9678, LR: 0.000034\n",
      "Epoch 36/100: Train Loss: 0.1540, Test Loss: 0.1517, Train Acc: 0.9678, Test Acc: 0.9678, LR: 0.000034\n",
      "Epoch 37/100: Train Loss: 0.1538, Test Loss: 0.1516, Train Acc: 0.9678, Test Acc: 0.9678, LR: 0.000034\n",
      "Epoch 38/100: Train Loss: 0.1537, Test Loss: 0.1515, Train Acc: 0.9678, Test Acc: 0.9678, LR: 0.000034\n",
      "Epoch 39/100: Train Loss: 0.1535, Test Loss: 0.1513, Train Acc: 0.9678, Test Acc: 0.9678, LR: 0.000034\n",
      "Epoch 40/100: Train Loss: 0.1534, Test Loss: 0.1512, Train Acc: 0.9682, Test Acc: 0.9700, LR: 0.000024\n",
      "Epoch 41/100: Train Loss: 0.1533, Test Loss: 0.1511, Train Acc: 0.9698, Test Acc: 0.9700, LR: 0.000024\n",
      "Epoch 42/100: Train Loss: 0.1532, Test Loss: 0.1510, Train Acc: 0.9698, Test Acc: 0.9700, LR: 0.000024\n",
      "Epoch 43/100: Train Loss: 0.1532, Test Loss: 0.1509, Train Acc: 0.9698, Test Acc: 0.9700, LR: 0.000024\n",
      "Epoch 44/100: Train Loss: 0.1531, Test Loss: 0.1509, Train Acc: 0.9698, Test Acc: 0.9700, LR: 0.000024\n",
      "Epoch 45/100: Train Loss: 0.1530, Test Loss: 0.1508, Train Acc: 0.9698, Test Acc: 0.9700, LR: 0.000024\n",
      "Epoch 46/100: Train Loss: 0.1529, Test Loss: 0.1507, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.000024\n",
      "Epoch 47/100: Train Loss: 0.1529, Test Loss: 0.1507, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.000024\n",
      "Epoch 48/100: Train Loss: 0.1528, Test Loss: 0.1506, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.000024\n",
      "Epoch 49/100: Train Loss: 0.1527, Test Loss: 0.1505, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.000024\n",
      "Epoch 50/100: Train Loss: 0.1527, Test Loss: 0.1505, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.000017\n",
      "Epoch 51/100: Train Loss: 0.1526, Test Loss: 0.1504, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.000017\n",
      "Epoch 52/100: Train Loss: 0.1526, Test Loss: 0.1504, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.000017\n",
      "Epoch 53/100: Train Loss: 0.1526, Test Loss: 0.1503, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.000017\n",
      "Epoch 54/100: Train Loss: 0.1525, Test Loss: 0.1503, Train Acc: 0.9698, Test Acc: 0.9701, LR: 0.000017\n",
      "Epoch 55/100: Train Loss: 0.1524, Test Loss: 0.1503, Train Acc: 0.9697, Test Acc: 0.9700, LR: 0.000017\n",
      "Epoch 56/100: Train Loss: 0.1524, Test Loss: 0.1502, Train Acc: 0.9698, Test Acc: 0.9700, LR: 0.000017\n",
      "Epoch 57/100: Train Loss: 0.1524, Test Loss: 0.1502, Train Acc: 0.9698, Test Acc: 0.9700, LR: 0.000017\n",
      "Epoch 58/100: Train Loss: 0.1523, Test Loss: 0.1501, Train Acc: 0.9698, Test Acc: 0.9700, LR: 0.000017\n",
      "Epoch 59/100: Train Loss: 0.1523, Test Loss: 0.1501, Train Acc: 0.9696, Test Acc: 0.9700, LR: 0.000017\n",
      "Epoch 60/100: Train Loss: 0.1523, Test Loss: 0.1501, Train Acc: 0.9698, Test Acc: 0.9700, LR: 0.000012\n",
      "Epoch 61/100: Train Loss: 0.1522, Test Loss: 0.1500, Train Acc: 0.9698, Test Acc: 0.9700, LR: 0.000012\n",
      "Epoch 62/100: Train Loss: 0.1523, Test Loss: 0.1500, Train Acc: 0.9698, Test Acc: 0.9700, LR: 0.000012\n",
      "Early stopping triggered after 62 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13676   626]\n",
      " [  327 17165]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.96      0.98      0.97     17492\n",
      "\n",
      "    accuracy                           0.97     31794\n",
      "   macro avg       0.97      0.97      0.97     31794\n",
      "weighted avg       0.97      0.97      0.97     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.00046415888336127773, 'scheduler_step_size': 10, 'scheduler_gamma': 0.5, 'patience': 7, 'optimizer': 'rmsprop', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.06951927961775606, 'batch_size': 64}\n",
      "Epoch 1/30: Train Loss: 0.1296, Test Loss: 0.1194, Train Acc: 0.9719, Test Acc: 0.9736, LR: 0.069519\n",
      "Epoch 2/30: Train Loss: 0.1247, Test Loss: 0.1251, Train Acc: 0.9737, Test Acc: 0.9742, LR: 0.069519\n",
      "Epoch 3/30: Train Loss: 0.1246, Test Loss: 0.1252, Train Acc: 0.9736, Test Acc: 0.9732, LR: 0.069519\n",
      "Epoch 4/30: Train Loss: 0.1245, Test Loss: 0.1415, Train Acc: 0.9736, Test Acc: 0.9577, LR: 0.069519\n",
      "Epoch 5/30: Train Loss: 0.1247, Test Loss: 0.1207, Train Acc: 0.9739, Test Acc: 0.9737, LR: 0.069519\n",
      "Epoch 6/30: Train Loss: 0.1247, Test Loss: 0.1253, Train Acc: 0.9736, Test Acc: 0.9750, LR: 0.069519\n",
      "Epoch 7/30: Train Loss: 0.1247, Test Loss: 0.1193, Train Acc: 0.9738, Test Acc: 0.9743, LR: 0.069519\n",
      "Epoch 8/30: Train Loss: 0.1244, Test Loss: 0.1408, Train Acc: 0.9737, Test Acc: 0.9639, LR: 0.069519\n",
      "Early stopping triggered after 8 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13379   923]\n",
      " [  225 17267]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.94      0.96     14302\n",
      "         1.0       0.95      0.99      0.97     17492\n",
      "\n",
      "    accuracy                           0.96     31794\n",
      "   macro avg       0.97      0.96      0.96     31794\n",
      "weighted avg       0.96      0.96      0.96     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 4.641588833612782e-05, 'scheduler_step_size': 15, 'scheduler_gamma': 0.7, 'patience': 5, 'optimizer': 'rmsprop', 'num_epochs': 75, 'model': 'lr', 'learning_rate': 0.06951927961775606, 'batch_size': 128}\n",
      "Epoch 1/75: Train Loss: 0.1261, Test Loss: 0.1122, Train Acc: 0.9722, Test Acc: 0.9753, LR: 0.069519\n",
      "Epoch 2/75: Train Loss: 0.1161, Test Loss: 0.1118, Train Acc: 0.9754, Test Acc: 0.9763, LR: 0.069519\n",
      "Epoch 3/75: Train Loss: 0.1161, Test Loss: 0.1105, Train Acc: 0.9755, Test Acc: 0.9754, LR: 0.069519\n",
      "Epoch 4/75: Train Loss: 0.1159, Test Loss: 0.1120, Train Acc: 0.9755, Test Acc: 0.9763, LR: 0.069519\n",
      "Epoch 5/75: Train Loss: 0.1162, Test Loss: 0.1123, Train Acc: 0.9756, Test Acc: 0.9751, LR: 0.069519\n",
      "Epoch 6/75: Train Loss: 0.1162, Test Loss: 0.1103, Train Acc: 0.9755, Test Acc: 0.9764, LR: 0.069519\n",
      "Epoch 7/75: Train Loss: 0.1161, Test Loss: 0.1101, Train Acc: 0.9755, Test Acc: 0.9751, LR: 0.069519\n",
      "Epoch 8/75: Train Loss: 0.1161, Test Loss: 0.1136, Train Acc: 0.9755, Test Acc: 0.9754, LR: 0.069519\n",
      "Epoch 9/75: Train Loss: 0.1163, Test Loss: 0.1105, Train Acc: 0.9755, Test Acc: 0.9756, LR: 0.069519\n",
      "Epoch 10/75: Train Loss: 0.1162, Test Loss: 0.1108, Train Acc: 0.9754, Test Acc: 0.9760, LR: 0.069519\n",
      "Epoch 11/75: Train Loss: 0.1162, Test Loss: 0.1139, Train Acc: 0.9754, Test Acc: 0.9758, LR: 0.069519\n",
      "Epoch 12/75: Train Loss: 0.1161, Test Loss: 0.1110, Train Acc: 0.9754, Test Acc: 0.9759, LR: 0.069519\n",
      "Early stopping triggered after 12 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13823   479]\n",
      " [  288 17204]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.97      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.98     31794\n",
      "   macro avg       0.98      0.98      0.98     31794\n",
      "weighted avg       0.98      0.98      0.98     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.00046415888336127773, 'scheduler_step_size': 15, 'scheduler_gamma': 0.5, 'patience': 5, 'optimizer': 'sgd', 'num_epochs': 50, 'model': 'lr', 'learning_rate': 0.0001, 'batch_size': 64}\n",
      "Epoch 1/50: Train Loss: 0.7377, Test Loss: 0.7075, Train Acc: 0.4511, Test Acc: 0.4846, LR: 0.000100\n",
      "Epoch 2/50: Train Loss: 0.6814, Test Loss: 0.6573, Train Acc: 0.5370, Test Acc: 0.5986, LR: 0.000100\n",
      "Epoch 3/50: Train Loss: 0.6362, Test Loss: 0.6170, Train Acc: 0.6623, Test Acc: 0.7191, LR: 0.000100\n",
      "Epoch 4/50: Train Loss: 0.5999, Test Loss: 0.5844, Train Acc: 0.7411, Test Acc: 0.7567, LR: 0.000100\n",
      "Epoch 5/50: Train Loss: 0.5703, Test Loss: 0.5577, Train Acc: 0.7601, Test Acc: 0.7662, LR: 0.000100\n",
      "Epoch 6/50: Train Loss: 0.5459, Test Loss: 0.5355, Train Acc: 0.7754, Test Acc: 0.7971, LR: 0.000100\n",
      "Epoch 7/50: Train Loss: 0.5255, Test Loss: 0.5168, Train Acc: 0.8165, Test Acc: 0.8392, LR: 0.000100\n",
      "Epoch 8/50: Train Loss: 0.5083, Test Loss: 0.5009, Train Acc: 0.8464, Test Acc: 0.8607, LR: 0.000100\n",
      "Epoch 9/50: Train Loss: 0.4934, Test Loss: 0.4870, Train Acc: 0.8685, Test Acc: 0.8818, LR: 0.000100\n",
      "Epoch 10/50: Train Loss: 0.4805, Test Loss: 0.4750, Train Acc: 0.8842, Test Acc: 0.8892, LR: 0.000100\n",
      "Epoch 11/50: Train Loss: 0.4692, Test Loss: 0.4643, Train Acc: 0.8884, Test Acc: 0.8878, LR: 0.000100\n",
      "Epoch 12/50: Train Loss: 0.4591, Test Loss: 0.4547, Train Acc: 0.8889, Test Acc: 0.8888, LR: 0.000100\n",
      "Epoch 13/50: Train Loss: 0.4501, Test Loss: 0.4461, Train Acc: 0.8883, Test Acc: 0.8880, LR: 0.000100\n",
      "Epoch 14/50: Train Loss: 0.4420, Test Loss: 0.4384, Train Acc: 0.8917, Test Acc: 0.8910, LR: 0.000100\n",
      "Epoch 15/50: Train Loss: 0.4345, Test Loss: 0.4312, Train Acc: 0.8930, Test Acc: 0.8920, LR: 0.000050\n",
      "Epoch 16/50: Train Loss: 0.4293, Test Loss: 0.4279, Train Acc: 0.8934, Test Acc: 0.8931, LR: 0.000050\n",
      "Epoch 17/50: Train Loss: 0.4262, Test Loss: 0.4247, Train Acc: 0.8943, Test Acc: 0.8937, LR: 0.000050\n",
      "Epoch 18/50: Train Loss: 0.4229, Test Loss: 0.4216, Train Acc: 0.8972, Test Acc: 0.8965, LR: 0.000050\n",
      "Epoch 19/50: Train Loss: 0.4200, Test Loss: 0.4187, Train Acc: 0.8977, Test Acc: 0.8957, LR: 0.000050\n",
      "Epoch 20/50: Train Loss: 0.4171, Test Loss: 0.4158, Train Acc: 0.8978, Test Acc: 0.8964, LR: 0.000050\n",
      "Epoch 21/50: Train Loss: 0.4143, Test Loss: 0.4131, Train Acc: 0.8973, Test Acc: 0.8953, LR: 0.000050\n",
      "Epoch 22/50: Train Loss: 0.4115, Test Loss: 0.4104, Train Acc: 0.8972, Test Acc: 0.8953, LR: 0.000050\n",
      "Epoch 23/50: Train Loss: 0.4089, Test Loss: 0.4078, Train Acc: 0.8974, Test Acc: 0.8960, LR: 0.000050\n",
      "Epoch 24/50: Train Loss: 0.4064, Test Loss: 0.4053, Train Acc: 0.8983, Test Acc: 0.9003, LR: 0.000050\n",
      "Epoch 25/50: Train Loss: 0.4040, Test Loss: 0.4029, Train Acc: 0.9010, Test Acc: 0.8988, LR: 0.000050\n",
      "Epoch 26/50: Train Loss: 0.4016, Test Loss: 0.4006, Train Acc: 0.9001, Test Acc: 0.8988, LR: 0.000050\n",
      "Epoch 27/50: Train Loss: 0.3994, Test Loss: 0.3983, Train Acc: 0.9003, Test Acc: 0.8992, LR: 0.000050\n",
      "Epoch 28/50: Train Loss: 0.3971, Test Loss: 0.3961, Train Acc: 0.9006, Test Acc: 0.8992, LR: 0.000050\n",
      "Epoch 29/50: Train Loss: 0.3949, Test Loss: 0.3939, Train Acc: 0.9006, Test Acc: 0.8992, LR: 0.000050\n",
      "Epoch 30/50: Train Loss: 0.3928, Test Loss: 0.3919, Train Acc: 0.9007, Test Acc: 0.8994, LR: 0.000025\n",
      "Epoch 31/50: Train Loss: 0.3913, Test Loss: 0.3908, Train Acc: 0.9007, Test Acc: 0.8994, LR: 0.000025\n",
      "Epoch 32/50: Train Loss: 0.3902, Test Loss: 0.3898, Train Acc: 0.9007, Test Acc: 0.8995, LR: 0.000025\n",
      "Epoch 33/50: Train Loss: 0.3892, Test Loss: 0.3888, Train Acc: 0.9011, Test Acc: 0.9002, LR: 0.000025\n",
      "Epoch 34/50: Train Loss: 0.3883, Test Loss: 0.3878, Train Acc: 0.9023, Test Acc: 0.9004, LR: 0.000025\n",
      "Epoch 35/50: Train Loss: 0.3873, Test Loss: 0.3869, Train Acc: 0.9016, Test Acc: 0.9004, LR: 0.000025\n",
      "Epoch 36/50: Train Loss: 0.3864, Test Loss: 0.3859, Train Acc: 0.9017, Test Acc: 0.9004, LR: 0.000025\n",
      "Epoch 37/50: Train Loss: 0.3854, Test Loss: 0.3850, Train Acc: 0.9014, Test Acc: 0.8999, LR: 0.000025\n",
      "Epoch 38/50: Train Loss: 0.3844, Test Loss: 0.3840, Train Acc: 0.9013, Test Acc: 0.9000, LR: 0.000025\n",
      "Epoch 39/50: Train Loss: 0.3835, Test Loss: 0.3831, Train Acc: 0.9013, Test Acc: 0.9001, LR: 0.000025\n",
      "Epoch 40/50: Train Loss: 0.3827, Test Loss: 0.3822, Train Acc: 0.9015, Test Acc: 0.9016, LR: 0.000025\n",
      "Epoch 41/50: Train Loss: 0.3817, Test Loss: 0.3813, Train Acc: 0.9023, Test Acc: 0.9013, LR: 0.000025\n",
      "Epoch 42/50: Train Loss: 0.3809, Test Loss: 0.3804, Train Acc: 0.9024, Test Acc: 0.9013, LR: 0.000025\n",
      "Epoch 43/50: Train Loss: 0.3799, Test Loss: 0.3795, Train Acc: 0.9026, Test Acc: 0.9019, LR: 0.000025\n",
      "Epoch 44/50: Train Loss: 0.3791, Test Loss: 0.3787, Train Acc: 0.9029, Test Acc: 0.9019, LR: 0.000025\n",
      "Epoch 45/50: Train Loss: 0.3782, Test Loss: 0.3778, Train Acc: 0.9029, Test Acc: 0.9020, LR: 0.000013\n",
      "Epoch 46/50: Train Loss: 0.3776, Test Loss: 0.3774, Train Acc: 0.9029, Test Acc: 0.9020, LR: 0.000013\n",
      "Epoch 47/50: Train Loss: 0.3772, Test Loss: 0.3769, Train Acc: 0.9030, Test Acc: 0.9020, LR: 0.000013\n",
      "Epoch 48/50: Train Loss: 0.3768, Test Loss: 0.3765, Train Acc: 0.9030, Test Acc: 0.9020, LR: 0.000013\n",
      "Epoch 49/50: Train Loss: 0.3763, Test Loss: 0.3761, Train Acc: 0.9030, Test Acc: 0.9020, LR: 0.000013\n",
      "Epoch 50/50: Train Loss: 0.3759, Test Loss: 0.3757, Train Acc: 0.9031, Test Acc: 0.9020, LR: 0.000013\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12047  2255]\n",
      " [  860 16632]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.84      0.89     14302\n",
      "         1.0       0.88      0.95      0.91     17492\n",
      "\n",
      "    accuracy                           0.90     31794\n",
      "   macro avg       0.91      0.90      0.90     31794\n",
      "weighted avg       0.90      0.90      0.90     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 2.1544346900318823e-05, 'scheduler_step_size': 15, 'scheduler_gamma': 0.5, 'patience': 5, 'optimizer': 'adam', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.0001, 'batch_size': 128}\n",
      "Epoch 1/30: Train Loss: 0.7016, Test Loss: 0.6424, Train Acc: 0.5166, Test Acc: 0.7004, LR: 0.000100\n",
      "Epoch 2/30: Train Loss: 0.5979, Test Loss: 0.5597, Train Acc: 0.7175, Test Acc: 0.7306, LR: 0.000100\n",
      "Epoch 3/30: Train Loss: 0.5299, Test Loss: 0.5041, Train Acc: 0.7757, Test Acc: 0.8318, LR: 0.000100\n",
      "Epoch 4/30: Train Loss: 0.4826, Test Loss: 0.4634, Train Acc: 0.8554, Test Acc: 0.8673, LR: 0.000100\n",
      "Epoch 5/30: Train Loss: 0.4463, Test Loss: 0.4305, Train Acc: 0.8718, Test Acc: 0.8745, LR: 0.000100\n",
      "Epoch 6/30: Train Loss: 0.4162, Test Loss: 0.4026, Train Acc: 0.8803, Test Acc: 0.8822, LR: 0.000100\n",
      "Epoch 7/30: Train Loss: 0.3902, Test Loss: 0.3782, Train Acc: 0.8881, Test Acc: 0.8915, LR: 0.000100\n",
      "Epoch 8/30: Train Loss: 0.3675, Test Loss: 0.3567, Train Acc: 0.9002, Test Acc: 0.9038, LR: 0.000100\n",
      "Epoch 9/30: Train Loss: 0.3474, Test Loss: 0.3379, Train Acc: 0.9108, Test Acc: 0.9150, LR: 0.000100\n",
      "Epoch 10/30: Train Loss: 0.3298, Test Loss: 0.3212, Train Acc: 0.9183, Test Acc: 0.9190, LR: 0.000100\n",
      "Epoch 11/30: Train Loss: 0.3141, Test Loss: 0.3064, Train Acc: 0.9225, Test Acc: 0.9222, LR: 0.000100\n",
      "Epoch 12/30: Train Loss: 0.3001, Test Loss: 0.2931, Train Acc: 0.9251, Test Acc: 0.9243, LR: 0.000100\n",
      "Epoch 13/30: Train Loss: 0.2876, Test Loss: 0.2811, Train Acc: 0.9269, Test Acc: 0.9299, LR: 0.000100\n",
      "Epoch 14/30: Train Loss: 0.2763, Test Loss: 0.2702, Train Acc: 0.9325, Test Acc: 0.9344, LR: 0.000100\n",
      "Epoch 15/30: Train Loss: 0.2660, Test Loss: 0.2603, Train Acc: 0.9372, Test Acc: 0.9394, LR: 0.000050\n",
      "Epoch 16/30: Train Loss: 0.2588, Test Loss: 0.2557, Train Acc: 0.9389, Test Acc: 0.9364, LR: 0.000050\n",
      "Epoch 17/30: Train Loss: 0.2542, Test Loss: 0.2511, Train Acc: 0.9387, Test Acc: 0.9396, LR: 0.000050\n",
      "Epoch 18/30: Train Loss: 0.2498, Test Loss: 0.2467, Train Acc: 0.9406, Test Acc: 0.9399, LR: 0.000050\n",
      "Epoch 19/30: Train Loss: 0.2455, Test Loss: 0.2425, Train Acc: 0.9416, Test Acc: 0.9415, LR: 0.000050\n",
      "Epoch 20/30: Train Loss: 0.2413, Test Loss: 0.2384, Train Acc: 0.9443, Test Acc: 0.9439, LR: 0.000050\n",
      "Epoch 21/30: Train Loss: 0.2374, Test Loss: 0.2345, Train Acc: 0.9450, Test Acc: 0.9448, LR: 0.000050\n",
      "Epoch 22/30: Train Loss: 0.2336, Test Loss: 0.2308, Train Acc: 0.9454, Test Acc: 0.9450, LR: 0.000050\n",
      "Epoch 23/30: Train Loss: 0.2300, Test Loss: 0.2272, Train Acc: 0.9471, Test Acc: 0.9473, LR: 0.000050\n",
      "Epoch 24/30: Train Loss: 0.2264, Test Loss: 0.2237, Train Acc: 0.9480, Test Acc: 0.9482, LR: 0.000050\n",
      "Epoch 25/30: Train Loss: 0.2231, Test Loss: 0.2204, Train Acc: 0.9487, Test Acc: 0.9486, LR: 0.000050\n",
      "Epoch 26/30: Train Loss: 0.2199, Test Loss: 0.2172, Train Acc: 0.9490, Test Acc: 0.9487, LR: 0.000050\n",
      "Epoch 27/30: Train Loss: 0.2167, Test Loss: 0.2142, Train Acc: 0.9507, Test Acc: 0.9505, LR: 0.000050\n",
      "Epoch 28/30: Train Loss: 0.2137, Test Loss: 0.2112, Train Acc: 0.9511, Test Acc: 0.9507, LR: 0.000050\n",
      "Epoch 29/30: Train Loss: 0.2108, Test Loss: 0.2083, Train Acc: 0.9516, Test Acc: 0.9512, LR: 0.000050\n",
      "Epoch 30/30: Train Loss: 0.2081, Test Loss: 0.2056, Train Acc: 0.9525, Test Acc: 0.9525, LR: 0.000025\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13238  1064]\n",
      " [  446 17046]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.93      0.95     14302\n",
      "         1.0       0.94      0.97      0.96     17492\n",
      "\n",
      "    accuracy                           0.95     31794\n",
      "   macro avg       0.95      0.95      0.95     31794\n",
      "weighted avg       0.95      0.95      0.95     31794\n",
      "\n",
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': 0.00021544346900318823, 'scheduler_step_size': 15, 'scheduler_gamma': 0.7, 'patience': 5, 'optimizer': 'adam', 'num_epochs': 30, 'model': 'lr', 'learning_rate': 0.0008858667904100823, 'batch_size': 64}\n",
      "Epoch 1/30: Train Loss: 0.4095, Test Loss: 0.2882, Train Acc: 0.8618, Test Acc: 0.9284, LR: 0.000886\n",
      "Epoch 2/30: Train Loss: 0.2434, Test Loss: 0.2079, Train Acc: 0.9445, Test Acc: 0.9567, LR: 0.000886\n",
      "Epoch 3/30: Train Loss: 0.1875, Test Loss: 0.1683, Train Acc: 0.9602, Test Acc: 0.9639, LR: 0.000886\n",
      "Epoch 4/30: Train Loss: 0.1579, Test Loss: 0.1457, Train Acc: 0.9661, Test Acc: 0.9681, LR: 0.000886\n",
      "Epoch 5/30: Train Loss: 0.1410, Test Loss: 0.1328, Train Acc: 0.9698, Test Acc: 0.9708, LR: 0.000886\n",
      "Epoch 6/30: Train Loss: 0.1315, Test Loss: 0.1253, Train Acc: 0.9724, Test Acc: 0.9734, LR: 0.000886\n",
      "Epoch 7/30: Train Loss: 0.1260, Test Loss: 0.1211, Train Acc: 0.9730, Test Acc: 0.9737, LR: 0.000886\n",
      "Epoch 8/30: Train Loss: 0.1230, Test Loss: 0.1187, Train Acc: 0.9737, Test Acc: 0.9750, LR: 0.000886\n",
      "Epoch 9/30: Train Loss: 0.1213, Test Loss: 0.1173, Train Acc: 0.9742, Test Acc: 0.9745, LR: 0.000886\n",
      "Epoch 10/30: Train Loss: 0.1203, Test Loss: 0.1164, Train Acc: 0.9742, Test Acc: 0.9750, LR: 0.000886\n",
      "Epoch 11/30: Train Loss: 0.1196, Test Loss: 0.1159, Train Acc: 0.9743, Test Acc: 0.9748, LR: 0.000886\n",
      "Epoch 12/30: Train Loss: 0.1192, Test Loss: 0.1154, Train Acc: 0.9744, Test Acc: 0.9753, LR: 0.000886\n",
      "Epoch 13/30: Train Loss: 0.1190, Test Loss: 0.1151, Train Acc: 0.9745, Test Acc: 0.9754, LR: 0.000886\n",
      "Epoch 14/30: Train Loss: 0.1187, Test Loss: 0.1149, Train Acc: 0.9745, Test Acc: 0.9749, LR: 0.000886\n",
      "Epoch 15/30: Train Loss: 0.1186, Test Loss: 0.1148, Train Acc: 0.9747, Test Acc: 0.9752, LR: 0.000620\n",
      "Epoch 16/30: Train Loss: 0.1185, Test Loss: 0.1147, Train Acc: 0.9748, Test Acc: 0.9754, LR: 0.000620\n",
      "Epoch 17/30: Train Loss: 0.1185, Test Loss: 0.1146, Train Acc: 0.9748, Test Acc: 0.9753, LR: 0.000620\n",
      "Epoch 18/30: Train Loss: 0.1183, Test Loss: 0.1145, Train Acc: 0.9748, Test Acc: 0.9753, LR: 0.000620\n",
      "Epoch 19/30: Train Loss: 0.1184, Test Loss: 0.1145, Train Acc: 0.9748, Test Acc: 0.9752, LR: 0.000620\n",
      "Epoch 20/30: Train Loss: 0.1183, Test Loss: 0.1145, Train Acc: 0.9748, Test Acc: 0.9752, LR: 0.000620\n",
      "Epoch 21/30: Train Loss: 0.1183, Test Loss: 0.1145, Train Acc: 0.9748, Test Acc: 0.9752, LR: 0.000620\n",
      "Epoch 22/30: Train Loss: 0.1182, Test Loss: 0.1145, Train Acc: 0.9748, Test Acc: 0.9752, LR: 0.000620\n",
      "Epoch 23/30: Train Loss: 0.1182, Test Loss: 0.1145, Train Acc: 0.9748, Test Acc: 0.9753, LR: 0.000620\n",
      "Early stopping triggered after 23 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13792   510]\n",
      " [  275 17217]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.98     31794\n",
      "   macro avg       0.98      0.97      0.98     31794\n",
      "weighted avg       0.98      0.98      0.98     31794\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Configurations:\n",
      "   Learning Rate  Batch Size Optimizer Model  Test Accuracy  Test Loss\n",
      "0       0.069519         128   rmsprop    lr       0.975876   0.110950\n",
      "1       0.007848          32   rmsprop    lr       0.975561   0.110418\n",
      "2       0.001833         128      adam    lr       0.975530   0.109975\n",
      "3       0.011288         256   rmsprop    lr       0.975499   0.110533\n",
      "4       0.000886         128      adam    lr       0.975499   0.113490\n",
      "\n",
      "Best Configuration:\n",
      "Best Accuracy: 0.9759\n",
      "Best Params: {'weight_decay': 4.641588833612782e-05, 'scheduler_step_size': 15, 'scheduler_gamma': 0.7, 'patience': 5, 'optimizer': 'rmsprop', 'num_epochs': 75, 'model': 'lr', 'learning_rate': 0.06951927961775606, 'batch_size': 128}\n",
      "Epoch 1/75: Train Loss: 0.1261, Test Loss: 0.1122, Train Acc: 0.9722, Test Acc: 0.9753, LR: 0.069519\n",
      "Epoch 2/75: Train Loss: 0.1161, Test Loss: 0.1118, Train Acc: 0.9754, Test Acc: 0.9763, LR: 0.069519\n",
      "Epoch 3/75: Train Loss: 0.1161, Test Loss: 0.1105, Train Acc: 0.9755, Test Acc: 0.9754, LR: 0.069519\n",
      "Epoch 4/75: Train Loss: 0.1159, Test Loss: 0.1120, Train Acc: 0.9755, Test Acc: 0.9763, LR: 0.069519\n",
      "Epoch 5/75: Train Loss: 0.1162, Test Loss: 0.1123, Train Acc: 0.9756, Test Acc: 0.9751, LR: 0.069519\n",
      "Epoch 6/75: Train Loss: 0.1162, Test Loss: 0.1103, Train Acc: 0.9755, Test Acc: 0.9764, LR: 0.069519\n",
      "Epoch 7/75: Train Loss: 0.1161, Test Loss: 0.1101, Train Acc: 0.9755, Test Acc: 0.9751, LR: 0.069519\n",
      "Epoch 8/75: Train Loss: 0.1161, Test Loss: 0.1136, Train Acc: 0.9755, Test Acc: 0.9754, LR: 0.069519\n",
      "Epoch 9/75: Train Loss: 0.1163, Test Loss: 0.1105, Train Acc: 0.9755, Test Acc: 0.9756, LR: 0.069519\n",
      "Epoch 10/75: Train Loss: 0.1162, Test Loss: 0.1108, Train Acc: 0.9754, Test Acc: 0.9760, LR: 0.069519\n",
      "Epoch 11/75: Train Loss: 0.1162, Test Loss: 0.1139, Train Acc: 0.9754, Test Acc: 0.9758, LR: 0.069519\n",
      "Epoch 12/75: Train Loss: 0.1161, Test Loss: 0.1110, Train Acc: 0.9754, Test Acc: 0.9759, LR: 0.069519\n",
      "Early stopping triggered after 12 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13823   479]\n",
      " [  288 17204]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.97      0.97     14302\n",
      "         1.0       0.97      0.98      0.98     17492\n",
      "\n",
      "    accuracy                           0.98     31794\n",
      "   macro avg       0.98      0.98      0.98     31794\n",
      "weighted avg       0.98      0.98      0.98     31794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_results = hyperparameter_random_search(data_dict)\n",
    "best_params = hyperparameter_results['best_params']\n",
    "results_summary = hyperparameter_results['summary_df']\n",
    "\n",
    "final_model = train_model(\n",
    "    data_dict, \n",
    "    **best_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
